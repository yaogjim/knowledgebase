---
title: "AI代理历史记录管理最佳实践"
source: "https://x.com/hhh69251498/status/1947358719081386464"
author:
  - "[[@hhh69251498]]"
created: 2025-07-29
description:
tags:
  - "@hhh69251498 #Agent #历史记录 #上下文工程 #AI代理"
---
**0xhhh** @hhh69251498 2025-07-18

1/5

Agent最佳实践之「历史记录管理」：

(以 @OpenAI 的api举例)

@ManusAI\_HQ 也在他们最新的Agent文章「AI代理的上下文工程」用了很大篇幅的内容写「怎么做好历史记录管理」。

这是 Agent上下文工程里我觉得

「最重要」

「最简单」

「也最容易犯错」

的部分，如果你没有这部分认知，那么即便你有claude code你也很难写好一个LLM的api调用，建议每个想做Agent的朋友都看看。

因为「历史记录实现的好坏」决定了：

1\. 你的模型能不能用好cache token

2\. 推理模型的能不能复用之前的思维链，如果思维链断了就需要重新推理。

下图是你的请求有 cache token和没有cache token带来的成本和响应时间差异：

> 2025-07-18
> 
> After four overhauls and millions of real-world sessions, here are the lessons we learned about context engineering for AI agents: https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus…  
> 经过四次全面检修以及数百万次实际应用，以下是我们在为人工智能代理进行情境工程方面所学到的经验教训：https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus…
> 
> ![Image](https://pbs.twimg.com/media/GwZSNy7akAAPN_g?format=jpg&name=large)

---

**0xhhh** @hhh69251498 [2025-07-21](https://x.com/hhh69251498/status/1947358725356064917)

2/5

什么是 Cache Token?

所以我们可以总结我们历史记录核心要考虑的因素就是怎么尽可能命中我们之前Cache Write写的内容。

因此历史管理里最好的方式就是:

「Append History」:

\--------------------

Turn\_1:

user\_message --> LLM(with tools) --> {toolCalls,text}

toolCalls --> ToolExecutor

![Image](https://pbs.twimg.com/media/GwZTTmubEAQL1uK?format=jpg&name=large) ![Image](https://pbs.twimg.com/media/GwZZL2WasAAIOD8?format=png&name=large)

---

**0xhhh** @hhh69251498 [2025-07-21](https://x.com/hhh69251498/status/1947358730804466048)

3/5

但是前面我们还有提到一个点，就是如果我们使用的是推理模型：

那么我们要尽可能考虑让推理模型的复用之前的思维链，否则如果思维链断了推理模型又要浪费很多token去重新推理。

因此实际上 现在的 推理模型都会提供对应的 thinking 的输出：

那么你在使用这些模型的时候，如果你最好把Turn

![Image](https://pbs.twimg.com/media/GwZhDcjaQAApA3f?format=jpg&name=large)

---

**0xhhh** @hhh69251498 [2025-07-21](https://x.com/hhh69251498/status/1947358736609370585)

4/5

因此假如我们使用 openai的response api的时候可以有两种来处理历史记录的方式：

1) 我们把上个response的结果append起来，如下代码：

这里需要注意 store=true 意味openai会帮你做 cache write的工作，所以希望cache token这是必须要将 store=true

2) 还有一种是就是我要吹response

![Image](https://pbs.twimg.com/media/GwZll49a8AA2MED?format=jpg&name=large) ![Image](https://pbs.twimg.com/media/GwZmS8gbEAAjA_U?format=jpg&name=large)

---

**0xhhh** @hhh69251498 [2025-07-21](https://x.com/hhh69251498/status/1947358740484919342)

5/6

另外还要注意2点是:

1\. 一个用户一分钟的请求不要超过15次，不然超过 rate limit之后，你的第16次请求会分配给新的机器，就没办法命中 cache token

2\. 如果上下文窗口不够的时候，直接使用 compact history , 这样你的prompt结构会变成：

sysytem prompt + compact history + new history

---

**0xhhh** @hhh69251498 [2025-07-21](https://x.com/hhh69251498/status/1947358743420862493)

6/6

欢迎大家 star 我的 miniAgent项目持续build中
