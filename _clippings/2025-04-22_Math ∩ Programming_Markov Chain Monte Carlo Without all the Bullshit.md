---
title: "Markov Chain Monte Carlo Without all the Bullshit"
source: "https://www.jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/"
author:
  - "[[Math ∩ Programming]]"
published: 2015-04-06
created: 2025-04-22
description: "I have a little secret: I don’t like the terminology, notation, and style of writing in statistics. I find it unnecessarily complicated. This shows up when trying to read about Markov Chain Monte Carlo methods. Take, for example, the abstract to the Markov Chain Monte Carlo article in the Encyclopedia of Biostatistics.Markov chain Monte Carlo (MCMC) is a technique for estimating by simulation the expectation of a statistic in a complex model."
tags:
  - "clippings"
---
本文是从我的旧 Wordpress 博客移植到这里的。如果你在渲染或布局方面发现任何问题，请给我发电子邮件。

我有一个小秘密：我不喜欢统计学中的术语、符号和写作风格。我觉得它过于复杂了。在尝试阅读关于马尔可夫链蒙特卡罗方法的内容时就会体现出来。例如，以《生物统计学百科全书》中马尔可夫链蒙特卡罗文章的摘要为例。

> 马尔可夫链蒙特卡罗（MCMC）是一种通过模拟来估计复杂模型中统计量期望的技术。连续的随机选择形成一个马尔可夫链，其平稳分布就是目标分布。它在评估复杂贝叶斯模型的后验分布时特别有用。在梅特罗波利斯-黑斯廷斯算法中，从任意“提议”分布中选择项目，并根据接受规则决定是否保留。吉布斯采样器是一种特殊情况，其中提议分布是向量参数单个分量的条件分布。文中考虑了各种特殊情况和应用。

我只能大致理解作者在这里所说的内容（实际上只是因为我提前知道马尔可夫链蒙特卡罗法是什么）。这里肯定提到了比我在这篇文章中要涵盖的更高级的内容。但是，似乎很难找到一个没有多余术语的马尔可夫链蒙特卡罗法的解释。这里的“废话”是作者隐含的一种说法，即需要这样的术语。也许这是为了解释高级应用（比如尝试在“贝叶斯网络中进行推理”），但定义或分析基本概念肯定不需要这些术语。

所以为了应对，以下是我自己对马尔可夫链蒙特卡罗方法的解释，灵感来自约翰·霍普克罗夫特和拉维·坎南的论述。

## 问题在于从一个分布中进行抽样

马尔可夫链蒙特卡罗是一种用于解决从复杂分布中采样问题的技术。让我通过以下虚构场景来解释。假设我有一个神奇的盒子，它能很好地估计婴儿名字的概率。我可以给它一个像“马尔科姆”这样的字符串，它会告诉我你为下一个孩子选择这个名字的确切概率 $pMalcolm$ 。所以在所有名字上存在一个分布 $D$ ，它非常符合你的偏好，为了便于讨论，假设这个分布是固定的，你不能对其进行篡改。

现在出现了一个问题：我想从这个分布 $D$ 中高效地抽取一个名字。这就是马尔可夫链蒙特卡罗旨在解决的问题。为什么这是个问题呢？因为我不知道你用什么过程来挑选名字，所以我自己无法模拟那个过程。这里有另一种你可以尝试的方法：随机均匀地生成一个名字 $x$ ，向机器询问 $px$ ，然后抛一枚偏向概率为 $px$ 的硬币，如果硬币正面朝上就使用 $x$ 。这个方法的问题在于名字的数量呈指数级增长！这里的变量是写下一个名字 $n=|x|$ 所需的比特数。所以要么概率 $px$ 会呈指数级小，我要抛很长时间才能得到一个名字，要么只有少数几个名字有非零概率，而我要进行指数级多次抽取才能找到它们。效率低下可是我的致命伤。

所以这是个严重的问题！为了明确起见，我们来正式重述一下。

定义（采样问题）：设 $D$ 为有限集 $X$ 上的一种分布。你可以通过黑盒访问概率分布函数 $p(x)$ ，该函数根据 $D$ 输出抽取 $x∈X$ 的概率。设计一种高效的随机算法 $A$ ，该算法输出 $X$ 中的一个元素，使得输出 $x$ 的概率约为 $p(x)$ 。更一般地，输出从 $X$ 中根据 $p(x)$ 抽取的元素样本。

假设 $A$ 只能使用公平的随机硬币，不过这使得人们能够有效地模拟抛掷出具有任何所需概率的有偏硬币。

请注意，使用这样一种算法，我们将能够做一些事情，比如估计某个随机变量 $f:X→R$ 的期望值。我们可以通过解决采样问题获得一个大样本 $S⊂X$ ，然后计算该样本上 $f$ 的平均值。这就是蒙特卡洛方法在采样容易时所做的事情。实际上，如果你愿意，采样问题的马尔可夫链解决方案将使我们能够一举完成采样和对 $E(f)$ 的估计。

但核心问题实际上是一个抽样问题，“马尔可夫链蒙特卡罗”更准确地说应该叫做“马尔可夫链抽样方法”。那么让我们看看为什么马尔可夫链可能会对我们有所帮助。

## 随机游走，马尔可夫链蒙特卡罗方法（MCMC）中的“马尔可夫链”部分

马尔可夫链本质上是图上随机游走的一个花哨术语。

你给我一个有向图 $G=(V,E)$ ，并且对于每条边 $e=(u,v)∈E$ 你给我一个数字 $pu,v∈[0,1]$ 。为了使随机游走有意义， $pu,v$ 需要满足以下约束条件：

对于任何顶点 $x∈V$ ，出边 $(x,y)$ 上的所有值 $px,y$ 的集合必须总和为 1，即形成一个概率分布。

如果满足此条件，那么我们可以根据以下概率在 $G$ 上进行随机游走：从某个顶点 $x0$ 开始。然后根据出边的概率随机选择一条出边，并沿着它到达 $x1$ 。如果可能的话重复此过程。

我说“如果可能的话”是因为任意一个图不一定会有从给定顶点出发的任何出边。为了将随机游走应用于马尔可夫链蒙特卡罗，我们需要对图施加一些额外条件，但无论如何，随机游走的概念是明确的，并且我们把整个对象 $(V,E,{pe}e∈E)$ 称为一个马尔可夫链。

这是一个示例，其中图中的顶点对应于情绪状态。

![](https://www.jeremykun.com/img/2015/markov01.gif)

#### 一个马尔可夫链示例

在统计学领域，他们非常严肃地对待随机游走的“状态”解释。他们将边缘概率称为“状态到状态的转移”。我们要对马尔可夫链做任何有用的事情所需的主要定理是平稳分布定理（有时称为“马尔可夫链基本定理”，这是有充分理由的）。直观地说，它表明对于非常长的随机游走，最终到达某个顶点 $v$ 的概率与起始位置无关！所有这些概率合在一起称为随机游走的平稳分布，并且它由马尔可夫链唯一确定。

然而，出于我们上述所述的原因（“如果可能的话”），平稳分布定理并非对每个马尔可夫链都成立。我们需要的主要性质是图 $G$ 是强连通的。回想一下，有向图在忽略方向时，如果从每个顶点到其他每个顶点都存在一条路径，那么该有向图就称为连通的。如果在考虑方向时仍然处处都有路径，那么它就称为强连通的。如果我们另外要求那个愚蠢的边界情况捕捉器，即没有边可以具有零概率，那么（图的一个组件的）强连通性等同于以下性质：

对于每个顶点 $v∈V(G)$ ，从 $v$ 开始的无限随机游走将以概率 1 返回 $v$ 。

事实上，它会无限次返回。统计学家将这种属性称为状态 $v$ 的持续性。我不喜欢这个术语，因为它似乎描述的是一个顶点的属性，而在我看来，它描述的是包含该顶点的连通分量的属性。无论如何，由于在马尔可夫链蒙特卡罗方法中我们将选择要在其上行走的图（剧透！），我们将通过设计确保图是强连通的。

最后，为了以更常见的方式（使用线性代数）描述平稳分布，我们将转移概率写为矩阵 $A$ ，其中如果存在边 $(i,j)∈E$ ，则元素为 $aj,i=p(i,j)$ ，否则为零。这里行和列对应于 $G$ 的顶点，并且每列 $i$ 形成在随机游走的一步中从状态 $i$ 转移到其他某个状态的概率分布。注意 $A$ 是有向加权图 $G$ 的加权邻接矩阵的转置，其中权重是转移概率（我这样做的原因是因为矩阵与向量相乘时矩阵在左边而不是右边；见下文）。

这个矩阵让我能够用线性代数的语言很好地描述事物。特别地，如果你给我一个基向量 $ei$ ，将其解释为“当前处于顶点 $i$ 的随机游走”，那么 $Aei$ 给出一个向量，其第 $j$ 个坐标是随机游走再走一步后处于顶点 $j$ 的概率。同样地，如果你给我一个顶点上的概率分布 $q$ ，那么 $Aq$ 给出一个按如下方式解释的概率向量：

如果随机游走处于状态 $i$ 的概率为 $qi$ ，那么 $Aq$ 的第 $j$ 个元素是随机游走再走一步后到达顶点 $j$ 的概率。

以这种方式解释，平稳分布是一个概率分布 $π$ ，使得 $Aπ=π$ ，换句话说， $π$ 是 $A$ 的特征值为 1 的特征向量。

对于本博客的忠实读者，有一个简短的补充说明：对随机游走的这种分析正是我们在本博客早期研究用于网页排名的 PageRank 算法时所做的。在那里，我们将矩阵 $A$ 称为“网络矩阵”，在其上进行随机游走，并找到了一个特殊的特征值，其特征向量是一个“平稳分布”，我们用它来对网页进行排名（这用到了一个叫做佩龙 - 弗罗贝尼乌斯定理的东西，该定理表明随机游走矩阵有那个特殊的特征向量）。在那里，我们描述了一种通过迭代乘以 $A$ 来实际找到该特征向量的算法。下面的定理本质上是该算法的一个变体，但在更弱的条件下也能起作用；对于网络矩阵，我们添加了额外的“虚拟”边，以给出所需的更强条件。

定理：设 $G$ 为具有相关边概率 ${pe}e∈E$ 且构成马尔可夫链的强连通图。对于概率向量 $x0$ ，对所有 $t≥1$ 定义 $xt+1=Axt$ ，并设 $vt$ 为长期平均值 $vt=1t∑s=1txs$ 。那么：

1. 存在一个唯一的概率向量 $π$ ，其具有 $Aπ=π$ 。
2. 对于所有 $x0$ ，极限 $limt→∞vt=π$ 。

证明。由于 $vt$ 是一个概率向量，我们只需证明 $|Avt–vt|→0$ 如 $t→∞$ 所示。实际上，我们可以将这个量展开为

$$
Avt–vt=1t(Ax0+Ax1+⋯+Axt−1)–1t(x0+⋯+xt−1) =1t(xt–x0)
$$

但是 $xt,x0$ 是单位向量，所以它们的差最多为 2，这意味着 $|Avt–vt|≤2t→0$ 。现在很明显，这并不依赖于 $v0$ 。为了证明唯一性，我们将偷懒并诉诸佩龙 - 弗罗贝尼乌斯定理，该定理表明任何这种形式的矩阵都有唯一的这样一个（归一化的）特征向量。

$$
◻
$$

另一个需要说明的是，除了通过实际计算这个平均值或使用特征求解器来计算平稳分布外，还可以将其作为特定矩阵的逆进行解析求解。定义 $B=A−In$ ，其中 $In$ 是 $n×n$ 单位矩阵。设 $C$ 是 $B$ ，在其底部附加一行全为 1 的元素，并删除最上面一行。那么可以证明（相当晦涩地） $C−1$ 的最后一列是 $π$ 。我们把这个留给读者作为练习，因为我很确定在实际中没有人使用这种方法。

最后一点说明是关于为什么我们需要对上述定理中的所有 $xt$ 取平均值。对于强连通性，可以添加一个额外的技术条件，称为非周期性，这使得人们可以强化该定理，从而使 $xt$ 自身收敛到平稳分布。严格来说，非周期性是这样一种性质：无论随机游走从何处开始，在经过足够多的步数 $n$ 之后，随机游走在每一个后续步骤处于每个顶点都有正概率。作为一个非周期性不成立的图的例子：具有偶数个顶点的无向环。在那种情况下，每隔一步处于某些顶点才有正概率，对这两个长期序列取平均值就得到了实际的平稳分布。

![](https://www.jeremykun.com/img/2015/screen-shot-2015-04-07-at-6-55-39-pm.png)

#### 图像来源：维基百科

保证你的马尔可夫链是无周期的一种方法是确保在任何顶点停留都有正概率。也就是说，你的图有自环。这就是我们将在下一节中要做的。

## 构建一个可供遍历的图

请记住，我们试图解决的问题是从具有概率函数 $p(x)$ 的有限集 $X$ 上的分布中进行抽样。MCMC 方法是构建一个马尔可夫链，其平稳分布恰好是 $p$ ，即使你只能通过黑盒方式来评估 $p$ 。也就是说，你（隐式地）选择一个图 $G$ 并（隐式地）为边选择转移概率，以使平稳分布为 $p$ 。然后你在 $G$ 上进行足够长的随机游走，并输出与你最终到达的任何状态相对应的 $x$ 。

简单的部分是想出一个具有正确平稳分布的图（实际上，“大多数”图都可行）。困难的部分是想出一个图，在其中你可以证明随机游走向平稳分布的收敛相对于 $X$ 的大小来说是快速的。这样的证明超出了本文的范围，但图的“正确”选择并不难理解。

我们在这篇文章中要选用的算法叫做梅特罗波利斯-黑斯廷斯算法。输入是你对 $p(x)$ 的黑箱访问，输出是一组规则，这些规则隐含地定义了在一个顶点集为 $X$ 的图上的随机游走。

它的工作方式如下：你选择某种方法将 $X$ 放置在一个格点上，使得每个状态对应于 ${0,1,…,n}d$ 中的某个向量。然后你向所有相邻的格点添加（双向有向）边。对于 $n=5,d=2$ ，它看起来会是这样：

![](https://www.jeremykun.com/img/2015/2dlattice.jpg)

图片来源 http://www.ams.org/samplings/feature-column/fcarc-taxi

而对于 $d=3,n∈{2,3}$ ，它看起来会像这样：

![](https://www.jeremykun.com/img/2015/lattice.gif)

#### lattice

在这里你必须小心，以确保为 $X$ 选择的顶点不会断开连接，但在许多应用中， $X$ 自然已经是一个晶格。

现在我们必须描述转移概率。设 $r$ 为此晶格中顶点的最大度数（ $r=2d$ ）。假设我们处于顶点 $i$ ，并且我们想知道下一步去哪里。我们按如下操作：

1. 以概率 $1/r$ 选择邻居 $j$ （有一定概率停留在 $i$ ）。
2. 如果你选择了邻居 $j$ 和 $p(j)≥p(i)$ ，那么确定性地转到 $j$ 。
3. 否则， $p(j)<p(i)$ ，并且你以 $p(j)/p(i)$ 的概率前往 $j$ 。

我们可以更简洁地将边 $(i,j)$ 上的概率权重 $pi,j$ 表述为

$$
pi,j=1rmin(1,p(j)/p(i)) pi,i=1–∑(i,j)∈E(G);j≠ipi,j
$$

很容易验证，对于每个顶点 $i$ ，这确实是一个概率分布。所以我们只需要证明 $p(x)$ 是这个随机游走的平稳分布。

这是一个用于实现此目的的事实：如果一个概率分布 $v$ ，其对于每个 $x∈X$ 的条目为 $v(x)$ ，具有对于所有 $x,y∈X$ 都有 $v(x)px,y=v(y)py,x$ 的性质，那么 $v$ 就是平稳分布。为了证明这一点，固定 $x$ 并对该方程两边在所有 $y$ 上求和。结果恰好是方程 $v(x)=∑yv(y)py,x$ ，它与 $v=Av$ 相同。由于平稳分布是满足此方程的唯一向量，所以 $v$ 必定就是它。

使用我们选择的 $p(i)$ 来做这件事很容易，因为通过对定义应用一点代数运算， $p(i)pi,j$ 和 $p(i)pj,i$ 都等于 $1rmin(p(i),p(j))$ 。所以我们完成了！人们可以根据这些概率随机游走并得到一个样本。

## 遗言

关于马尔可夫链蒙特卡罗方法（MCMC）我想说的最后一点是，要表明在通过你的梅特罗波利斯-黑斯廷斯图（或任何平稳分布为 $p(x)$ 的图）进行随机游走时，你可以同时估计函数 $E(f)$ 的期望值。根据定义， $f$ 的期望值是 $∑xf(x)p(x)$ 。

现在我们能做的是仅在随机游走过程中访问过的那些状态中计算 $f(x)$ 的平均值。稍微多做一点工作，你可以证明这个量将在随机游走收敛到平稳分布的大致相同时间收敛到 $f$ 的真实期望值。（这里的“大致”意味着我们会有一个取决于 $f$ 的常数因子的偏差）。为了证明这一点，你需要一些我懒得在这篇文章中阐述的额外工具，但关键是它是有效的。

我没有一开始就从估计函数期望值的角度来描述马尔可夫链蒙特卡罗方法（MCMC），原因是核心问题是一个采样问题。此外，MCMC 有许多应用，只需要一个样本即可。例如，MCMC 可用于估计任意（可能是高维的）凸集的体积。更多内容可查看阿利斯泰尔·辛克莱的这些讲义。

如果需求足够热门，我可以用代码实现 metropolis - hastings 算法（它不会达到行业标准，但或许有启发性？我不太确定……）

下次见！

---

想回复吗？给我发封电子邮件，发布一条网页提及，或者在互联网上的其他地方找到我。

DOI：https://doi.org/10.59350/7b0tq-90902

---