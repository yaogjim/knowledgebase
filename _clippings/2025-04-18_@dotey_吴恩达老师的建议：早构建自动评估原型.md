---
title: "吴恩达老师的建议：早构建自动评估原型"
source: "https://x.com/dotey/status/1912920713901625678"
author:
  - "[[@dotey]]"
created: 2025-04-18
description:
tags:
  - "@dotey #自动化评估 #AI评估 #迭代改进"
---
**宝玉** @dotey 2025-04-17

吴恩达老师的建议：团队先快速构建一个初步的自动评估原型，再逐步迭代升级到成熟的状态，会帮助你更早地建立有效的评估体系，大幅加快项目进展。  
  
\*\*\*  
  
我注意到很多生成式AI应用项目，在自动化评估系统（evals）的建设上都拖得比较晚。他们往往更长时间地依靠人工来判断输出质量，而不是更早地构建自动评估工具。这是因为团队觉得建立一套有效的自动化评估需要巨大的前期投入，比如制作100个甚至1000个评估样本，设计和验证各项指标，这些工作似乎永远找不到一个“合适的时机”来完成。  
  
我建议团队转变一下思路，把自动评估的构建看作一个渐进式的过程。也就是说，刚开始没必要一步到位，完全可以从简单粗糙的版本做起，比如先用5个样本和一些未经优化的评价指标来进行初步评估，然后再一步步迭代优化。通过这样的方式，你可以逐渐减少对人工判断的依赖，把更多评估工作转移到自动化系统上。  
  
我之前在《The Batch》中写过，建立有效的自动评估方法既重要又困难。举个例子，比如你在开发一个面向客户的聊天机器人，用户可能输入任意内容，而机器人的回复往往并没有一个绝对“正确”的答案。这种情况下，团队通常只能依靠人工一遍遍审查系统每次更新后的大量输出，来判断改动是否带来了真正的提升。尽管最近一些新技术，比如让另一个大语言模型（LLM）来当“裁判”，能帮上一点忙，但要真正把这些技术用好，比如怎样设计提示词、提供什么上下文给裁判等等，依旧是件特别琐碎且繁杂的事。这些挑战使得人们认为做自动评估必须投入大量前期成本，而在实际开发过程中，团队往往觉得，与其花力气研究自动化评估，还不如直接让人力判断来得简单快捷。  
  
我鼓励你们尝试另一种思路：刚开始建立的评估工具，可以只是部分的、不完整的，甚至准确度较低的，也没关系。你可以通过不断迭代逐步提高这些评估工具的质量。自动评估不需要立刻完全取代人工判断，而是可以作为人工评估的重要补充。随着时间推移，你可以慢慢改进自动评估方法，使它们与人工判断的差距逐渐缩小。  
  
举几个具体的例子：  
  
最开始的评估样本可以非常少，比如仅5个。之后逐渐增加数量，甚至必要时减少一些样本，尤其是发现某些样本过于简单或过于困难，无法有效区分系统版本差异时。  
  
一开始的评估可以只测量系统表现中的一部分内容，或者一些你认为与整体表现相关、但并不全面的指标。例如，在客服机器人某个特定时刻需要同时完成两个任务：(1) 调用API执行退款操作；(2) 向用户发送合适的消息。你完全可以先只评估API是否正确调用，而暂时不管消息内容是否合适。又比如机器人在特定情境下需要推荐某个产品时，你可以只评估机器人有没有提到该产品，而不关注具体推荐的方式或措辞是否完美。

只要这些自动评估的结果与总体性能呈现一定的相关性，即便初期只覆盖了你关心的表现的一部分，也是完全可以接受的。  
  
这样以来，整个开发过程就包含两个并行的迭代循环：  
  
改进系统本身的性能：通过自动评估与人工判断的组合，逐步提升系统表现；  
  
改进自动评估本身的准确性：逐渐使自动评估更贴近人工判断。

和AI领域许多工作一样，我们第一次往往不会完全做好。因此，快速搭建起一个初步的、端到端的系统，再通过迭代逐步改进，是更明智的做法。我们已经习惯了这种方式来构建AI系统，同样，我们也可以用这种方式来构建评估工具。  
  
对我而言，一个成功的评估工具应当符合以下两个标准：假设我们现在有个系统A，通过一些修改得到系统B：  
  
如果在人类专家看来，系统A明显比系统B表现得更好，那么自动评估也应该给系统A明显更高的分数。  
  
如果系统A和系统B在人类看来表现类似，自动评估的得分也应相差无几。  
  
当某次评估给出与上述标准相反的结果时，就说明这个评估存在“错误”，我们就应该及时调整，确保它能够正确地区分两个系统的优劣。这种理念类似于机器学习中的误差分析（error analysis），不过我们这里关注的是评估工具本身的“错误”，而不是模型输出的错误（例如模型输出了错误的标签）。评估工具的“错误”表现为无法正确区分两个系统的表现，因此也无法帮助我们做出有效的决策。  
  
项目初期依靠纯人工评估是不错的选择，但对于大多数团队来说，先快速构建一个初步的自动评估原型，再逐步迭代升级到成熟的状态，会帮助你更早地建立有效的评估体系，大幅加快项目进展。

> 2025-04-17
> 
> I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or  
> 我注意到，许多生成式人工智能应用项目对系统输出的自动化评估（评测）可能设置得较晚，而且依靠人工判断输出结果的时间比应有的更长。这是因为构建评测被视为一项巨大的投入（比如，创建 100 个或……
> 
> ![Image](https://pbs.twimg.com/media/GowPPJqWMAAm6-Y?format=jpg&name=large)

---

**RichChat** @richardchang [2025-04-18](https://x.com/richardchang/status/1913034781484528081)

Eval 的重要程度在落地实施时确实经常被忽视