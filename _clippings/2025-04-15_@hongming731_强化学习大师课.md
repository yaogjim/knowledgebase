---
title: "强化学习：从概念到应用"
source: "https://x.com/hongming731/status/1911787900699910329"
author:
  - "[[@hongming731]]"
created: 2025-04-15
description:
tags:
  - "@hongming731"
---
**ginobefun** @hongming731 [2025-04-14](https://x.com/hongming731/status/1911787900699910329)

我刚看了篇 42章经对话吴翼（清华大佬，以前在 OpenAI 搞过研究）的文章，把强化学习 (RL) 这事儿讲得特别透彻！  
  
1\. RL 到底是个啥？  
  
他解释说，RL 跟咱们平时熟悉的机器学习（比如认猫认狗）不一样。那种是喂一堆标准答案让机器背下来，一步到位。  
  
RL 更像是打游戏：需要做好多步决策（发球、接球、走位），没有唯一的“正确”走法，最后看结果（赢了还是输了）来判断好坏。它处理的是多步决策问题，更接近真实生活（比如出差，咋去都行，只要最后顺利）。  
  
2\. 为啥 RL 现在又火了？跟大模型 (LLM)啥关系？  
  
一开始 LLM（像 GPT-3）虽然能“熟读唐诗三百首”，但不太听话，你让它解释登月，它可能回你“是什么”。  
  
OpenAI 就想了个办法，用 RL 来训练 LLM “听指令”，这就是 RLHF (人类反馈强化学习) 的由来。人来判断哪个回答更好，训练出一个“奖励模型”，让 LLM 学会怎么说话讨人喜欢。  
  
重点来了：RLHF 主要是让 LLM 变好用（对齐），但不能让它变 聪明。  
  
3\. 那 RL 咋让 LLM 变聪明的？  
  
后来大家发现，人解决复杂问题前会先思考。于是就想让 LLM 也学会慢思考。  
  
咋实现呢？一个简单粗暴的办法就是让模型在回答前先“多吐点字”（生成很多中间过程），这个过程就像思考。  
  
训练这种“思考”过程，又用到了 RL！因为我们只关心最终答案对不对（比如解数学题），中间它咋想的（吐了啥字）不重要，这又回到了 RL 的逻辑。  
  
4\. RL 和 LLM 是黄金搭档  
  
只靠 RL 行不行？试过，不行。RL 决策能力强，但理解能力弱，需要 LLM 提供基础的理解和记忆能力。  
  
所以，LLM (理解/记忆) + RL (决策/推理/慢思考) = 更强的 AI，两者是乘法关系，缺一不可。像 OpenAI 的 Agent (Operator, Deep Research) 就是这么搞出来的。  
  
5\. 未来 RL 方向 & 挑战  
  
RL 的潜力还很大，它的 scaling law (规模效应) 才刚开始。  
  
不同公司路径也开始分化：OpenAI 搞 Agent，Anthropic 搞编程，DeepSeek 搞通用推理（比如哲学问题也能答）。  
  
搞 RL 很难！ 门槛高（数学、工程要求都高），基建（框架）特别重要，数据也很关键，而且过程像“炼丹”，玄学成分不少。人才特别稀缺。  
  
6\. 对人生的启发 (这个有意思！)  
  
传统 RL 只认准一个最优策略反复用。但吴翼研究发现，人是 Diversity-driven (追求多样性) 的，总想换着法子赢。  
  
他觉得人生也该追求 “熵值最大化”，多探索不同的可能性，别总选最稳妥的路，尤其年轻时，试错成本低。  
  
人生也像 RL，需要主动探索，才能找到自己的 “奖励函数” (目标、意义)。  
  
总的来说，这篇对谈把 RL 的前世今生、跟 LLM 的关系、未来的潜力以及挑战都讲清楚了，还拔高到了人生哲学，信息量很大，值得琢磨琢磨！

![Image](https://pbs.twimg.com/media/GogI6txXAAAoBho?format=jpg&name=large)

---

**ginobefun** @hongming731 [2025-04-14](https://x.com/hongming731/status/1911792994296221924)

#BestBlogs 一堂「强化学习」大师课 | 42章经

吴翼教授在访谈中深入解析强化学习与大语言模型的结合，并分享了 RL 在实际应用和人生决策中的价值。

摘要：

本文是曲凯对清华大学交叉信息研究院助理教授吴翼的访谈，深入探讨了强化学习（RL）这一概念及其最新进展。吴翼从 RL 与传统机器学习的区别入手，解释了 RL 在解决多步决策问题上的优势。随后，讨论了 RL 与大语言模型（LLM）的结合，特别是在解决 LLM 指令遵从问题上的应用，以及 RLHF (Reinforcement Learning from Human Feedback) 的作用。吴翼还分享了 OpenAI 在 RL 领域的探索，以及 RL 在 Agent 范式中的应用。此外，还探讨了 RL 在人才培养中的基建重要性，以及在人生决策中的启示，强调了动手能力、不设限以及主动探索的重要性，并指出创业公司不该有终局思维。

主要内容:

1\. RL 和 LLM 是乘法关系，二者相辅相成 -- LLM 负责理解和记忆，RL 负责决策和执行，二者结合才能实现更高级的智能，缺一不可。

2\. RL 擅长解决多步决策问题，更贴近现实生活 -- 与传统机器学习不同，RL 不需要标准答案，通过反馈机制来评判决策的好坏，模拟了真实世界中解决问题的逻辑。

3\. RL 与 LLM 结合，提升了 LLM 的指令遵从能力 -- 通过 RLHF，LLM 能够更好地理解人类指令并给出符合期望的答案，解决了 LLM 最初存在的“不听话”问题。

4\. Agent 的核心在于对文本之外的世界产生影响 -- RL 赋予了 Agent 自主探索能力，使其能够端到端地完成复杂任务，而不仅仅依赖于 Prompt Engineering。

5\. 人生也是一场强化学习，要敢于探索和试错 -- 通过主动探索，尝试不同的选择，才能找到自己的奖励函数，并追求“熵值最大化”的生活方式。

文章链接：https://bestblogs.dev/article/0b43f3

---

**notice\_u** @ID\_CKChen [2025-04-15](https://x.com/ID_CKChen/status/1911962371448590390)

@readwise save thread  
@readwise 保存线程