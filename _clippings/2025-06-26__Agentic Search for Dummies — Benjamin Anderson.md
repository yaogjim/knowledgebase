---
title: "Agentic Search for Dummies — Benjamin Anderson"
source: "https://benanderson.work/blog/agentic-search-for-dummies/"
author:
published: 2025-07-11
created: 2025-06-26
description: "A simple, effective baseline for building AI search agents."
tags:
  - "clippings"
---
## 《傻瓜式自主搜索指南》

克劳德代码证明了仅仅给模型提供工具并让它们自行解决其余问题所具有的超乎常理的有效性。在这篇文章中，我将阐述在一个模型足够智能，能够将任务或用户请求转化为对搜索索引的一系列查询的世界里，我是如何进行搜索的。我认为，带有离线增强的全文搜索是让模型搜索文档语料库的一种简单而有效的方法。我并未声称这是最先进的方法，而且在这篇文章中你也找不到任何评估数据。相反，我正在展示一种我所喜欢的设置智能搜索的方法，这种方法效果良好，足以成为一个不错的基线。

我假设你已经对搜索以及“智能体”是什么有了一些了解。如果你感到困惑，我建议你查看托尔斯滕·鲍尔的文章《<如何构建一个智能体>》，它会向你展示如何从一个语言模型开始，用几百行代码将其转变为一个简单的编码智能体。

## 概述

这里的“搜索代理”指的是这样一种系统：它接收请求或查询，并使用必要的搜索来收集上下文以完成任务，然后返回针对该请求的 *答案* 或 *结果* 。它由几个基本部分组成，我将逐一介绍。

- 用于构建搜索索引的文档语料库
- 一个搜索 API（无人工智能），它接收查询，命中索引，并返回一个结果页面
- 一组允许语言模型使用搜索应用程序编程接口（API）的“工具”
- 被赋予任务和工具并被指示给出答案的人工智能模型

## 第1部分 - 语料库

搜索很有用，因为它能让人工智能根据任务动态收集上下文信息，而无需你事先确切知道完成任务需要哪些信息。你提供的不是恰好所需的上下文，而是大得多的信息集（语料库），人工智能的部分工作就是弄清楚为完成任务要查看其中的哪些部分。所以，语料库通常是非常大的文档集（数百、数千甚至数百万）。

准备一个干净的语料库非常重要，因为它既是：a）将由你的搜索引擎进行搜索的文本，也是 b）将由模型读取的文本。你要非常确定已移除了随机的 SVG 文件、Base64 编码的数据、导航栏等等，为你的模型留下一组格式良好的文档以供读取。

像 Gemini、Claude、GPT-4.1 这样的多模态大语言模型（LLMs）现在在光学字符识别（OCR）方面已经足够出色，你可以给它们发送 PDF 文件并得到相当干净的 Markdown 文本。但是对于大规模处理 PDF 文件的 OCR，我喜欢 DataLab 的工具 [Marker](https://github.com/datalab-to/marker) ，你可以在 H100 上免费自行运行（如果用于商业目的且你的初创公司筹集了大量资金，则需要许可证）。DOCX 文件可以使用 `python-docx` 以保留编号的方式导入，这很有帮助。HTML 可以转换为 Markdown，我通常会用大语言模型（LLM）对网页进行后处理以去除模板内容。

最后，我认为一个重要的关键技巧（很遗憾，同样没有数据支持这一点！）是离线文档增强。这意味着使用语言模型为每个文档创建一组关键词和一个摘要，它有两个作用。首先，这使得人工智能更容易搜索文档，因为查询可能与文档文本不匹配，但仍可能命中我们预先生成的关键词之一。其次，它能提供更多信息，以便在搜索结果页面上的文档“预览”中显示（稍后会详细介绍）。增强是通过这样一个提示来执行的：

```markdown
Given a section from the {document_name},
provide the following metadata as JSON, with
\`keywords\` and \`description\` keys as follows:

 - \`keywords\` list[str]: A list of as many
keywords/keyphrases as you can think of that
someone might search for where the section
would be relevant. The keywords/phrases do
NOT have to occur in the section, they can be
semantic matches, synonyms, etc. However, they
should be specific to the section, not keywords
that would apply to literally any section of
the {document_name}.
- \`description\` str: A summary/overview of what
the section says. Be mindful of your tendency
to make overlong summaries, and remember that
the goal is to provide a SHORT overview of the
section. A long summary is pointless because
you may as well just read the original.

Here is the {document_name} section:

{section}

Now provide your JSON response,
no prelude or commentary needed.
```

这些文档以及生成的关键词和摘要，都会被保存到一个 JSONL 文件中，该文件可根据需要加载到搜索索引中。

## 第2部分 - 搜索索引

文档准备好后，需要进行索引。搜索索引只是存储文档的一种特殊方式，它能让搜索文档变得快速。例如， [倒排索引](https://en.wikipedia.org/wiki/Inverted_index) 存储从关键词到文档的映射。这样一来，当你搜索一个关键词时，就不必在每个文档中扫描该关键词，那将与文档数量呈线性关系。相反，你可以直接跳到映射中的那个关键词，获取所有相关文档，这只需要固定时间。

对于我们的搜索索引，我们使用 Tantivy。它速度快、开源、实现了优秀的算法，并且有易于安装的 Python 绑定。我们将上一部分生成的文本、关键词和摘要用作可搜索列来构建一个 Tantivy 索引，对于成百上千篇文档来说，这只需几秒钟。我们用一个简单的 API 封装 Tantivy 索引，该 API 实现了两个操作： `search` 和 `read` 。

```python
def search(queries: list[str], limit: int = 10) -> list[SearchResult]:
    pass

def read(document_ids: list[int]) -> list[Document]:
    pass
```

**此 API 将搜索与读取分离。** 这一点很重要，因为如果强制 AI 读取它搜索到的每一篇文档，上下文窗口很快就会被不相关或重复的文档填满。相反，对于每次搜索，我们会得到一个搜索结果列表（就像谷歌搜索页面一样），并将其提供给 AI。然后，AI 可以决定它实际想要“点击”并读取其中哪些文档（如果有的话）。

你还会注意到， `queries` 是一个列表。我们的搜索索引允许多个查询，并融合结果。这意味着你可以提供 5 个、10 个或 100 个查询，但仍然只会得到一个搜索结果页面。每个查询都是单独运行的，然后会使用像 [Reciprocal Rank Fusion](https://www.elastic.co/docs/reference/elasticsearch/rest-apis/reciprocal-rank-fusion) 这样的算法，根据每个文档在所有查询中的排名高低，将所有 5 个、10 个或 100 个页面 “融合” 成 1 个搜索结果页面。允许模型像这样 “随意抛洒” 查询可以提高召回率，因为这增加了至少有一个查询包含命中所需文档的关键字的机会。

## 第3部分 - 工具

人工智能模型可以编写 Python 代码，所以它们本可以使用这个搜索索引，但执行由语言模型生成的任意代码被认为是有害的，所以通常会使用“工具”抽象，这使得人工智能能够调用非常特定的函数，而不能调用其他函数。在我们的例子中，上面的 `search` 和 `read` 函数分别被转换成人工智能能理解的形式，并与请求一起传递给模型。

实现这一点的一种方法是使用 *工具* ，这是一种抽象概念，其中每个函数都有一个 JSON 模式来解释如何使用它。这个工具列表会直接传递给模型。另一种方法是使用 MCP 服务器，它现在似乎很流行。MCP（模型上下文协议）服务器类似于 API 服务器，但专门为人工智能使用工具而设计。OpenAI、Anthropic 和谷歌都在某种程度上支持 MCP 集成。我们使用 [FastMCP](https://github.com/jlowin/fastmcp) 将工具转换为服务器。

使用 MCP 时，你只需提供有关服务器的信息。我仍然不完全相信这种额外的抽象是值得的。MCP 部署有点脆弱且不直观，并且提供程序集成可能不稳定（缺少一个尾随斜杠可能会使服务器无法正常工作！）

关于多个索引的一点说明：如果你需要你的搜索代理能够查阅大量来源，我发现将每个来源作为一个单独的索引来处理会很有帮助。例如，一个遗产规划助手应该能够搜索遗嘱检验法规和《国内税收法典》。你 *可以* 将两者合并到一个语料库中，但这会导致不理想的结果：遗嘱检验文件出现在对《国内税收法典》文件的搜索中，反之亦然。相反，你可以将搜索索引分开，并为模型为每个索引提供单独的工具。让人工智能决定它想要搜索哪个语料库——如果你的工具描述足够详细，它通常足够聪明能够弄清楚。

## 第4部分 - 搜索代理

构建一个搜索智能体，你只需要一个语料库、一个搜索索引和一些工具。剩下的就是告诉人工智能它需要完成什么任务，向模型提供工具，并循环调用模型。每个搜索查询都会产生一个结果页面，每次读取文档的请求都会将这些文档直接放入模型的上下文中。搜索和读取以循环方式进行，直到模型确定它已经读取了足够多的内容，并在不使用任何工具的情况下生成最终答案。

使用 MCP 服务器时，模型提供商会为你处理这个循环。如果你要传递工具，那么你必须自己将每个工具的结果发送回模型。不管怎样，这个搜索和读取的循环最终产生一个答案，这就是一个搜索智能体！我们完成了。

## 为什么你的搜索代理不使用嵌入？

密集嵌入并没有什么问题。一个真正的先进搜索系统可能应该使用它们。但它们有严重的弱点，并且会带来开销。对于长文档，嵌入效果会变差，而许多实际任务（包括我们感兴趣的那些）都需要长文档。这意味着使用嵌入要么会带来性能损失，要么就得把文档拆分成非常小的片段（“分块”）。无论如何，非常长的文档都必须分块，但将块限制在500 - 1000字是很苛刻的——这使得在文档的“节点”处进行分割变得更加困难。

将全文搜索与嵌入相结合也会增加排名融合的开销。现在你不仅要跨不同查询进行融合，还要跨两种搜索模式（关键字搜索和神经搜索）进行融合，并且需要调整分配给每种模式的权重。使用密集嵌入还需要进行 LLM 推理，这对 CPU 要求很高，可能不适合在你的 Web 服务器上运行。老实说，Tantivy 也不适合，但 Transformer 推理几乎肯定更糟糕。总体而言，这需要一个更复杂的架构。

关于嵌入的观点是，如果文档在语义上相似，那么即使确切的关键词不匹配，嵌入也能让文档实现匹配。我认为这对智能体搜索来说没那么重要。如果是人类在搜索，并且想用一个写得很差的查询立即得到一个好结果，仅靠全文搜索是不行的。但当一个智能体在搜索时：

- 它可以一次编写大量查询（我们通过多查询搜索来利用这一点）
- 它可以编写更长、更详尽的查询语句（涵盖更多同义词和关键词）
- 当它的初始查询不起作用时，它可以再次尝试

如果说有什么不同的话，那么对于人工智能模型而言，全文搜索是比黑箱搜索引擎更强大的原语，因为查询和结果之间的关系是可预测的，而不是基于直觉的。我知道如果在查询中输入“披萨”这个词，我会得到其中包含“披萨”一词的文档。相比之下，黑箱搜索 API 是模型必须即时学习如何使用的东西（“我是输入意大利辣香肠还是达美乐？在直觉空间中哪个最接近我想要的？”）

## 结论

如果你有任何意见或问题，请在推特上找我（@andersonbcdefg）。通常在这部分我会说如果这类内容让你感兴趣就加入我们，或者请你使用我们的产品，但我们目前不招人，也不想向开发者推销产品，所以……就这些。感谢阅读！

---