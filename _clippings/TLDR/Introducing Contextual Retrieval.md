---
title: "Introducing Contextual Retrieval"
source: "https://www.anthropic.com/news/contextual-retrieval"
author:
  - "[[@AnthropicAI]]"
published:
created: 2025-03-26
description: "Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems."
tags:
  - "clippings"
---
Product 公告

## 引入上下文检索

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F519f63e0ea393f33e56c2e812713d65dcf27a79a-2880x1620.png&w=3840&q=75)

为了使人工智能模型在特定情境中发挥作用，它通常需要获取背景知识。例如，客户支持聊天机器人需要了解其所服务的特定业务的知识，而法律分析机器人需要知晓大量过往案例。

开发者通常使用检索增强生成（RAG）来增强人工智能模型的知识。RAG 是一种从知识库中检索相关信息并将其附加到用户提示中的方法，可显著增强模型的响应。问题在于，传统的 RAG 解决方案在对信息进行编码时会去除上下文，这常常导致系统无法从知识库中检索到相关信息。

在这篇文章中，我们概述了一种能显著改进 RAG 中检索步骤的方法。该方法称为“上下文检索”，并使用了两种子技术：上下文嵌入和上下文 BM25。这种方法可以将失败检索的数量减少 49%，与重排相结合时可减少 67%。这些都代表了检索准确性的显著提高，这直接转化为下游任务中更好的性能。

你可以通过我们的操作手册，轻松地使用 Claude 部署你自己的上下文检索解决方案。

### 关于简单使用更长提示的说明

有时最简单的解决方案就是最好的。如果你的知识库小于 20 万个标记（约 500 页材料），你可以将整个知识库包含在提供给模型的提示中，无需使用检索增强生成（RAG）或类似方法。

几周前，我们为 Claude 发布了提示缓存功能，这使得这种方法显著更快且更具成本效益。开发人员现在可以在 API 调用之间缓存常用提示，将延迟降低 2 倍以上，并将成本降低多达 90%（您可以通过阅读我们的提示缓存手册了解其工作原理）。

但是，随着知识库的增长，您将需要一个更具可扩展性的解决方案。这就是上下文检索发挥作用的地方。

## 关于 RAG 的入门指南：扩展到更大的知识库

对于无法放入上下文窗口的更大知识库，检索增强生成（RAG）是典型的解决方案。RAG 的工作方式是通过以下步骤对知识库进行预处理：

1. 将知识库（文档的“语料库”）分解为较小的文本块，通常不超过几百个词元；
2. 使用嵌入模型将这些块转换为编码含义的向量嵌入；
3. 将这些嵌入向量存储在一个支持通过语义相似度进行搜索的向量数据库中。

在运行时，当用户向模型输入查询时，向量数据库用于根据与查询的语义相似性找到最相关的块。然后，将最相关的块添加到发送给生成模型的提示中。

虽然嵌入模型擅长捕捉语义关系，但它们可能会错过关键的精确匹配。幸运的是，有一种更古老的技术可以在这些情况下提供帮助。BM25（最佳匹配 25）是一种排序函数，它使用词汇匹配来找到精确的单词或短语匹配。它对于包含唯一标识符或技术术语的查询特别有效。

BM25 通过基于 TF-IDF（词频-逆文档频率）概念来工作。TF-IDF 衡量一个词对于文档集中某一文档的重要程度。BM25 通过考虑文档长度并对词频应用饱和函数来对此进行优化，这有助于防止常用词主导结果。

以下是 BM25 能够在语义嵌入失败的地方取得成功的方式：假设用户在技术支持数据库中查询“错误代码 TS-999”。一个嵌入模型可能会找到关于一般错误代码的内容，但可能会错过与“TS-999”的确切匹配。BM25 会查找这个特定的文本字符串以识别相关文档。

RAG 解决方案可以通过以下步骤结合嵌入和 BM25 技术，更准确地检索最适用的文本块：

1. 将知识库（文档的“语料库”）分解为较小的文本块，通常不超过几百个词元；
2. 为这些文本块创建 TF-IDF 编码和语义嵌入；
3. 使用 BM25 基于精确匹配来查找顶级块；
4. 使用嵌入向量根据语义相似度找到顶级文本块；
5. 使用排序融合技术合并并去重(3)和(4)的结果；
6. 将前 K 个块添加到提示中以生成响应。

通过利用 BM25 和嵌入模型，传统的 RAG 系统可以提供更全面、准确的结果，在精确的术语匹配和更广泛的语义理解之间取得平衡。

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F45603646e979c62349ce27744a940abf30200d57-3840x2160.png&w=3840&q=75)

一个使用嵌入和最佳匹配 25（BM25）来检索信息的标准检索增强生成（RAG）系统。TF-IDF（词频-逆文档频率）衡量词的重要性，并构成 BM25 的基础。

这种方法使您能够经济高效地扩展到庞大的知识库，远远超出单个提示所能容纳的范围。但这些传统的检索增强生成（RAG）系统有一个重大局限性：它们常常会破坏上下文。

### 传统检索增强生成（RAG）中的上下文难题

在传统的检索增强生成（RAG）中，文档通常会被分割成较小的块以便于高效检索。虽然这种方法在许多应用中都能很好地工作，但当单个块缺乏足够的上下文时，可能会导致问题。

例如，假设你的知识库中嵌入了一组财务信息（比如说美国证券交易委员会的文件），然后你收到了以下问题：“ACME 公司在 2023 年第二季度的收入增长情况如何？”

一个相关的文本块可能包含这样的文本：“该公司的收入较上一季度增长了 3%。” 然而，仅这个文本块本身并没有指明它所指的是哪家公司或相关的时间段，这使得很难检索到正确的信息或有效地使用这些信息。

## 引入上下文检索

上下文检索通过在嵌入之前为每个块添加特定于块的解释性上下文（“上下文嵌入”）并创建 BM25 索引（“上下文 BM25”）来解决此问题。

让我们回到美国证券交易委员会文件收集的示例。以下是一个块可能如何转换的示例：

```
original_chunk = "The company's revenue grew by 3% over the previous quarter."

contextualized_chunk = "This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter."
```

值得注意的是，过去已经提出了其他利用上下文来改进检索的方法。其他提议包括：向数据块添加通用文档摘要（我们进行了实验，收益非常有限）、假设文档嵌入和基于摘要的索引（我们进行了评估，性能较低）。这些方法与本文所提出的方法不同。

### 实现上下文检索

当然，手动注释知识库中的数千甚至数百万个文本块工作量太大。为了实现上下文检索，我们求助于 Claude。我们编写了一个提示，指示模型提供简洁的、特定于文本块的上下文，使用整个文档的上下文来解释该文本块。我们使用以下 Claude 3 俳句提示为每个文本块生成上下文：

```
<document> 
{{WHOLE_DOCUMENT}} 
</document> 
Here is the chunk we want to situate within the whole document 
<chunk> 
{{CHUNK_CONTENT}} 
</chunk> 
Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.
```

生成的上下文文本通常为 50 到 100 个词元，在对块进行嵌入之前以及创建 BM25 索引之前，会将其添加到块的开头。

以下是预处理流程在实际中的样子：

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160.png&w=3840&q=75)

*上下文检索是一种提高检索准确性的预处理技术。*

如果你对使用上下文检索感兴趣，可以从我们的烹饪书开始。

### 使用提示缓存来降低上下文检索的成本

由于我们上面提到的特殊提示缓存功能，使用 Claude 以低成本实现上下文检索是独一无二的。通过提示缓存，您无需为每个块传入参考文档。您只需将文档一次性加载到缓存中，然后引用先前缓存的内容。假设每个块为 800 个标记、文档为 8000 个标记、上下文指令为 50 个标记，且每个块有 100 个标记的上下文，那么生成上下文块的一次性成本为每百万文档标记 1.02 美元。

#### 方法

我们在各种知识领域（代码库、小说、ArXiv 论文、科学论文）、嵌入模型、检索策略和评估指标上进行了实验。我们在附录 II 中包含了每个领域使用的一些问答示例。

以下图表展示了在所有知识领域中，使用表现最佳的嵌入配置（Gemini Text 004）并检索前 20 个块时的平均性能。我们使用 1 减去召回率@20 作为评估指标，该指标衡量在前 20 个块中未能检索到的相关文档的百分比。你可以在附录中查看完整结果——情境化在我们评估的每个嵌入源组合中都提高了性能。

#### 性能提升

我们的实验表明：

- 上下文嵌入将前 20 个块的检索失败率降低了 35%（从 5.7%降至 3.7%）。
- 结合上下文嵌入和上下文 BM25 将前 20 个块检索失败率降低了 49%（从 5.7%降至 2.9%）。

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7f8d739e491fe6b3ba0e6a9c74e4083d760b88c9-3840x2160.png&w=3840&q=75)

*结合上下文嵌入和上下文 BM25 可将前 20 个块的检索失败率降低 49%。*

#### 实施注意事项

在实施上下文检索时，有几点需要牢记：

1. 块边界：考虑如何将文档拆分为块。块大小、块边界和块重叠的选择会影响检索性能 1。
2. 嵌入模型：虽然上下文检索在我们测试的所有嵌入模型中都提高了性能，但有些模型可能比其他模型受益更多。我们发现 Gemini 和 Voyage 嵌入特别有效。
3. 自定义上下文提示：虽然我们提供的通用提示效果很好，但针对特定领域或用例定制的提示可能会取得更好的效果（例如，包括可能仅在知识库中的其他文档中定义的关键术语词汇表）。
4. 块数：在上下文窗口中添加更多块会增加包含相关信息的机会。然而，更多信息可能会分散模型的注意力，所以这有一个限度。我们尝试了提供 5、10 和 20 个块，发现使用 20 个块在这些选项中表现最佳（比较见附录），但值得根据你的用例进行试验。

始终运行评估：通过向其传递上下文块并区分哪些是上下文哪些是块，响应生成可能会得到改善。

## 使用重排进一步提升性能

在最后一步中，我们可以将上下文检索与另一种技术相结合，以进一步提高性能。在传统的基于检索增强生成（RAG）中，人工智能系统会在其知识库中进行搜索，以找到潜在相关的信息块。对于大型知识库，这种初始检索通常会返回大量不同相关性和重要性的信息块，有时会有数百个。

重排是一种常用的过滤技术，用于确保仅将最相关的块传递给模型。重排可提供更好的响应，并降低成本和延迟，因为模型处理的信息较少。关键步骤如下：

1. 执行初始检索以获取最相关的潜在文本块（我们使用了前 150 个）；
2. 将前 N 个块与用户查询一起通过重排模型；
3. 使用重排模型，根据每个文本块与提示的相关性和重要性为其打分，然后选择前 K 个文本块（我们使用了前 20 个）；
4. 将前 K 个块作为上下文输入模型以生成最终结果。

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8f82c6175a64442ceff4334b54fac2ab3436a1d1-3840x2160.png&w=3840&q=75)

*结合上下文检索和重排以最大化检索准确性。*

### 性能提升

市场上有几种重排模型。我们使用 Cohere 重排器进行了测试。Voyage 也提供了一种重排器，不过我们没有时间对其进行测试。我们的实验表明，在各个领域中，添加重排步骤可以进一步优化检索。

具体而言，我们发现重新排序的上下文嵌入和上下文 BM25 将前 20 个块的检索失败率降低了 67%（从 5.7%降至 1.9%）。

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F93a70cfbb7cca35bb8d86ea0a23bdeeb699e8e58-3840x2160.png&w=3840&q=75)

*重新排序的上下文嵌入和上下文 BM25 将前 20 个块检索失败率降低了 67%。*

#### 成本和延迟考量

重新排序的一个重要考虑因素是对延迟和成本的影响，特别是在对大量块进行重新排序时。由于重新排序在运行时增加了一个额外的步骤，即使重新排序器并行对所有块进行评分，它也不可避免地会增加少量延迟。在为了获得更好的性能而重新排序更多块与为了降低延迟和成本而重新排序更少块之间存在内在的权衡。我们建议针对您的特定用例试验不同的设置，以找到合适的平衡。

## 结论

我们进行了大量测试，比较了上述所有技术（嵌入模型、BM25 的使用、上下文检索的使用、重排器的使用以及检索到的前 K 个结果的总数）的不同组合，涵盖了各种不同的数据集类型。以下是我们的发现总结：

1. 嵌入向量+BM25 比单独使用嵌入向量更好；
2. Voyage 和 Gemini 在我们测试的产品中具有最佳的嵌入效果；
3. 将排名前 20 的块传递给模型比仅传递排名前 10 或前 5 的块更有效；
4. 为块添加上下文可以显著提高检索准确性；
5. 重新排序总比不重新排序要好
6. 所有这些优势相互叠加：为了最大程度地提高性能改进，我们可以将（来自 Voyage 或 Gemini 的）上下文嵌入与上下文 BM25 相结合，再加上一个重新排序步骤，并将 20 个片段添加到提示中。

我们鼓励所有使用知识库的开发者使用我们的烹饪手册来试验这些方法，以解锁新的性能水平。

以下是按数据集、嵌入提供程序、除嵌入外使用 BM25 的情况、上下文检索的使用以及针对检索结果前 20 名的重排使用情况对结果进行的细分。

有关检索@10 和@5 的分类以及每个数据集的示例问题和答案，请参阅附录 II。

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F646a894ec4e6120cade9951a362f685cd2ec89b2-2458x2983.png&w=3840&q=75)

*1 减去跨数据集和嵌入提供程序在 20 个结果处的召回率。*

#### 脚注

1\. 如需进一步了解分块策略，请查看此链接和此链接。