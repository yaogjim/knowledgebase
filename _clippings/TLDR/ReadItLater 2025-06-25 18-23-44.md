好的，这是一份根据您提供的视频内容生成的详尽文字稿。文稿力求内容详尽、通俗易懂且引人入胜，并采用了适当的格式以优化阅读体验。

***

## 开篇：为什么我们需要 RAG？—— 连接大模型与私有世界

大家好，我是来自 LangChain 的软件工程师 Lance Martin。

在今天的课程中，我将带大家从零开始学习并实现一个非常核心的技术——**RAG**，也就是“**检索增强生成**”（Retrieval Augmented Generation）。可以说，这是目前大语言模型（LLM）领域里最热门、也最实用的理念之一。

那么，我们为什么要学习 RAG 呢？这背后的动机非常简单，却也无比重要：**世界上绝大多数的数据，都是私有的。**

而我们所熟知的大语言模型，比如 GPT-4、Claude 3，它们都是用海量的、公开的互联网数据训练出来的。你可以看到，这些模型在预训练阶段“吃”掉了天量的数据，从几万亿到我们甚至无法知晓具体数字的天文级别。

但一个更有趣的趋势是，这些模型的“**上下文窗口**”——也就是它们一次性能够接收和处理外部信息的能力——正在以惊人的速度扩张。

就在一年前，这个窗口可能还只能容纳几千个词（token），大概相当于十几页纸的内容。而现在，我们已经拥有了能够处理一百万个词的模型，这相当于几千页书的体量！

这意味着什么？虽然大模型本身并不了解你的私人数据——比如你公司的内部文档、你的个人笔记、或者那些尚未公开的研究报告——但我们却越来越有能力，将这些海量的、私有的信息“喂”给它们去消化和处理。

**RAG 技术，正是为了实现这个宏伟目标而生的。**

你可以把它想象成一个全新的操作系统，其核心就是大语言模型。而这个系统的核心使命，就是将模型强大的处理能力，与我们这个世界中海量的、重要的、但却处于信息孤岛中的私有数据连接起来。

### RAG 的核心三步曲：一次优雅的信息处理流程

那么，RAG 到底是怎么工作的呢？你可以把它想象成一个非常清晰的三步流程：

1.  **索引 (Indexing)**：首先，我们需要对外部的私有数据进行预处理。你可以把它想象成是为你的海量文件建立一个高效的数字图书馆索引。这个过程会把你的文档（无论是文本、PDF还是数据库记录）转换成一种机器容易搜索和理解的格式。

2.  **检索 (Retrieval)**：当用户提出一个问题时，系统不会直接把问题丢给大模型。相反，它会先利用第一步建立好的索引，去你的私人数据中进行“检索”，像一个高明的图书管理员一样，精准地找出与问题最相关的那几段信息或文档。

3.  **生成 (Generation)**：最后，系统会将用户的问题和上一步检索到的相关文档片段，一起打包，作为上下文情境（Context）提交给大语言模型。模型会基于这些“证据确凿”的文档信息，来生成一个有理有据、内容扎实、并且不会凭空捏造的答案。

这就是 RAG 的核心思想。它完美地结合了**大模型的通用知识和推理能力**与**我们私有数据的具体性和准确性**，其威力不言而喻。

### RAG 进阶：一个更完整、更强大的系统蓝图

当然，一个真正强大的、生产级别的 RAG 系统，比这三步要复杂得多。我们可以把整个流程拆解得更细致，就像一张系统蓝图：

*   **查询转换 (Query Translation)**：用户的原始问题可能很模糊或过于复杂。所以第一步，我们可以利用大模型对问题进行改写、拆解成子问题，或者从不同角度重新表述，使其更适合后续的检索。

*   **路由 (Routing)**：改写后的问题应该去哪里寻找答案？是去存储非结构化文档的向量数据库，还是去存储结构化数据的关系型数据库，或者是图数据库？“路由”这一步负责做出智能决策。

*   **查询构建 (Query Construction)**：这一步是将用户的自然语言问题，转换成特定数据库能够听懂的“专业语言”。最经典的例子就是将文本转换成 SQL 查询语句（Text-to-SQL），或者用于图数据库的 Cypher 语句。

*   **索引 (Indexing)**：这是我们刚才提到的核心环节，为文档建立可供检索的索引。

*   **检索与后处理 (Retrieval & Post-processing)**：在检索出相关文档后，我们还可以进行一步“后处理”。比如，对检索到的结果进行**重新排序（Rerank）**，把质量最高、最相关的文档排在最前面；或者进行过滤，剔除掉不那么重要的信息。

*   **生成与反馈 (Generation & Feedback)**：在生成答案的阶段，我们可以引入一个“**主动式**”的检查与反馈循环。比如，我们可以让另一个模型来评估：
    *   生成的答案是否忠于原文？（检查幻觉）
    *   检索到的文档是否真的和问题高度相关？
    如果检查不通过，系统可以退回上一步，重新检索、改写问题，或者重新生成答案。这种带有反馈循环的机制，我们称之为“**主动式 RAG (Active RAG)**”。

在接下来的内容中，我们将一步步深入这些环节，从最基础的开始，逐步带你构建一个对 RAG 完整而深刻的理解。

---

## 第一部分：RAG 基础三剑客

让我们从 RAG 最核心的三个基础组件开始，一步步构建我们的系统。

### 1. 索引 (Indexing)：为你的数据建立“数字指纹”

索引的第一步，是加载我们想要处理的外部文档。我们的最终目标，是构建一个高效的“**检索器（Retriever）**”，它的使命非常单纯：**根据用户的问题，快速地从信息海洋中，捕捞出最相关的文档**。

如何判断“相关性”呢？在计算机的世界里，比较两段自由流动的文本是非常困难的。但比较两串数字（也就是向量）的相似度，却非常容易。因此，现代 RAG 系统的核心，就是将文本转换成**向量（Vector）**。

近年来，基于机器学习的**嵌入（Embedding）**方法成为了主流。它的工作流程如下：

1.  **分割文档 (Split)**：因为嵌入模型一次能处理的文本长度有限（即上下文窗口），我们通常需要把一篇长文档切分成一个个更小的、语义完整的片段（Chunks）。
2.  **生成嵌入 (Embed)**：我们使用一个强大的嵌入模型（比如 OpenAI 的 `text-embedding-ada-002`），将每一个文本片段“压缩”成一个固定长度的向量。这个向量，就像是这段文本的“**数字指纹**”或“**语义DNA**”，它浓缩了文本的核心含义。
3.  **存入索引 (Index)**：最后，我们将这些向量与它们对应的原文片段一起，存储在一个专门为此设计的数据库里，我们称之为“**向量数据库（Vector Store）**”。

在我们的代码示例中，我们可以看到，我们加载文档，然后使用 OpenAI 的嵌入模型将每个分割后的文本块转换成一个长度为 1536 的向量。这个过程，就是在为我们的私有数据创建独一无二的“数字指纹”，为下一步的精准检索做好万全准备。

### 2. 检索 (Retrieval)：在信息宇宙中进行精准导航

索引库建好了，现在我们来看“检索”是如何运作的。

这个过程的底层逻辑非常直观。我们可以把高维、抽象的向量空间，想象成一个浩瀚的、看不见的三维星空地图：

*   我们数据库里的每一份文档片段，经过“嵌入”之后，都会成为这个星图上的一颗星星，拥有自己独一无二的坐标。
*   这个坐标的位置不是随机的，而是由它的**语义**决定的。这意味着，**语义上相似的文档，它们在星图上的位置就越是彼此靠近。**

这个简单而强大的想法，正是所有现代向量搜索和检索方法的基石。

所以，检索的过程就演变成了：

1.  当用户提出一个问题时，我们也用同样的嵌入模型，将这个问题变成星图上的一个点。
2.  然后，以这个“问题点”为中心，在它周围进行一次“**邻居搜索**”，寻找离它最近的那些星星。
3.  这些被找到的“邻居星星”，就是和用户问题在语义上最相关的文档片段。

我们可以自由指定要寻找多少个“邻居”，这个数量在技术上被称为参数 **K**。比如，设定 `K=3`，就意味着我们要检索出最相关的 3 个文档片段。

在代码中，我们通过设置 `k` 的值，然后调用 `get_relevant_documents` 方法，系统就能根据我们的问题，返回指定数量的最相关的文档。这个过程，就像在浩瀚的信息宇宙中，进行了一次精准的“K-近邻”搜索。

### 3. 生成 (Generation)：让模型“看文说话”，有据可依

我们拿到了与问题相关的文档，现在进入最后一步——“生成”答案。

这一步的核心，是把我们辛辛苦苦检索到的文档，有效地“塞进”大语言模型的上下文窗口里，然后用一个精心设计的**提示词（Prompt）**来引导它进行思考和回答。

一个典型的、基础的 RAG 提示词模板看起来是这样的：

> “请你根据下面提供的‘上下文’信息，来回答‘问题’。请确保你的回答完全基于所提供的上下文。
> **上下文**: {这里会自动填入我们检索到的所有文档内容}
> **问题**: {这里会自动填入用户的原始问题}”

为了将这三步（检索、构建提示词、调用模型）流畅地串联起来，我们可以使用 **LangChain 表达式语言（LCEL）**。它就像一根强大的管道，能够将“检索器”、“提示词模板”和“大语言模型”这三个独立的组件，无缝地连接成一个自动化的工作流，我们称之为“**链（Chain）**”。

整个流程是这样的：

1.  用户输入问题。
2.  “链”自动将问题传递给检索器。
3.  检索器运行，找到相关文档，并将其返回。
4.  “链”自动将返回的文档填充到提示词模板的“上下文”部分，并将原始问题填充到“问题”部分。
5.  填充好的、完整的提示词被发送给大语言模型。
6.  模型基于这些信息生成最终答案。

在代码中，我们只需定义好这个 RAG 链，然后用一个问题来调用它，就能一气呵成地得到最终答案。借助 LangSmith 这样的可观测性工具，我们还能清晰地看到每一步的输入和输出，这对于调试和优化我们的 RAG 系统至关重要。

---

## 第二部分：RAG 进阶技巧——让你的系统更“聪明”

掌握了基础三剑客之后，让我们来看看如何通过一系列进阶技巧，让我们的 RAG 系统变得更加强大、智能和鲁棒。

### 1. 查询转换 (Query Translation)：从源头提升检索质量

用户的提问五花八门，有时很模糊，有时太复杂，有时又过于具体。如果问题本身的质量不高，那么即使我们有再好的索引和检索算法，效果也会大打折扣。“垃圾进，垃圾出”的道理在这里同样适用。因此，在检索之前，对用户的原始问题进行“翻译”或“改造”至关重要。

*   #### 多查询 (Multi-Query)：一个问题，多种问法

    **核心思想**：用户的原始提问方式，可能只是众多有效提问方式中的一种。它在向量空间中的位置，可能恰好与我们想要的文档“擦肩而过”。

    **解决方法**：与其只依赖这一个问题，不如让大模型扮演一个头脑风暴的角色，从不同的角度、用不同的措辞，将原始问题改写成三到五个语义相近但略有不同的新问题。然后，我们用所有这些问题分别去数据库中进行检索，最后将所有的检索结果汇总起来，去重后提供给最终的生成模型。这就像**“广撒网”**，极大地增加了捕获到正确、关键信息的概率。

*   #### RAG-Fusion：多查询的升级版，增加了智能排序

    **核心思想**：RAG-Fusion 在多查询的基础上，引入了一个更智能的排序机制，以提升最终文档列表的质量。

    **解决方法**：它和多查询一样，也会生成多个问题并分别进行检索。不同之处在于，它会对所有检索到的文档，使用一种叫做**“倒数排序融合”（Reciprocal Rank Fusion, RRF）**的先进算法进行重新排序。这个算法能智能地综合考虑一个文档在不同检索结果列表中的排名，给出一个更权威、更合理的最终排序。简单来说，一个文档如果在多个不同的查询结果中都名列前茅，那么它在最终列表中的排名就会非常高。

*   #### 分解 (Decomposition)：化繁为简，逐个击破

    **核心思想**：当面对一个复杂的、包含多个子任务的问题时，直接进行检索往往效果不佳。

    **解决方法**：我们可以采用“**分而治之**”的策略。让大模型先将这个复杂问题，拆解成一系列更小、更具体的子问题。例如，对于问题“请比较 LangChain 和 LlamaIndex 在构建 RAG 应用时的主要区别”，可以拆解为：“LangChain 构建 RAG 的核心组件是什么？”、“LlamaIndex 构建 RAG 的核心组件是什么？”、“它们在数据索引方面有何不同？”等等。系统可以逐个回答这些子问题，甚至可以将前一个子问题的答案，作为回答下一个子问题的背景知识，形成一个连贯的“**思维链（Chain of Thought）**”，最终汇总成一个全面而深入的答案。

*   #### 回退提示 (Step-Back Prompting)：从具体到抽象，拓宽视野

    **核心思想**：有时候，用户的问题过于具体，反而限制了检索的范围，导致找不到信息。

    **解决方法**：我们可以先“**退一步**”，让模型根据具体问题，生成一个更宏观、更抽象的“**上位问题**”。例如，用户问：“爱因斯坦在哪篇论文中首次提出了质能方程 E=mc²？” 这是一个非常具体的问题。直接检索可能找不到。但如果我们先退一步，问一个更宏观的问题：“爱因斯坦在狭义相对论方面的主要贡献和关键论文有哪些？” 检索到关于他狭义相对论的综述性文档后，就很容易在其中找到那篇具体的论文和方程。最后，系统结合宏观检索结果和原始具体问题，生成精准的答案。

*   #### 假想文档嵌入 (HyDE - Hypothetical Document Embeddings)：用“想象”来导航

    **核心思想**：用户的“问题”和数据库中的“文档”，在文本形式和长度上存在巨大差异（问题短小精悍，文档详细冗长），这可能导致向量匹配效果不佳。

    **解决方法**：HyDE 的想法非常巧妙和反直觉。我们不直接用问题的向量去检索，而是先让大模型根据用户的问题，**“凭空”生成一个“假想的、完美的”回答文档**。这个假想文档在用词、风格和内容结构上，会更接近我们真实数据库中的文档。然后，我们对这个假想文档进行嵌入，用它的向量去进行相似度搜索。实践证明，用这个“理想答案”的向量去导航，往往能更精准地找到我们想要的真实文档。

### 2. 路由 (Routing)：智能分发任务的调度中心

当我们的 RAG 系统变得复杂，连接了多个不同的数据源时——比如一个向量数据库存储着非结构化文档，一个 SQL 数据库存储着用户的交易记录——我们就需要一个智能的“**路由**”来决定一个进来的问题应该被分发给谁处理。

*   **逻辑路由 (Logical Routing)**：这种方法充分利用了大模型的“**推理能力**”。我们通过提示词告诉模型：“你有几个可用的工具，一个叫‘向量检索器’，它擅长回答关于产品功能和技术文档的问题；另一个叫‘SQL查询器’，它擅长回答关于销售数据和用户统计的问题。” 当用户提问时，模型会自己分析问题的意图，然后决定调用哪个工具。这通常是通过模型的“**函数调用（Function Calling）**”或“**工具使用（Tool Use）**”能力来实现的。

*   **语义路由 (Semantic Routing)**：这种方法利用的是“**向量相似度**”。我们可以为每个数据源或每个可能的任务，都创建一个简短的描述性提示词，比如“这是一个用于回答物理学相关问题的工具”或“这是一个用于闲聊的工具”。然后，我们对这些描述本身进行“嵌入”，得到它们的向量。当用户提问时，我们计算用户问题的向量与哪个工具描述的向量最相似，就将问题“路由”到对应的工具或任务上。

### 3. 查询构建 (Query Construction)：从人话到机器码的精准翻译官

这个环节是“路由”的下一步，它的目标是将用户的自然语言问题，转换成特定数据库能够执行的、结构化的查询语言。最经典的例子就是 **Text-to-SQL**，将“上个月销售额最高的三个产品是什么？”这样的自然语言，转换成精确的 SQL 查询语句。

在 RAG 的世界里，一个非常常见的应用是为向量数据库的**元数据过滤（Metadata Filtering）**构建查询。比如，用户问：“**帮我找一下 2024 年之后发布的，关于 LangChain 的，并且长度超过 10 分钟的视频。**”

一个强大的 RAG 系统需要能将这句话，精准地解析成类似下面这样的结构化查询：
`{ "topic": "LangChain", "publish_date": { "$gt": "2024-01-01" }, "duration_minutes": { "$gt": 10 } }`
然后，用这个结构化查询去过滤元数据，再在符合条件的文档中进行语义搜索。这同样可以利用模型的函数调用能力，通过向模型提供元数据的“**结构说明书（Schema）**”，让模型学会自动生成这种精确的查询。

### 4. 索引进阶：超越简单的文本分块

传统的索引方法是简单地将文档切块后嵌入。但我们可以做得更聪明，以适应更复杂的检索需求，特别是长上下文时代的到来。

*   #### 多表征索引 (Multi-representation Indexing)：检索用“简介”，生成用“全文”

    **核心思想**：我们用于**检索的单元**，不必与我们最终用于**生成的单元**完全相同。

    **解决方法**：我们不再直接对切分后的文档小块进行索引。取而代之的是，我们为每一份**完整的、原始的文档**，利用大模型生成一个高度浓缩、包含核心关键词的**摘要（Summary）**。在我们的向量索引库里，我们只存储这些摘要的向量。

    当检索时，我们用这个高度优化的摘要来快速、精准地定位到相关的**完整文档**。一旦找到，我们不再返回小片段，而是直接将**未经切分的、完整的原始文档**交给拥有长上下文窗口的大模型去生成答案。这样做的好处是，模型在回答问题时，拥有了最全面的上下文信息，避免了因文档切分不当而导致的关键信息丢失。

*   #### Raptor：构建信息的“金字塔”式层级索引

    **核心思想**：有些问题需要细节信息，而另一些问题则需要跨越多个文档的宏观洞察。我们需要一个能够同时满足这两种需求的索引结构。

    **解决方法**：Raptor 是一种创新的**分层索引**技术。它的构建过程如下：
    1.  将所有原始文档（或文档块）视为金字塔的**底层**。
    2.  通过聚类算法，将语义上相似的文档分组。
    3.  为每一个分组，利用大模型生成一个更高层次的、概括性的**摘要**。
    4.  不断重复这个过程，对摘要再进行聚类和总结，形成更高层次的摘要，直到最后只剩一个概括了所有文档的**顶层摘要**。

    最终，我们把所有这些层级的文档块和摘要（从最底层的具体信息到最顶层的宏观概括）**全部**放进同一个向量索引库里。这样，如果用户问一个细节问题，系统就能匹配到底层的具体文档；如果用户问一个需要综合多个文档信息的宏观问题，系统就能匹配到更高层级的摘要。这极大地提升了系统对不同粒度问题的覆盖和理解能力。

*   #### ColBERT (语境化后期交互)：更精细的词级匹配

    **核心思想**：将一个长文档压缩成单个向量，可能会丢失太多细节。ColBERT 提出了一种更精细的匹配方式。

    **解决方法**：传统的嵌入方法是为整个文档或文档块生成一个向量（所谓的“密集向量”）。而 ColBERT 不同，它为**文档中的每一个词（token）**都生成一个独立的、语境化的向量。在检索时，它也是为**查询中的每一个词**生成一个向量。然后，它通过计算查询中的每个词与文档中所有词的最大相似度，并将其累加，来得到最终的相关性分数。这种“**后期交互**”的模式，允许更细粒度的词级匹配，据报道在某些基准测试中表现非常出色。

### 5. 主动式 RAG：让 RAG 系统拥有思考、评估和修正的能力

传统的 RAG 流程像一条单向的、线性的传送带，一步接一步，直到终点。但更高级的“**主动式 RAG（Active RAG）**”则像一个拥有大脑和神经系统的智能体，它能够在流程中进行**思考、检查、判断和修正**。

**LangGraph** 是 LangChain 生态中一个强大的库，它非常适合构建这种复杂的、带有逻辑判断和循环的流程。我们可以把 RAG 流程从一个简单的“**链（Chain）**”升级为一个拥有节点和边的“**图（Graph）**”或“**状态机（State Machine）**”。

以一个名为 **Corrective RAG (CRAG)** 的先进方法为例，它的工作流程是这样的：

1.  **检索**：和普通 RAG 一样，首先从内部向量数据库中检索文档。
2.  **评估相关性**：增加一个“**评估节点**”。让一个轻量级的大模型来判断，检索到的每一份文档与用户问题的相关性，并给出一个“相关”或“不相关”的标签。
3.  **条件路由（核心）**：
    *   **如果**大部分文档都被评估为“相关”，那么流程就正常继续，进入“生成答案”的步骤。
    *   **如果**大部分文档都被评估为“不相关”或“模糊”，系统就认为内部知识库可能无法很好地回答这个问题。此时，它会**触发一个备用方案**，比如启动 **Web 搜索**，去互联网上查找最新的、更广泛的信息。
4.  **知识提炼与生成**：将内部检索到的相关文档（如果有的话）和外部 Web 搜索的结果进行整合和提炼，再将这些最优质的信息交给大模型，生成最终的答案。

这种带有**评估、判断和自适应回退机制**的流程，让 RAG 系统变得更加鲁棒和智能。它不仅能回答知识库内的问题，还能在知识库不足时，优雅地寻求外部帮助，而不是简单地回答“我不知道”或生成错误信息。

---

## 终章：RAG 已死？不，它正在重生

随着大模型的上下文窗口从几千扩展到百万级别，一个响亮的问题出现了：“**我们还需要 RAG 吗？为什么不直接把所有文档都一次性塞给模型呢？**” 这就是所谓的“**上下文填充（Context Stuffing）**”策略。

为了回答这个问题，我们进行了一项压力测试，它比经典的“大海捞针（Needle in a Haystack）”测试更进一步，我们称之为“**多针测试（Multi-needle in a haystack）**”。我们把多个互不相关的事实（“针”）分散地隐藏在一篇极长的文本（“草堆”）中，然后考察模型能否将它们全部、准确地找出来。

**我们的测试结果揭示了“上下文填充”策略的几个严峻挑战：**

1.  **事实越多，性能越差**：当需要从长文本中找回的事实数量增加时（从 1 个增加到 10 个），模型的召回率显著下降。
2.  **推理比检索更难**：如果不仅要求模型找出事实，还要求它基于这些事实进行一步简单的推理（比如返回每个成分的首字母），性能会进一步恶化。
3.  **存在“迷失在中间（Lost in the middle）”效应**：模型似乎存在一种“**近因偏见（Recency Bias）**”。它更容易记住和检索到文本末尾的信息，而对于文本开头或中间部分的信息，则有很高的概率会“忘记”或忽略掉。

**结论是：** 简单粗暴地将所有文档都塞进上下文，不仅**成本高昂、延迟极高**，而且在检索的**可靠性上毫无保障**。它是一种脆弱且不负责任的策略。

所以，我们得出的最终答案是：
**RAG 不会死，但它正在以前所未有的方式重生和进化。**

未来的 RAG，或者说下一代 RAG 系统，将会呈现出以下几个鲜明的特征：

1.  **以文档为中心，而非以分块为中心**：随着长上下文模型的普及，我们不再需要过度纠结于如何完美地切分文档。未来的趋势是操作和检索**完整的文档**，让模型自己去理解和消化全部上下文。

2.  **更智能的、分层化的索引策略**：像**多表征索引**和 **Raptor** 这样的技术会变得更加主流。它们天生就是为处理和检索完整文档而设计的，能够更好地平衡检索效率和信息完整性。

3.  **无处不在的“推理与反馈循环”**：RAG 系统将不再是线性的。它会内置更多的检查、评估和反馈循环，就像我们之前讨论的“主动式 RAG”。系统会在检索后评估相关性，在生成后检查幻觉，在需要时自动触发 Web 搜索……它会变得更加“自反”和智能。

总而言之，我们正在从一个简单的“提示-响应”模式，大步迈向一个更复杂的“**流程工程（Flow Engineering）**”时代。在这个新时代，RAG 作为连接大模型强大智能与我们海量私有数据的核心桥梁，其重要性非但没有减弱，反而将以一种更高级、更智能、更可靠的形态，继续扮演着不可或缺的关键角色。

希望这次的分享对大家有所启发。谢谢！
