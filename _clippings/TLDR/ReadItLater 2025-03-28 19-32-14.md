好的，这是根据您提供的英文语音转文字稿整理和修正后的简体中文内容：

---

## LangGraph BigTool：处理大量工具的 Agent

### 引言与演示

构建 Agent 的一个核心组件是工具调用。我们经常遇到的最大挑战之一是如何将大量工具绑定到给定的 Agent。我将展示一个实现这一目标的技巧。

我将使用我们最近发布的一个名为 LangGraph BigTool 的包中预构建的 Agent。你可以在这里看到我们导入了它，我们所要做的就是 `pip install langraph-bigtool`。

现在，我将通过 Ollama 使用 Qwen 1.5 14B 来初始化这个 Agent。所以它实际上将在我的笔记本电脑上本地运行。我还将传入这个工具注册表（tool registry），它是一个包含 51 个不同数学工具的列表，Agent 可以用它们来解决数学问题。

现在，我将提出一个问题并给出一些简单的指令，然后运行它。

我们可以看到 Agent 根据我们的查询进行了一次工具调用。它接收到一组可用的工具，然后从这些工具中决定调用哪个与问题相关，并得到了正确的解决方案。

所以这是一个本地运行的 140 亿参数模型，能够从包含 50 多个不同数学工具的大集合中正确地进行工具调用。

### 工作原理：传统方法及其局限性

那么这是如何运作的呢？嗯，其背后的直觉其实非常直接。

通常，当你构建 Agent 时，你只是简单地拿一个大语言模型（LLM），绑定一组工具，让 LLM 基于这组工具进行思考以执行动作或工具调用，接收这些动作的输出（即观察结果），并循环执行此过程，直到不再需要进行工具调用。

现在，这种方法经常出现的一个问题是，LLM 必须在整个工具集上进行推理。对于少量工具来说，这没问题，但随着工具集变大，LLM 可能会感到困惑。当然，这取决于你使用的模型，取决于你的工具是如何设计的，也取决于你的工具定义。但直觉很清晰：你给 Agent 越多的工具，它的认知负荷就越大。

### BigTool 的核心思想：按需检索工具

现在我们有了这个名为 LangGraph BigTool 的精巧预构建工具，它旨在解决这个问题，并借鉴了许多不同论文中的一些思想，这些思想基本上可以归结为：如果可以按需向 Agent 提供工具呢？也就是说，有一个独立的进程来检索与任务最相关的工具，并将其绑定到 Agent 以实际解决问题。

因此，不是让 LLM 在一个非常大的工具集上进行推理，而是使用一个独立的搜索过程来仅检索与任务相关的工具。这就是核心思想。

### BigTool 的工作流程

那么实际发生的是，我们非常简单地提供了一个名为 `retrieve_tools` 的单一工具，LLM 最初可以调用它。

这个工具会执行工具检索，在这个例子中是从向量存储（vector store）中检索，但这并非必须。你实际上可以为你的工具实现任意的搜索和检索逻辑。但关键在于，最相关的工具是基于某种搜索被检索出来的。

例如，最直观的情况是在向量存储上进行语义相似性搜索，你已经索引了工具的描述，这样就可以检索到与给定任务相关的工具。所以你可以获取用户输入，将其嵌入（embed），然后对工具描述进行语义相似性搜索，以检索与用户问题最相关的工具。

在这个简单的例子中，（假设）工具 2 从该过程中被检索出来，并且只有工具 2 被绑定到 LLM。然后 LLM 可以调用工具 2，然后像我们之前看到的那样进行迭代和循环。

所以关键的区别在于，在传统方法中，LLM 必须查看从 1 到 N 的完整工具集，并确定哪个适合调用。而在这种情况下（使用 BigTool），LLM 的认知负荷显著降低，它只看到基于搜索得出的相关工具。

### BigTool 的优势

这样做明显的好处是降低了 Agent 的认知负荷，并且具有可扩展性，可以处理非常大量的工具。它基本上只受限于你的搜索系统。

### 与多 Agent 架构的比较

这里可能会出现一个有趣的问题：如何看待这种方法与多 Agent 架构的区别？

一个很好的思考方式是，在多 Agent 架构中，你构建了一系列子 Agent，理想情况下，这些子 Agent 将功能按明确的分组进行组织。例如，你可能有一个数学子 Agent、规划子 Agent、酒店预订子 Agent，但通常这些子 Agent 具有某种分组的角色，并相应地有特定的提示（prompts）。

而在 BigTool 这种特定情况下，你不需要这样做。你可以将所有工具添加到一个单一 Agent 中，并使用搜索来找出正确的工具。

所以可以这样考虑：如果你的工具本身具有某种底层组织结构，那么当你能轻易地将这些功能划分到具有不同系统提示的独立 Agent 中时，使用更像多 Agent 的方法可能更有意义。但如果没有这种结构，BigTool 方法可能就很好，因为你可以非常简单地拥有一个单一 Agent，并使用某个搜索系统来为正确的问题检索正确的工具。

所以这是构建 Agent 的两种不同架构，它们各有优缺点，实际上也可以结合使用。你可以拥有一个多 Agent 系统，其中每个 Agent 都使用类似 BigTool 的方法来检索其工具。

### 代码实现细节

我之前展示了 Qwen 1.5 14B，伯克利函数调用排行榜（Berkeley Function Calling Leaderboard）是了解本地模型函数调用能力的一个很好的资源，Qwen 1.5 14B 排在第 30 位，我使用 Ollama 来运行它。

我们已经在 notebook 中展示了它的工作过程，现在我想更详细地介绍一下。

#### 设置与工具创建

所以，我首先要做的就是 `pip install`（可能指 `langraph-bigtool` 等依赖），然后我从 `langraph_bigtool` 导入 `create_agent`。我也用 OpenAI 进行了测试。

要开始使用，你真正需要的只是一个 LangChain 工具（Tool）的列表。好的，在这个特定示例中，我们收集了 Python 内置 `math` 模块中的所有函数，并进行了一些轻微的过滤，只保留了函数部分。

就在这里，我们将每个函数转换成了一个工具。这就是所发生的一切。我们来看一个例子。这是其中一个函数。我们可以看看文档字符串（docstring）的一部分，它说明了这个函数的作用。很好。我们可以看到这个函数（转换后）只是返回一个带有名称和描述的工具。我们看到这个描述就是从函数的文档字符串中提取出来的。这就是这里发生的事情。

所以，我们还是从一个 LangChain 工具列表开始，我们可以使用我们包里的这个便捷函数来帮助创建这些工具。我们实际上可以看看这里的代码。可以看到 `convert_positional_only_functions_to_tools` 基本上就是为仅包含位置参数的函数执行工具创建。这就是这个小工具所做事情的泛化。

#### 工具索引与 Agent 初始化

然后我们所做的就是将它们索引起来。所以我们创建了一个字典（dict），将修饰符（modifiers）添加到每个工具上（注：此处原文 modifiers 可能指代工具元数据或配置，具体含义需看代码）。我们初始化了一个向量存储。

在这个特定案例中，我使用了 LangGraph 的内存存储（in-memory store），但你并非必须这样做。你可以独立地初始化一个向量存储。特别是，如果你查看 README 文件，可以看到我们有一些关于在不使用 LangGraph 存储的情况下检索工具的说明。你可以实现任意的工具检索逻辑。

再次强调，这里的核心组件就是定义这个工具注册表（tool registry）。这里发生的是，工具的描述实际上被嵌入（embedded）了，并且我们有这些 ID，它们是嵌入之上的元数据（metadata）。

我稍后会展示追踪（trace）信息来解释这一切是如何连接起来的。但现在，只需记住我们在这里所做的是嵌入描述，然后可以根据用户的问题使用自然语言进行搜索，以检索最相关的工具。这确实是这里的关键点。

我们初始化我们的 Agent。我们将工具注册表传递给我们的 Agent。因此，我们的 Agent 也知道这个工具注册表，它是一个从工具 ID 映射到实际工具本身的字典。

#### 执行追踪分析

现在我们运行了我们的 Agent，非常简单地传递了一个用户请求，我们可以查看追踪信息来精确了解发生了什么。

所以我们首先进行了这次调用，同样，在这个例子中是使用 Ollama 运行 Qwen 1.5 14B。这是我们的系统提示，这是用户查询，完美。

所以我们对 `retrieve_tools` 工具进行了一次工具调用。这个 `retrieve_tools` 工具就是我们的存储，它根据给定的查询对所有工具描述进行相似性搜索，并返回最相关工具的 UUID。再次强调，这些 UUID 是作为元数据保存的。

所以你会在这里看到一些很酷的事情。我们收到了工具消息（tool message）返回，它会自动获取从语义搜索返回的工具，并将它们绑定到我们的模型。你可以看到我们的模型现在有 `acos` 和 `cos` 作为可用工具（示例）。这些来自于我们的检索步骤。

记住，我们的 Agent 有一个从这些 ID 映射到实际工具本身的字典。所以我们可以非常容易地根据相似性搜索返回的 ID 获取实际的工具，并自动将它们绑定到我们的 LLM。

现在我们的 LLM 只需在（比如）这几个可用的工具中做出决定，这是一个容易得多的决定，因为它能够清楚地识别出（比如）`acos` 是要调用的正确工具。它用参数调用该工具，工具运行并将结果返回给 Agent，然后 Agent 直接响应。

所以你可以通过追踪信息非常快速地看到一切是如何运作的。再次说明，你接收用户的自然语言输入，然后用它对包含所有工具的向量存储执行语义相似性搜索，检索出最相关的工具，特别是传回它们的 UUID，然后 BigTool 会自动将这些工具绑定到我们的 LLM。所以这有点像按需获取并绑定最相关工具的想法，并且我们按预期进行了正确的工具调用。

### 结论

所以，工具选择是 Agent 面临的最具挑战性的问题之一。显然，模型在这方面正变得越来越好。但可能存在这样的情况：你希望 Agent 能够操作非常大量的工具，而你可能希望将工具选择过程从 Agent 本身中移除。

举个例子，你可以有一个系统（在这种情况下像向量存储），它会向 LLM 提供一组与任务最相关的工具，以减少认知负荷并可能提高可靠性。

我们还看到了这种方法的好处：你实际上可以使用能力较弱的模型。这使得你可以采用即使是本地运行的模型，并让它们能够访问非常大量的工具，因为它们实际上不必在如此庞大的工具集上进行推理。这个系统在某种程度上帮助了它们，只为给定任务提供最相关的工具。

所以这是一个不错的技巧。欢迎在下方留下任何评论或问题。谢谢。

---
