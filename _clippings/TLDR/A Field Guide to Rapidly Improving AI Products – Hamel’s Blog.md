---
title: "A Field Guide to Rapidly Improving AI Products – Hamel’s Blog"
source: "https://hamel.dev/blog/posts/field-guide/"
author:
  - "[[Hamel Husain]]"
published:
created: 2025-03-26
description: "Evaluation methods, data-driven improvement, and experimentation techniques from 30+ production implementations."
tags:
  - "clippings"
---
大多数人工智能团队关注的都是错误的事情。以下是我在咨询工作中常见的一幕：

AI TEAM

> 这是我们的智能体架构——这里有 RAG，那里有一个路由器，并且我们正在使用这个新框架用于…

ME

> \[举起手示意热情的技术主管暂停。\]
> 
> “你能向我展示你是如何衡量这一切是否真的有效吗？”

… 房间安静下来

  

在过去两年里，这样的场景已经上演了几十次。团队花费数周时间构建复杂的人工智能系统，但却无法告诉我他们的更改是有帮助还是有损害。

这并不奇怪。每周都有新的工具和框架出现，专注于我们能够控制的切实事物是很自然的——使用哪个向量数据库、选择哪个LLM 提供商、采用哪个代理框架。但在帮助 30 多家公司构建人工智能产品之后，我发现成功的团队几乎根本不谈论工具。相反，他们痴迷于衡量和迭代。

在这篇文章中，我将向你展示这些成功的团队究竟是如何运作的。你将学到：

1. [错误分析如何持续揭示最高投资回报率的改进方法](https://hamel.dev/blog/posts/field-guide/#the-most-common-mistake-skipping-error-analysis)
2. [为什么一个简单的数据查看器是你最重要的人工智能投资](https://hamel.dev/blog/posts/field-guide/#the-most-important-ai-investment-a-simple-data-viewer)
3. [如何让领域专家（而不仅仅是工程师）赋能来改进你的人工智能](https://hamel.dev/blog/posts/field-guide/#empower-domain-experts-to-write-prompts)
4. [为什么合成数据比你想象的更有效](https://hamel.dev/blog/posts/field-guide/#bootstrapping-your-ai-with-synthetic-data-is-effective-even-with-zero-users)
5. [如何维护对评估系统的信任](https://hamel.dev/blog/posts/field-guide/#maintaining-trust-in-evals-is-critical)
6. [为什么你的人工智能路线图应该计算实验次数，而不是功能特性](https://hamel.dev/blog/posts/field-guide/#your-ai-roadmap-should-count-experiments-not-features)

我将用实际例子解释这些主题中的每一个。虽然每种情况都是独特的，但你会看到一些模式，无论你的领域或团队规模如何，这些模式都是适用的。

让我们从审视我所见到的团队最常犯的错误开始——这是一个在人工智能项目甚至尚未启动之前就使其脱轨的错误。

## 1\. 最常见的错误：跳过错误分析

“工具优先”思维模式是人工智能开发中最常见的错误。团队陷入架构图、框架和仪表板之中，却忽视了实际了解哪些有效、哪些无效的过程。

一位客户自豪地向我展示了这个评估仪表盘：

![](https://hamel.dev/blog/posts/field-guide/images/dashboard.png)

预示失败的那种仪表盘。

这就是“工具陷阱”——认为采用正确的工具或框架（在这种情况下，通用指标）就能解决你的人工智能问题。通用指标不仅毫无用处——它们还会以两种方式积极阻碍进展：

首先，它们制造了一种虚假的衡量感和进步感。团队认为自己是数据驱动的，因为他们有仪表盘，但他们追踪的是虚荣指标，这些指标与实际用户问题并无关联。我见过一些团队在实际用户仍在为基本任务苦苦挣扎时，庆祝他们的“帮助得分”提高了 10%。这就好比在结账流程出现问题时优化网站的加载时间——你在错误的事情上越做越好。

其次，过多的指标会分散你的注意力。你不是专注于对你的特定用例重要的少数指标，而是试图同时优化多个维度。当一切都重要时，就没有什么是重要的了。

另一种选择呢？错误分析——人工智能开发中最有价值的单项活动，而且始终是投资回报率最高的活动。让我向你展示有效的错误分析在实际中是什么样的。

### 错误分析过程

当 Nurture Boss 的创始人雅各布需要改进他们的公寓行业人工智能助手时，他的团队构建了一个简单的查看器来检查他们的人工智能与用户之间的对话。每次对话旁边都有一个空白处，用于记录关于失败模式的开放式笔记。

在对数十段对话进行注释后，清晰的模式出现了。他们的人工智能在处理日期方面存在困难——当用户说“我们从现在起两周后安排一次参观”之类的话时，有 66%的情况会出错。

他们没有寻求新工具，而是：1. 查看实际对话日志 2. 对日期处理失败的类型进行分类 3. 构建特定测试以捕捉这些问题 4. 衡量这些指标的改进情况

结果如何？他们的日期处理成功率从 33%提高到了 95%。

以下是雅各布自己对这个过程的解释：

![](https://www.youtube.com/watch?v=e2i6JbU2R-s)

### 自下而上分析与自上而下分析

在识别错误类型时，你可以采用“自上而下”或“自下而上”的方法。

自上而下的方法从诸如“幻觉”或“毒性”等通用指标以及特定于您任务的指标开始。虽然很方便，但它常常会忽略特定领域的问题。

更有效的自下而上方法迫使你查看实际数据，并让指标自然浮现。在 NurtureBoss，我们从一个电子表格开始，其中每一行代表一次对话。我们针对任何不良行为写下开放式笔记。然后我们使用LLM来构建常见失败模式的分类法。最后，我们将每一行映射到特定的失败模式标签，并统计每个问题的出现频率。

结果令人震惊——仅三个问题就占了所有问题的 60%以上：

![](https://hamel.dev/blog/posts/field-guide/images/pivot.png)

Excel 数据透视表是个简单的工具，但很管用！

- 对话流程问题（上下文缺失、回答尴尬）
- 交接失败（无法识别何时转接给人工）
- 重新安排计划问题（处理日期方面的困扰）

影响立竿见影。雅各布的团队发现了如此多可付诸行动的见解，以至于他们花了好几周时间才对我们已经发现的问题进行修复。

如果你想观看错误分析的实际操作，我们在此录制了一个现场演示。

这就引出了一个关键问题：如何让团队轻松查看他们的数据？答案将我们引向我认为任何人工智能团队都能做出的最重要投资……

## 2\. 最重要的人工智能投资：一个简单的数据查看器

我所见过的人工智能团队做出的最具影响力的单项投资，不是一个花哨的评估仪表盘，而是构建一个定制界面，让任何人都能检查他们的人工智能实际在做什么。我强调定制是因为每个领域都有独特的需求，现成的工具很少能满足这些需求。在审查公寓租赁对话时，你需要查看完整的聊天记录和日程安排背景。对于房地产咨询，你需要直接看到房产细节和原始文件。即使是小的用户体验决策——比如在哪里放置元数据或公开哪些过滤器——也可能决定一个工具是人们实际会使用的，还是会避开的。

我见过团队在使用通用标签界面时遇到困难，他们要在多个系统中查找，只为了解一次单一的交互。这种摩擦不断累积：点击进入不同系统查看上下文，将错误描述复制到单独的跟踪表中，在工具之间切换以验证信息。这种摩擦不仅会拖慢团队的速度，还会积极阻碍那种能够发现细微问题的系统分析。

拥有精心设计的数据查看器的团队迭代速度比没有这些工具的团队快 10 倍。关键在于：使用人工智能辅助开发（如 Cursor 或 Loveable），这些工具可以在数小时内构建完成。与回报相比，投资微乎其微。

让我向你说明我的意思。这是为 NurtureBoss 构建的数据查看器（我们之前讨论过）：

![](https://hamel.dev/blog/posts/field-guide/images/nboss_filter.png)

搜索和筛选会话

![](https://hamel.dev/blog/posts/field-guide/images/nboss_annotate.png)

注释并添加笔记

![](https://hamel.dev/blog/posts/field-guide/images/nboss_analysis.png)

汇总并统计错误

以下是一个优秀的数据标注工具应具备的要素：

1. 在一处显示所有上下文。不要让用户在不同系统中查找以了解发生了什么。
2. 使反馈易于捕捉。一键式的正确/错误按钮比冗长的表单更好用。
3. 收集开放式反馈。这使您能够收集那些不符合预定义分类法的细微问题。
4. 启用快速筛选和排序。团队需要能够轻松深入了解特定的错误类型。在上面的示例中，NurtureBoss 可以按渠道（语音、文本、聊天）或他们想要快速查看的特定属性进行快速筛选。
5. 拥有允许用户在数据示例之间导航并无需点击即可进行注释的热键。

你使用什么 Web 框架并不重要——使用任何你熟悉的框架。因为我是一名 Python 开发者，我目前最喜欢的 Web 框架是 FastHTML 与 MonsterUI 相结合，因为它允许我在一个小的 Python 文件中定义后端和前端代码。

关键是要从某个地方开始，即使很简单。我发现定制的网络应用程序能提供最佳体验，但如果你刚开始，电子表格总比没有好。随着需求的增长，你可以相应地升级你的工具。

这给我们带来了另一个违反直觉的教训：最有能力改进你的人工智能系统的人往往是那些对人工智能了解最少的人。

## 3\. 让领域专家编写提示

我最近与一家教育初创公司合作，该公司正在使用LLMs构建一个交互式学习平台。他们的产品经理是一位学习设计专家，会创建详细的 PowerPoint 演示文稿来解释教学原则和示例对话。她会将这些展示给工程团队，然后工程团队会将她的专业知识转化为提示。

但问题在于：提示词只是英文的。让学习专家通过 PowerPoint 来传达教学原则，结果却要工程师再把这些原则翻译回英文提示词，这就产生了不必要的麻烦。最成功的团队会颠覆这种模式，他们会为领域专家提供直接编写和迭代提示词的工具。

### 搭建桥梁，而非设置关卡

提示游乐场是一个很好的起点。像 Arize、Langsmith 和 Braintrust 这样的工具让团队能够快速测试不同的提示，输入示例数据集，并比较结果。以下是这些工具的一些截图：

![](https://hamel.dev/blog/posts/field-guide/images/pp_phoenix2.png)

阿里泽凤凰

![](https://hamel.dev/blog/posts/field-guide/images/pp_langsmith.png)

LangSmith

![](https://hamel.dev/blog/posts/field-guide/images/pp_bt.png)

脑力信托

但许多团队都忽略了关键的下一步：将提示开发集成到他们的应用程序环境中。大多数人工智能应用程序不仅仅是提示——它们通常涉及从知识库中提取信息的 RAG 系统、协调多个步骤的智能体编排，以及特定于应用程序的业务逻辑。我合作过的最有效的团队并不局限于独立的实验平台。他们构建了我所说的集成提示环境——本质上是其实际用户界面的管理版本，可进行提示编辑。

以下是一个房地产人工智能助手的集成提示环境可能的样子的示例：

![](https://hamel.dev/blog/posts/field-guide/images/ipe_before.png)

用户（房地产经纪人）看到的用户界面。

![](https://hamel.dev/blog/posts/field-guide/images/ipe_after.png)

相同的用户界面，但工程和产品团队使用“管理员模式”来迭代提示并调试问题。

### 与领域专家沟通的技巧

还有另一个常常阻碍领域专家有效贡献的障碍：不必要的行话。我曾与一家教育初创公司合作，在那里，工程师、产品经理和学习专家在会议上各说各话。工程师们一直说：“我们要构建一个能做 XYZ 的智能体”，而实际上要完成的工作是编写一个提示。这造成了一个人为的障碍——作为实际领域专家的学习专家们觉得自己无法做出贡献，因为他们不理解“智能体”。

这种情况到处都在发生。我在法律科技公司见过律师这样，在心理健康初创公司见过心理学家这样，在医疗保健公司见过医生这样。LLMs的神奇之处在于，它们通过自然语言让人工智能变得易于使用，但我们常常把所有内容都用专业术语包装起来，从而破坏了这一优势。

以下是一个如何翻译常见人工智能术语的简单示例：

| “我们正在实施一种检索增强生成（RAG）方法” | “我们正在确保模型拥有回答问题的正确上下文” |
| --- | --- |
| “我们需要防止提示注入” | “我们需要确保用户无法诱使人工智能忽视我们的规则” |
| “我们的模型存在幻觉问题” | “有时人工智能会编造内容，所以我们需要检查它的答案” |

这并不意味着简化事情——而是要明确你实际在做什么。当你说“我们正在构建一个智能体”时，你具体增加了什么能力？是函数调用？工具使用？还是只是一个更好的提示？明确具体有助于每个人了解实际发生的情况。

这里有细微差别。技术术语的存在是有原因的——它在与其他技术利益相关者交流时提供了精确性。关键是要根据你的受众调整你的语言。

许多团队在这一点上提出的挑战是：“这听起来都很棒，但如果我们还没有任何数据呢？当我们刚刚起步时，我们如何查看示例或迭代提示？”这就是我们接下来要讨论的内容。

## 4\. 使用合成数据引导你的人工智能是有效的（即使在零用户的情况下）

我从团队那里听到的最常见的障碍之一是：“我们无法进行适当的评估，因为我们还没有足够的真实用户数据。” 这就产生了一个先有鸡还是先有蛋的问题 —— 你需要数据来改进你的人工智能，但你需要一个不错的人工智能来吸引能生成数据的用户。

幸运的是，有一个效果出奇好的解决方案：合成数据。LLMs 可以生成涵盖你的人工智能将会遇到的各种场景的逼真测试用例。

正如我在我的“作为一名评委的 1001”博客文章中所写的，合成数据在评估方面可能非常有效。Hex 公司的前人工智能负责人布莱恩·比肖夫（Bryan Bischof）说得很到位：

> “LLMs 在生成出色且多样的用户提示示例方面出奇地出色。这对于支持应用程序功能可能很重要，而且偷偷地说，对于构建评估也很重要。如果这听起来有点像大语言蛇在自食其尾，那我和你一样惊讶！我只能说：它有效，那就发布吧。”

### 生成逼真测试数据的框架

有效合成数据的关键在于选择正确的维度进行测试。虽然这些维度会根据你的具体需求而有所不同，但我发现从三个大致类别来考虑会有所帮助：

1. 功能：你的人工智能需要支持哪些功能？
2. 场景：它会遇到哪些情况？
3. 用户角色：谁将使用它以及如何使用？

这些并不是你可能关心的唯一维度——你可能还想测试不同的语气、技术复杂程度，甚至不同的地区和语言。重要的是确定对你的特定用例重要的维度。

对于我与 Rechat 合作开发的一款房地产客户关系管理人工智能助手，我们是这样定义这些维度的：

```python
features = [
    "property search",      # Finding listings matching criteria
    "market analysis",      # Analyzing trends and pricing
    "scheduling",          # Setting up property viewings
    "follow-up"           # Post-viewing communication
]

scenarios = [
    "exact match",         # One perfect listing match
    "multiple matches",    # Need to help user narrow down
    "no matches",         # Need to suggest alternatives
    "invalid criteria"     # Help user correct search terms
]

personas = [
    "first_time_buyer",    # Needs more guidance and explanation
    "investor",           # Focused on numbers and ROI
    "luxury_client",      # Expects white-glove service
    "relocating_family"   # Has specific neighborhood/school needs
]
```

但是定义这些维度只是成功的一半。真正的挑战在于确保你的合成数据能够实际触发你想要测试的场景。这需要两件事：

1. 一个具有足够多样性以支持您的场景的测试数据库
2. 一种验证生成的查询是否实际触发预期场景的方法

对于 Rechat，我们维护了一个列表测试数据库，我们知道这些列表会触发不同的边界情况。有些团队更喜欢使用生产数据的匿名副本，但无论哪种方式，你都需要确保你的测试数据有足够的多样性来演练你关心的场景。

以下是一个示例，展示了我们如何使用这些维度和实际数据为房产搜索功能生成测试用例（这只是伪代码，仅用于说明）：

```python
def generate_search_query(scenario, persona, listing_db):
    """Generate a realistic user query about listings"""
    # Pull real listing data to ground the generation
    sample_listings = listing_db.get_sample_listings(
        price_range=persona.price_range,
        location=persona.preferred_areas
    )
    
    # Verify we have listings that will trigger our scenario
    if scenario == "multiple_matches" and len(sample_listings) < 2:
        raise ValueError("Need multiple listings for this scenario")
    if scenario == "no_matches" and len(sample_listings) > 0:
        raise ValueError("Found matches when testing no-match scenario")
    
    prompt = f"""
    You are an expert real estate agent who is searching for listings. You are given a customer type and a scenario.
    
    Your job is to generate a natural language query you would use to search these listings.
    
    Context:
    - Customer type: {persona.description}
    - Scenario: {scenario}
    
    Use these actual listings as reference:
    {format_listings(sample_listings)}
    
    The query should reflect the customer type and the scenario.

    Example query: Find homes in the 75019 zip code, 3 bedrooms, 2 bathrooms, price range $750k - $1M for an investor.
    """
    return generate_with_llm(prompt)
```

这产生了如下逼真的查询：

| 房产搜索 | 多个匹配项 | 首次购买者 | “在河畔地区寻找价格低于 50 万美元的三居室房屋。因为我们有小孩，所以希望房子靠近公园。” |
| --- | --- | --- | --- |
| 市场分析 | 没有匹配项 | 投资者 | “需要橡树街 123 号的房产信息。特别关注与半径 2 英里内类似房产的租金收益率比较。” |

有用的合成数据的关键在于将其置于实际系统约束条件之下。对于房地产人工智能助手而言，这意味着：

1. 使用来自其数据库的真实列表 ID 和地址
2. 纳入实际的代理日程安排和可用时间段
3. 遵守业务规则，如显示限制和通知期
4. 包括特定市场细节，如业主协会要求或当地法规

然后，我们将这些测试用例输入 Lucy 并记录交互情况。这为我们提供了一个丰富的数据集以供分析，确切地展示了人工智能在实际系统约束下如何处理不同情况。这种方法帮助我们在问题影响真实用户之前就解决了它们。

有时你无法访问生产数据库，尤其是对于新产品而言。在这种情况下，请使用LLMs来生成测试查询和基础测试数据。对于房地产人工智能助手来说，这可能意味着创建具有逼真属性的合成房产列表——价格符合市场范围、有效的地址带有真实的街道名称，以及适合每种房产类型的设施。关键在于将合成数据置于现实世界的约束条件之下，使其对测试有用。生成强大的合成数据库的具体细节超出了本文的范围。

### 使用合成数据的指南

在生成合成数据时，请遵循以下关键原则以确保其有效性：

1. 多样化你的数据集：创建涵盖广泛特征、场景和角色的示例。正如我在我的“作为一名评判者的 1001 个案例”一文中所写的，这种多样性有助于你识别那些你可能无法以其他方式预见的边缘情况和失败模式。
2. 生成用户输入，而非输出：使用LLMs来生成逼真的用户查询或输入，而不是预期的人工智能回复。这可防止你的合成数据继承生成模型的偏差或局限性。
3. 纳入实际系统约束：将合成数据建立在实际系统限制和数据的基础上。例如，在测试调度功能时，使用实际的可用时间段和预订规则。
4. 验证场景覆盖范围：确保你生成的数据实际触发了你想要测试的场景。一个旨在测试“未找到匹配项”的查询在针对你的系统运行时应实际返回零结果。
5. 从简单开始，然后增加复杂性：在添加细微差别之前，先从直接的测试用例开始。这有助于在处理边界情况之前隔离问题并建立基线。

这种方法并非只是理论上的——它已在数十家公司的生产环境中得到验证。通常一开始作为权宜之计的措施，即便在有了真实用户数据之后，也会成为评估基础设施的一个永久性组成部分。

让我们来看看在扩大规模时如何在你的评估系统中保持信任……

## 5\. 在评估中保持信任至关重要

这是我反复看到的一种模式：团队构建评估系统，然后逐渐对其失去信心。有时是因为指标与他们在生产中观察到的情况不一致。其他时候，是因为评估变得过于复杂而难以解释。不管怎样，结果都是一样的——团队又回到了凭直觉和传闻反馈来做决策，这就破坏了进行评估的整个目的。

维护对评估系统的信任与一开始建立它同样重要。以下是最成功的团队应对这一挑战的方法：

### 理解标准漂移

人工智能评估中最隐蔽的问题之一是“标准漂移”——一种随着你观察到更多模型输出，评估标准也会演变的现象。在他们的论文《谁来验证验证者？》中，尚卡尔等人描述了这种现象：

> 为了对输出进行评分，人们需要将他们的评估标准外化并加以定义；然而，对输出进行评分的过程有助于他们定义这些标准。

这就产生了一个悖论：在你看到大量输出之前，你无法完全定义你的评估标准，但你首先需要标准来评估这些输出。换句话说，在对LLM输出进行人工评判之前，不可能完全确定评估标准。

我在与蜂巢公司的菲利普·卡特合作其查询助手功能时，对此有过亲身观察。在我们评估人工智能生成数据库查询的能力时，菲利普注意到了一些有趣的事情：

> “看到LLM如何剖析其推理过程，我意识到自己在判断某些边缘情况时并不一致。”

审查人工智能输出的过程帮助他更清晰地阐明了自己的评估标准。这并不是规划不善的迹象——这是与产生多样化且有时出乎意料的输出的人工智能系统合作的固有特征。

那些对其评估系统保持信任的团队接受了这一现实，而不是与之对抗。他们将评估标准视为与他们对问题空间的理解一同发展的动态文档。他们也认识到不同的利益相关者可能有不同（有时相互矛盾）的标准，并且他们致力于协调这些观点，而不是强加单一标准。

### 创建可信评估系统

那么，如何构建即使在标准发生变化的情况下仍能保持可信度的评估系统呢？以下是我发现最有效的方法：

#### 1\. 优先选择二元决策而非任意尺度

正如我在我的“作为评委的 1001#”一文中所写，二元决策提供了一种清晰度，而更复杂的量表往往会模糊这种清晰度。当面对 1 到 5 的量表时，评估者常常难以区分 3 和 4 之间的差异，从而引入了不一致性和主观性。“有些帮助”和“有帮助”究竟有什么区别？这些边界情况消耗了不成比例的脑力，并在你的评估数据中产生了噪音。而且，即使企业使用 1 到 5 的量表，他们也不可避免地会问在哪里划定“足够好”的界限或触发干预，无论如何都迫使做出二元决策。

相比之下，二元的通过/失败迫使评估者做出明确的判断：这个输出是否达到了目的？这种清晰度延伸到衡量进展——通过的输出增加 10% 立即就有意义，而在五分制上提高 0.5 分则需要解读。

我发现，那些抵制二元评估的团队往往这样做是因为他们想要捕捉细微差别。但细微差别并没有丢失——它只是转移到了伴随判断的定性批评中。这种批评提供了关于某事物通过或失败的原因，以及哪些具体方面可以改进的丰富背景信息，而二元决策则明确了是否根本需要改进，从而产生可采取行动的清晰性。

#### 2\. 通过详细的批评来强化二元判断

虽然二元决策能带来清晰性，但当它们与详细的评估相结合时效果最佳，这些评估能够捕捉到某事物通过或失败原因的细微差别。这种组合能让你两全其美：清晰、可操作的指标以及丰富的情境理解。

例如，在评估一个正确回答了用户问题但包含不必要信息的回复时，一个不错的评价可能是：

> “人工智能成功提供了所需的市场分析（通过），但包含了与投资问题无关的过多邻里人口统计细节。这使得回答比必要的更长，并且可能会分散注意力。”

这些批评的作用不止于解释。它们促使领域专家将隐性知识显性化——我见过法律专家从某种“听起来不对劲”的模糊感觉到明确指出引用格式或推理模式方面的具体问题，而这些问题是可以系统解决的。

当作为少样本示例包含在评判提示中时，这些批评意见可提高LLM对复杂边缘情况的推理能力。我发现，与没有示例批评的提示相比，这种方法通常能使人工评估和LLM评估之间的一致率提高 15%-20%。这些批评意见还为生成高质量的合成数据提供了绝佳的原始材料，从而形成了一个改进的飞轮。

#### 3\. 衡量自动评估与人工判断之间的一致性

如果你正在使用LLMs来评估输出结果（在大规模情况下这通常是必要的），定期检查这些自动评估与人类判断的契合程度至关重要。

考虑到我们天生倾向于过度信任人工智能系统，这一点尤为重要。正如尚卡尔等人在《谁来验证验证者？》中所指出的，缺乏验证评估者质量的工具令人担忧。

> 研究表明，人们往往过度依赖和过度信任人工智能系统。例如，在一次备受瞩目的事件中，麻省理工学院的研究人员在 arXiv 上发表了一篇预印本论文，声称 GPT-4 可以通过麻省理工学院的电子工程与计算机科学考试。几个小时内，这项研究就被揭穿了……原因是过度依赖 GPT-4 进行自我评分而产生了问题。

这种过度信任的问题并不仅限于自我评估。研究表明，LLMs 可能会受到一些简单因素的影响而产生偏差，比如一组选项的排列顺序，甚至是提示中看似无害的格式变化。如果没有严格的人工验证，这些偏差可能会在不知不觉中破坏你的评估系统。

在使用 Honeycomb 时，我们跟踪了我们的“LLM作为评判者”与菲利普的评估之间的一致率：

![](https://hamel.dev/blog/posts/field-guide/images/score.png)

LLM评估者与人类专家之间的一致率。更多详情见此处。

经过三次迭代才实现了超过 90%的一致性，但这项投入在团队可以信赖的系统中得到了回报。如果没有这个验证步骤，随着时间的推移，自动化评估往往会偏离人类的预期，尤其是当输入的分布发生变化时。你可以在此处阅读更多相关内容。

像尤金·严（Eugene Yan）的 AlignEval 这样的工具很好地展示了这种对齐过程。它提供了一个简单的界面，在这个界面中，你上传数据，用二元的“好”或“坏”对示例进行标注，然后根据这些人工判断来评估基于LLM的评判。它之所以有效，是因为它简化了工作流程——你可以快速看到自动评估与你的偏好有何不同，根据这些见解完善你的标准，并衡量随时间的改进情况。这种方法强化了这样一个观点，即对齐不是一次性设置，而是人工判断和自动评估之间持续的对话。

### 在不失去信任的情况下实现扩展

随着你的人工智能系统不断发展，你不可避免地会面临减少评估过程中人工投入的压力。这正是许多团队出错的地方——他们自动化的程度过高、速度过快，从而失去了使评估保持可靠的人为联系。

最成功的团队采取更为审慎的方法：

1. 从高度的人工参与开始：在早期阶段，让领域专家评估很大比例的输出结果。
2. 研究对齐模式：不要将评估自动化，而是专注于理解自动化评估与人类判断在哪些方面一致，在哪些方面存在差异。这有助于你确定哪些类型的情况需要更仔细的人工关注。
3. 使用策略性抽样：不要评估每个输出，而是使用统计技术对能提供最多信息的输出进行抽样，尤其关注一致性最薄弱的领域。
4. 保持定期校准：即使在扩展业务时，也要继续定期将自动评估结果与人工判断进行比较，利用这些比较来完善你对何时信任自动评估的理解。

扩展评估不仅仅是为了减少人力——而是要将人力引导到最能产生价值的地方。通过将人的注意力集中在最具挑战性或信息量最大的案例上，即使系统不断发展，也能保持质量。

既然我们已经讨论了如何在评估中保持信任，那么让我们来谈谈在制定人工智能发展路线图时应如何进行的一个根本性转变……

## 6\. 你的人工智能路线图应计算实验次数，而非功能数量

如果你从事过软件开发工作，你就会熟悉传统的路线图：一份列出功能及其目标交付日期的清单。团队承诺在特定的截止日期前交付特定的功能，而成功与否则以他们实现这些目标的接近程度来衡量。

这种方法在人工智能方面彻底失败了。

我见过一些团队致力于制定诸如“在第二季度推出情感分析”或“在年底前部署基于代理的客户支持”这样的路线图，结果却发现相关技术根本还未达到他们的质量标准。他们要么为了赶截止日期而交付质量欠佳的产品，要么就完全错过截止日期。不管怎样，信任都会受到损害。

根本问题在于，传统路线图假定我们知道什么是可能的。对于传统软件来说，情况往往如此——只要有足够的时间和资源，你就能可靠地构建大多数功能。而对于人工智能，尤其是在前沿领域，你不断在测试可行性的边界。

### 实验与功能

Hex 公司的前人工智能主管布莱恩·比肖夫向我介绍了他所谓的人工智能路线图的“能力漏斗”方法。这种策略重新构建了我们对人工智能发展进程的思考方式。

能力漏斗并没有将成功定义为推出一项功能，而是将人工智能性能分解为逐步提升的实用程度级别。漏斗顶端是最基本的功能——系统能否做出响应？漏斗底部是完全解决用户的待办任务。在这两点之间是实用性不断提高的各个阶段。

例如，在一个查询助手当中，能力漏斗可能如下所示：1. 能够生成语法有效的查询（基本功能）2. 能够生成无错误执行的查询 3. 能够生成返回相关结果的查询 4. 能够生成符合用户意图的查询 5. 能够生成解决用户问题的最优查询（完整解决方案）

这种方法承认人工智能的进展不是非黑即白的——它是关于在多个维度上逐步提高能力。它还提供了一个即使在尚未达到最终目标时也能衡量进展的框架。

我合作过的最成功的团队围绕实验而非功能来构建他们的路线图。他们不是致力于特定的成果，而是致力于实验、学习和迭代的节奏。

尤金·严（Eugene Yan）是亚马逊的一名应用科学家，他分享了自己如何与领导层探讨机器学习项目规划——这个流程最初是为传统机器学习开发的，但同样适用于现代LLM开发：

> “这是一个常见的时间线。首先，我花两周时间进行数据可行性分析，即‘我有合适的数据吗？’\[…\] 然后我再花一个月进行技术可行性分析，即‘人工智能能解决这个问题吗？’在那之后，如果仍然可行，我将花六周时间构建一个我们可以进行 A/B 测试的原型。”

虽然LLMs可能不像传统机器学习那样需要相同类型的特征工程或模型训练，但基本原则是一样的：为你的探索设定时间限制，建立明确的决策点，并在全面实施之前专注于证明可行性。这种方法让领导层相信资源不会浪费在无限制的探索上，同时让团队在进行过程中有学习和适应的自由。

### 基金会：评估基础设施

制定基于实验的路线图的关键在于拥有强大的评估基础设施。没有它，你只是在猜测你的实验是否有效。有了它，你就可以快速迭代、测试假设并在成功的基础上继续发展。

我在 GitHub Copilot 的早期开发过程中亲眼目睹了这一点。大多数人没有意识到的是，该团队在构建复杂的离线评估基础设施方面投入了大量资金。他们创建了一些系统，可以根据 GitHub 上大量的代码库来测试代码补全功能，利用高质量代码库中已有的单元测试作为自动验证补全正确性的方法。这是一项巨大的工程任务——他们必须构建能够大规模克隆代码库、设置环境、运行测试套件并分析结果的系统，同时还要处理编程语言、框架和测试方法的惊人多样性。

这并非是浪费时间——它是加速一切的基础。有了扎实的评估，团队进行了数千次实验，迅速确定了有效的方法，并且能够自信地说“这一改变使质量提高了 X%”，而不是仅凭直觉。虽然前期在评估上的投入看似缓慢，但它避免了关于改变是有益还是有害的无休止争论，并在之后极大地加速了创新。

### 将此传达给利益相关者

当然，挑战在于高管们往往想要确定性。他们想知道功能何时发布以及会有什么作用。你如何弥合这一差距？

关键在于将对话从产出转向成果。与其承诺在特定日期实现特定功能，不如致力于一个能最大限度提高实现预期业务成果几率的过程。

尤金分享了他如何处理这些对话：

> “我试图通过时间盒来让领导层安心。三个月结束时，如果可行，我们就将其投入生产。在这个过程中的任何一步，如果不可行，我们就进行调整。”

这种方法为利益相关者提供了明确的决策点，同时承认人工智能开发中固有的不确定性。它还有助于管理对时间线的预期——不是承诺在六个月内推出一项功能，而是承诺在三个月内明确了解该功能是否可行。

布莱恩的能力漏斗方法提供了另一种强大的沟通工具。它使团队能够展示在漏斗各阶段取得的具体进展，即使最终解决方案尚未准备好。它还有助于高管了解问题出在哪里，并就是否投资资源做出明智的决策。

### 通过分享失败来建立一种实验文化

也许这种方法最违反直觉的方面是强调从失败中学习。在传统软件开发中，失败往往被隐藏或淡化。在人工智能开发中，失败是学习的主要来源。

尤金在他的组织中通过他所谓的“十五分钟写、五分钟读”的每周更新来将此付诸实践：

> 在我的“十五五”计划中，我记录自己的失败与成功。在我们团队内部，我们也会每周进行“无准备分享会”，在会上我们会讨论各自正在做的事情以及学到的东西。当我这么做时，我会特意分享失败经历。

这种做法将失败正常化为学习过程的一部分。它表明，即使是经验丰富的从业者也会遇到死胡同，并且通过公开分享这些经历来加速团队学习。而且，通过赞扬实验过程而不仅仅是结果，团队营造了一个让人们敢于冒险并从失败中学习的环境。

### 更好的前进之路

那么，基于实验的路线图在实际中是什么样的呢？这里有一个尤金参与的内容审核项目的简化示例：

> “我被要求进行内容审核。我说，‘我们是否能实现那个目标还不确定。即使那个目标在我们的数据上可行，或者哪种机器学习技术会起作用也不确定。但这是我的实验路线图。这是我要尝试的技术，我会以两周一次的节奏向你们汇报进展。’”

路线图并未承诺具体的功能或能力。相反，它致力于对可能的方法进行系统探索，并定期进行进度检查，必要时进行调整。

结果很能说明问题：

> 在最初的两到三个月里，没有任何进展。\[…\] 然后 \[一个突破\] 出现了。\[…\] 在一个月内，那个问题就解决了。所以你可以看到，在第一季度甚至四个月的时间里，进展甚微。\[…\] 但随后你也可以看到，突然之间，一些新技术出现了，一些新范式、一些新的思维框架出现了，一下子就 \[解决了\] 80% 的 \[问题\]。

这种模式——长时间看似失败后迎来突破——在人工智能开发中很常见。传统的基于特征的路线图会在数月的“失败”后扼杀该项目，从而错过最终的突破。

通过专注于实验而非功能特性，团队为这些突破的出现创造了空间。他们还构建了使突破更有可能发生的基础设施和流程——数据管道、评估框架和快速迭代周期。

我合作过的最成功的团队，在致力于开发特定功能之前，首先会构建评估基础设施。他们创建能加快迭代速度的工具，并专注于支持快速试验的流程。这种方法一开始可能看起来较慢，但从长远来看，它能让团队快速学习和适应，从而极大地加速开发。

人工智能路线图的关键指标不是已交付的功能，而是所进行的实验。获胜的团队是那些能够比竞争对手进行更多实验、学习更快且迭代更快的团队。而这种快速实验的基础始终是相同的：强大、可靠的评估基础设施，让每个人对结果都有信心。

通过围绕实验而非功能重新规划路线图，你可以为自己的组织创造实现类似突破的条件。

## 结论

在这篇文章中，我分享了我在数十个人工智能实施案例中观察到的模式。最成功的团队不是那些拥有最复杂工具或最先进模型的团队，而是那些掌握了衡量、迭代和学习基本原理的团队。

核心原则出奇地简单：

1. 查看你的数据。没有什么能取代从研究实际示例中获得的洞察力。错误分析始终能揭示出回报率最高的改进之处。
2. 构建能够消除摩擦的简单工具。定制数据查看器使检查人工智能输出变得容易，比具有通用指标的复杂仪表板能产生更多见解。
3. 赋予领域专家权力。最了解您所在领域的人往往是那些能够最有效地改进您的人工智能的人，无论他们的技术背景如何。
4. 战略性地使用合成数据。你无需真实用户即可开始测试和改进你的人工智能。精心生成的合成数据可以推动你的评估过程。
5. 保持对评估的信任。带有详细批评的二元判断在保留细微差别的同时创造了清晰度。定期进行一致性检查可确保自动评估保持可信。
6. 围绕实验而非功能来构建路线图。致力于建立一个实验和学习的节奏，而不是在特定日期实现特定结果。

这些原则适用于任何领域、团队规模或技术栈。它们对从早期初创公司到科技巨头的各类公司都有效，适用于从客户支持到代码生成的各种用例。

### 深入学习的资源

如果你想进一步探索这些主题，这里有一些可能会有帮助的资源：

- 我的博客提供了更多关于人工智能评估与改进的内容。我的其他文章深入探讨了诸如构建有效的LLM评判器、实施评估系统以及人工智能开发的其他方面等主题的更多技术细节。还可以查看 Shreya Shankar 和 Eugene Yan 的博客，他们也是这些主题的重要信息来源。
- 我正在教授的一门课程：《通过评估快速改进人工智能产品》，与施雷亚·尚卡尔共同授课。该课程提供了诸如错误分析、合成数据生成以及构建可信评估系统等技术的实践经验。它包括实践练习以及通过办公时间提供的个性化指导。
- 如果你正在寻找针对贵组织需求的实际操作指导，你可以在 Parlance Labs 了解更多与我合作的信息。