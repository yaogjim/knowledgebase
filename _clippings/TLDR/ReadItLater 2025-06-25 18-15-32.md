好的，这是根据视频内容生成的简体中文文字稿。

### 突破性智能体：构建AI研究智能体的经验教训

**讲者：**
*   Connor Heggie，Unify 联合创始人兼首席技术官
*   Kunal Rai，Unify 软件工程师

---

**Connor Heggie：** 大家好，我是 Connor，Unify 的联合创始人兼首席技术官。

**Kunal Rai：** 我是 Kunal，Unify 的一名工程师。

### Unify的使命：将增长科学化

**Connor Heggie：** 首先快速介绍一下 Unify。我们正在构建一个AI行动系统，帮助公司以可重复、可观察和可扩展的方式增长收入，也就是生成销售线索和寻找新业务。

我们的一个核心信念是，增长应该是一门科学，最好的产品理应获胜。我们发现，进入市场（go-to-market）从根本上说是一个搜索问题，关键在于找到那些你的产品能独特解决其问题的人和公司。

过去，要处理海量的非结构化、语义丰富的数据来解决这个搜索问题，你必须部署人力，比如一个做研究的销售团队。但在大语言模型和人工智能的时代，你现在可以用代码来完成这项工作。这带来了许多好处，如可重复性、可观察性和可扩展性，这些在过去是无法实现的。那么如何进行这项研究呢？答案是：通过智能体（agents）。

### AI研究智能体的应用

我们在产品中就是这样做的。我们接收客户的问题，然后利用互联网数据和开放研究来找到答案。我们向客户索取两样东西：第一，他们希望了解某个特定公司或个人的问题清单，并明确输出格式，比如文本、枚举或布尔值。第二，我们请他们提供一些指导，也就是用自由格式的文本来描述，就像告诉一个高中生应该如何进行实际研究一样。

这个智能体随后会在成千上万家公司上运行，在一个名为“Plays”的引擎中回答这些问题，并帮助你及时向这些公司发送有针对性的销售信息。

例如，我们的客户会研究的问题包括：这家公司最近一次宕机是什么时候？如果你销售的是事件响应工具，你可能想了解这一点。或者，如果你销售的是登录或认证工具，你可能会想找到客户抱怨登录体验不佳的时刻，并找到相关链接，及时地在邮件中提及。

我们运行了大量的智能体，通过 OpenAI 处理了海量 token。今年四月，我们处理了 360 亿个 token，并且这个数字每个月都在快速增长。在这个过程中，我们学到了很多，所以我们想分享一些在规模化运行这些通用研究智能体时获得的实践经验。

### 早期版本：Sam vs Connor

**Kunal Rai：** 谢谢 Connor。回到去年十一月，我们做的第一件事就是构建了我们智能体的第一个版本。我们的创始工程师 Sam 和 Connor 都使用了 ReAct 框架来尝试。这个框架大家可能已经很熟悉了，它代表“推理（Reasoning）”和“行动（Acting）”，允许智能体在其执行轨迹中进行纠正和反应性操作。Connor 和 Sam 都构建了该框架的版本，并配备了三个核心工具：搜索互联网、搜索网站和抓取网站。

大家可以看到，左边是 SamBot Mark 1，右边是 ConnorAgent。是的，我们给所有智能体都起了名字。你可以看到它们在架构上的差异。关键在于，Sam 选择了一个较弱但更快的模型（4.0）来生成和修正计划，而 Connor 则选择了当时最强的推理模型（o1-preview）来生成更强的计划。最终，两者都会进入使用工具的循环。

### 如何评估优劣？

构建完成后，我们首先想知道哪个更好。在建立任何评估或指标之前，我们花了大量时间手动查看运行轨迹，看哪个表现更好。我们初步发现，o1 模型生成的研究计划要详尽得多。

大家可以看到，o1-preview 和 4.0 接收了相同的提示。对于 o1-preview，它生成了大约 1600 个 token，而 4.0 只有 600 个。如果我们放大这个计划，可以看到对于同一个问题，o1-preview 的输出在质量和具体性上都优于 4.0。我们甚至在 o1 的提示中详细说明了如何回答问题、可能的错误和陷阱，以及期望的结构化输出类型。我们发现，这种更具体、更详尽的输出有助于智能体在后续的执行阶段取得更好的结果。

### 初代评估的经验教训

在进行了这种“感觉检查（vibe check）”之后，我们开始构建实际的评估体系。我们从准确率开始，即智能体正确回答问题的百分比，并手动标注了大量数据集。我个人为五个核心数据集中的每个数据集都标注了100家公司，总共大约500个例子。我们根据当时客户可能使用我们智能体的场景来选择这些数据集，比如判断公司是 B2B 还是 B2C、公司概况（总部、规模等）和技术栈等。

当我们让 ConnorAgent 和 SamBot Mark 1 进行对决时，ConnorAgent 胜出了。它在大多数类别中都击败了 SamBot，而且优势相当明显。

我们从这些初步评估中学到，推理模型对下游行动和最终准确率有巨大影响。基于准确率的评估是建立基线的好方法。但仅凭这些，我们仍然无法清晰地知道下一步该如何改进。

### 下一步：改进方向

于是，我们考虑了三个核心的改进方向：
1.  改变智能体的图结构或架构？
2.  改变模型和提示词？
3.  增加更多工具？

经过对客户需求的深思熟虑，我们决定将重点放在第二和第三点上，也就是改变模型和提示词，以及增加更多工具，作为首要的投入方向。

### 模型与提示词的改进

首先是性能和成本优化。像 o1、o3 和 o1-pro 这样的模型既昂贵又慢。而且今年年初，新模型层出不穷，我们几乎每周都在测试新模型。有趣的是，直到最近 4.1 模型发布，我们才看到显著的差异。这也是我们唯一在生产环境中替换 o1 用于智能体规划的模型。这一改变的结果是，一次智能体运行的成本从大约 35 美分降到了 10 美分，而性能基本没有变化。

我们还遇到了日期格式的问题。很多模型无法正确比较不同格式的日期，即使年份不同，也可能因为月份和日期靠前而误判。我们通过在提示词中提供多种日期格式，标准化了模型处理时间数据的能力。

最后是工具调用。最初，智能体经常进行一些“无效”的工具调用，比如只搜索“B2B”这样一个宽泛的词。我们通过修改工具的输入模式（pydantic models）来迫使智能体在调用工具时进行更精确的思考。

### 工具的改进与学习

总的来说，我们学到，智能体成本正在大幅下降。同时，有很多边缘案例是评估指标难以捕捉的，所以“感觉检查”（即人工审查）仍然是顶级的评估方法。不同的模型在不同用例中表现各异，所以针对不同节点进行评估很有必要。增加工具的多样性，意味着更多信息被纳入上下文，因此上下文管理至关重要。

于是，我们进入了第二个改进方向：构建更多工具。我们思考哪些是客户需要但现有工具无法支持的用例，以及增加哪个工具能立刻赋能新工作流。最终，我们决定构建四个工具：
*   深度互联网搜索
*   浏览器访问
*   HTML 搜索
*   数据集访问

#### 深度互联网搜索：升级自谷歌搜索

互联网搜索仍然很难。在谷歌上，你会被 SEO 文章和由大模型生成的摘要所干扰，搜索结果的质量很多时候不受你控制。我们发现，智能体进行研究的方式与人类不同。人类在搜索后会隐含地根据来源信誉筛选链接，打开多个标签页，快速浏览，然后决定是进行新的搜索还是已经找到答案。我们的智能体没有模仿这种行为。

因此，我们升级了搜索工具，从仅仅返回一个搜索词，到可以定义类别、是否实时抓取、是否包含特定短语、限制域名甚至发布日期。通过这些参数，我们改变了搜索的轨迹。现在，智能体可以一次性获取 URL 和完整的页面内容，而不是依赖不可靠的谷歌搜索预览。

#### 浏览器访问：模拟人类操作

第二个我们构建的主要工具是浏览器访问。很多丰富的数据无法通过简单的抓取获得，比如需要交互的搜索、谷歌地图或图片。我们希望让我们的 Unify 智能体能像人类一样使用浏览器。

我们把浏览器访问功能实现为一个子智能体。它接收一个任务，然后使用 o4-mini 模型将任务分解成一系列浏览器操作步骤，并用 `computer-use-preview` 来执行。我们评估了开源的 `browser-use` 工具，发现虽然它稍快一些，但在复杂的浏览器任务中表现不佳，所以我们最终选择了 `computer-use-preview`。

例如，当我们尝试查找谷歌是否有电动汽车充电站时，智能体最终会使用浏览器工具，打开谷歌地图，使用街景视图在停车场寻找充电站，然后切换到另一个标签页确认信息。最终，它结合了谷歌地图和另一个页面的信息，确认了充电站的存在。

### 总结与招聘

我们学到，不能依赖于简单的互联网搜索和抓取。深度搜索和浏览器访问这样的工具，解锁了智能体可以回答的全新问题类别。

**新冠军：KunalBrowserAgent**

经过这些改进，我们新的冠军智能体是 KunalBrowserAgent。它在各项评估中都表现出色。

**下一步：改进评估方法**

接下来，我们将投入更多时间来改进我们的评估体系，以更好地量化和发现我们通过人工审查发现的那些问题，让整个迭代过程更可重复、更具规模。

**Connor Heggie：** 好的。我们正在解决很多有趣的智能体问题。所以，如果你也想让你的名字出现在我们的代码库里，成为一个智能体，欢迎会后和我们聊聊，或者在线申请。我们正在招聘大量工程师。谢谢大家。
