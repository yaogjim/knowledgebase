---
title: " 简洁之道：利用二进制量化技术，打造内存效率提升 32 倍的 RAG 系统"
source: ""
author:
  - ""
published: 2025-08-11
created: 2025-08-11
description: ""
tags:
---

## 简洁之道：利用二进制量化技术，打造内存效率提升 32 倍的 RAG 系统

这份文件将引导您了解一项能将“检索增强生成” (RAG) 系统的内存效率提升约 32 倍的简单技术。这项技术已获得 Perplexity、微软云及 HubSpot 等业界领导者在其搜索索引和 AI 助理中采用。

我们将一步步说明如何构建一个能够在 **30 毫秒内查询超过 3600 万个向量** 的高性能 RAG 系统。

### **核心技术栈**

*   **编排 (Orchestration):** @llama_index
*   **向量数据库 (Vector Database):** @milvusio
*   **无服务器部署 (Serverless Deployment):** @beam_cloud
*   **大型语言模型 (LLM):** @Kimi_Moonshot (由 Groq 托管的 Kimi-K2)

### **整体工作流程**

1.  **文档提取与嵌入**: 读取原始文档，并将其转换为二进制嵌入向量。
2.  **索引创建**: 创建一个二进制向量索引，并将嵌入数据存入 Milvus 向量数据库。
3.  **内容检索**: 根据用户查询，检索出最相似的前 k 个文档片段。
4.  **内容生成**: 大型语言模型 (LLM) 根据检索到的上下文，生成精准的回答。

现在，让我们开始动手实作吧！

---

### **步骤 1：设置 Groq - 驱动极速 AI 推理**

首先，我们需要设置 Groq，它将为我们的系统提供世界上最快的 AI 推理服务。将您的 Groq API 密钥储存在一个 `.env` 文件中，并通过以下代码加载到您的执行环境中。

```python
import os
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

# 获取 Groq API 密钥
groq_api_key = os.getenv("GROQ_API_KEY")
```![Groq API Key Setup](https://storage.googleapis.com/gemini-prod/images/e9441fd2-df49-43c2-ab4f-56ac52a8a5f0)

### **步骤 2：加载数据 - 万物皆可为来源**

我们使用 LlamaIndex 强大的 `SimpleDirectoryReader` 工具来提取文档。这个工具支持多种数据格式，包括 Markdown、PDF、Word 文档、PowerPoint 简报，甚至是图片、音频和视频。

```python
from llama_index.core import SimpleDirectoryReader

# 初始化目录读取器，指定文件目录和类型
loader = SimpleDirectoryReader(
    input_dir="docs_dir",
    required_exts=[".pdf"],
    recursive=True
)

# 加载数据并提取纯文本内容
docs = loader.load_data()
documents = [doc.text for doc in docs]
```![LlamaIndex Data Loading](https://storage.googleapis.com/gemini-prod/images/5f0f353a-c852-4a0b-9df2-a39fa2776c5b)

### **步骤 3：生成二进制嵌入向量 - 效率提升的关键**

这是整个流程的核心步骤。我们首先使用 Hugging Face 的 `bge-large-en-v1.5` 模型生成标准的 `float32` 格式嵌入向量。接着，我们将这些向量转换为二进制向量。

这个过程被称为 **二进制量化 (Binary Quantization)**。它将每个浮点数转换为 0 或 1，从而将内存和存储需求大幅减少 32 倍。

```python
import numpy as np
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# 初始化嵌入模型
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-large-en-v1.5",
    trust_remote_code=True,
    cache_folder='./hf_cache'
)

binary_embeddings = []
# 以批处理方式处理文档
for context in batch_iterate(documents, batch_size=512):
    # 1. 生成 float32 向量嵌入
    batch_embeds = embed_model.get_text_embedding_batch(context)
    
    # 2. 将 float32 向量转换为二进制向量
    embeds_array = np.array(batch_embeds)
    binary_embeds = np.where(embeds_array > 0, 1, 0).astype(np.uint8)
    
    # 3. 转换为字节数组以利存储
    packed_embeds = np.packbits(binary_embeds, axis=1)
    byte_embeds = [vec.tobytes() for vec in packed_embeds]
    
    binary_embeddings.extend(byte_embeds)
```![Hugging Face Binary Embeddings](https://storage.googleapis.com/gemini-prod/images/7356c361-b1e0-474c-83b5-310e54101e4a)

### **步骤 4：向量索引 - 在 Milvus 中打造高效检索**

完成二进制量化后，我们将这些向量存储在 Milvus 向量数据库中，并为其创建索引。索引是一种优化数据检索性能的专门数据结构。

在这里，我们使用 `BIN_FLAT` 索引类型，这是一种专为二进制向量设计的精确搜索索引。同时，我们指定使用 **汉明距离 (HAMMING)** 作为度量标准，这是计算二进制向量之间差异最有效的方法。

```python
from pymilvus import MilvusClient, DataType

# 初始化 Milvus 客户端和结构 (Schema)
client = MilvusClient("milvus_binary_quantized.db")
schema = client.create_schema(auto_id=True, enable_dynamic_fields=True)

# 定义字段
schema.add_field(field_name="context", datatype=DataType.VARCHAR)
schema.add_field(field_name="binary_vector", datatype=DataType.BINARY_VECTOR)

# 指定二进制向量的索引参数
index_params = client.prepare_index_params()
index_params.add_index(
    field_name="binary_vector",
    index_name="binary_vector_index",
    index_type="BIN_FLAT",          # 二进制向量的精确搜索
    metric_type="HAMMING"           # 使用汉明距离
)

# 创建集合 (Collection)
client.create_collection(
    collection_name="fastest-rag",
    schema=schema,
    index_params=index_params
)

# 插入数据
client.insert(
    collection_name="fastest-rag",
    data=[
        {"context": context, "binary_vector": binary_embedding}
        for context, binary_embedding in zip(batch_context, binary_embeddings)
    ]
)
```![Milvus Indexing](https://storage.googleapis.com/gemini-prod/images/e99159d3-6490-4824-9b04-1b327b5f540c)

### **步骤 5：检索 - 毫秒级的相似性搜索**

在检索阶段，我们执行以下操作：
1.  将用户的查询同样进行嵌入和二进制量化。
2.  使用汉明距离在 Milvus 中执行相似性搜索。
3.  检索最相似的前 5 个文本区块。
4.  将检索到的文本区块整合为上下文。

```python
# 1. 生成 float32 查询嵌入
query_embedding = embed_model.get_query_embedding(query)
# 2. 对查询应用二进制量化
binary_query = binary_quantize(query_embedding)

# 3. 使用 Milvus 执行相似性搜索
search_results = client.search(
    collection_name="fastest-rag",
    data=[binary_query],
    anns_field="binary_vector",
    search_params={"metric_type": "HAMMING"},
    output_fields=["context"],
    limit=5  # 检索最相似的前 5 个区块
)

# 4. 存储检索到的上下文
full_context = []
for res in search_results:
    context = res["payload"]["context"]
    full_context.append(context)```![Milvus Similarity Search](https://storage.googleapis.com/gemini-prod/images/7a0f69f2-23c2-4028-a401-443b740523ec)

### **步骤 6：生成 - 结合 Kimi-K2 与 Groq 的智慧**

最后，我们使用 Kimi-K2 指令模型来构建生成管道。该模型由 Groq 提供支持，确保了极速的 AI 推理服务。我们在提示模板中整合查询和检索到的上下文，并将其传递给大型语言模型 (LLM)。

```python
from llama_index.llms.groq import Groq
from llama_index.core.base.llms.types import ChatMessage, MessageRole

# 初始化由 Groq 驱动的 LLM
llm = Groq(
    model="moonshotai/kimi-k2-instruct",
    api_key=groq_api_key,
    temperature=0.5,
    max_tokens=1000
)

# 定义提示模板
prompt_template = (
    "Context information is below.\n"
    "---------------------\n"
    "CONTEXT: {context}\n"
    "---------------------\n"

    "Given the context information above think step by step "
    "to answer the user's query in a crisp and concise manner. "
    "In case you don't know the answer say 'I don't know!'.\n"
    "QUERY: {query}\n"
    "ANSWER: "
)

# 格式化提示
prompt = prompt_template.format(context=full_context, query=query)
user_msg = ChatMessage(role=MessageRole.USER, content=prompt)

# 从 LLM 获取流式响应
streaming_response = llm.stream_complete(user_msg.content)
```![Groq LLM Generation](https://storage.googleapis.com/gemini-prod/images/76b71f92-c43c-4144-8c01-72f5d082833a)

### **步骤 7：使用 Beam 进行无服务器部署**

Beam 让任何 AI 工作流程的无服务器部署变得异常简单快速。我们将应用程序封装在一个 Streamlit 界面中，并在部署设置中指定所需的 Python 库和容器计算规格（如 GPU 和内存）。

只需几行代码，即可完成应用程序的部署。

```python
from beam import Image, Pod

# 定义应用程序的执行环境
streamlit_server = Pod(
    image=Image().add_python_packages([
        "streamlit",
        "pymilvus",
        "llama-index",
        "llama-index-embeddings-huggingface",
        "llama-index-llms-groq"
    ]),
    ports=[8501],  # Streamlit 的默认端口
    gpu="T4",
    memory="2Gi",
    entrypoint=["streamlit", "run", "app.py"],
)

# 创建并部署服务
res = streamlit_server.create()
```![Beam Deployment](https://storage.googleapis.com/gemini-prod/images/5f0f353a-c852-4a0b-9df2-a39fa2776c5b)

部署完成后，Beam 会启动容器并将我们的 Streamlit 应用部署到一个 HTTPS 服务器上，让用户可以通过网页浏览器轻松访问。

### **惊人的性能**

为了评估其真实的规模和推理速度，我们在 **PubMed 数据集（超过 3600 万个向量）** 上进行了测试。结果如下：

*   **在不到 30 毫秒内查询了超过 3600 万个向量。**
*   **在 1 秒内生成了完整的响应。**

这个基于二进制量化的 RAG 堆栈，结合了高效的检索和无服务器架构的快速部署，无疑是目前最快的解决方案之一。