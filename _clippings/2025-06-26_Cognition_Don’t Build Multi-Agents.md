---
title: "Don’t Build Multi-Agents"
source: "https://cognition.ai/blog/dont-build-multi-agents#applying-the-principles"
author:
  - "[[Cognition]]"
published: 2025-07-11
created: 2025-06-26
description: "Frameworks for LLM Agents have been surprisingly disappointing. I want to offer some principles for building agents based on our own trial & error, and explain why some tempting ideas are actually quite bad in practice."
tags:
  - "clippings"
---
## 不要构建多智能体

用于 LLM 智能体的框架一直出人意料地令人失望。我想根据我们自己的反复试验，提供一些构建智能体的原则，并解释为什么一些诱人的想法在实践中实际上相当糟糕。

2025-06-12 严华登

## 上下文工程原理

我们将逐步阐述以下原则：

1. 共享上下文
2. 行动蕴含着隐性决策

**为什么要思考原则？**

HTML 于 1993 年推出。2013 年，Facebook 向全世界发布了 React。如今到了 2025 年，React（及其衍生技术）主导着开发者构建网站和应用程序的方式。为什么呢？因为 React 不仅仅是一个编写代码的框架。它是一种理念。通过使用 React，你采用了一种响应式和模块化的模式来构建应用程序，如今人们已将其视为标准要求，但对于早期的网页开发者来说，这并不总是显而易见的。

在大语言模型（LLMs）和构建人工智能智能体的时代，感觉我们仍在摆弄原始的 HTML 和 CSS，并摸索如何将它们组合在一起以获得良好的体验。除了一些绝对基础的内容外，目前还没有一种构建智能体的单一方法成为标准。

> 在某些情况下，像 OpenAI 的 [https://github.com/openai/swarm](https://github.com/openai/swarm) 和微软的 [https://github.com/microsoft/autogen](https://github.com/microsoft/autogen) 这样的库积极推广一些我认为是构建智能体的错误方式的概念。也就是说，使用多智能体架构，我将解释原因。

话虽如此，如果你刚开始接触智能体构建，有很多关于如何搭建基本框架的资源\[[1](https://www.anthropic.com/engineering/building-effective-agents)\]\[[2](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf)\]。但当涉及到构建严肃的生产应用程序时，情况就不同了。

## 构建长期运行智能体的理论

让我们从可靠性说起。当智能体在长时间运行时必须真正做到可靠，并保持连贯的对话，你必须采取某些措施来控制复合错误的可能性。否则，如果你不小心，事情很快就会变得一团糟。可靠性的核心是上下文工程。

*上下文工程*

到 2025 年，市面上的模型将极其智能。但即使是最聪明的人，如果没有关于他们被要求做什么的背景信息，也无法有效地完成工作。“提示工程”这个术语是为了描述以 LLM 聊天机器人的理想格式编写任务所需的工作。“上下文工程”则是在此基础上更进一步。它是指在动态系统中自动完成这项工作。这需要更多的细微差别，实际上是构建人工智能代理的工程师的首要工作。

以一种常见类型的智能体为例。这个智能体

1. 将其工作分解为多个部分
2. 启动子代理来处理那些部分
3. 最终将那些结果结合起来
[![](https://cdn.sanity.io/images/2mc9cv2v/production/721e44474051c62156e15b5ffb1a249c996f0607-1404x1228.png)](https://cdn.sanity.io/images/2mc9cv2v/production/721e44474051c62156e15b5ffb1a249c996f0607-1404x1228.png)

这是一种很有吸引力的架构，特别是当你在一个具有多个并行组件的任务领域工作时。然而，它非常脆弱。关键的失败点在于：

> 假设你的 **任务** 是“制作一款《Flappy Bird》克隆游戏”。这又被细分为 **子任务 1** “构建一个带有绿色管道和碰撞盒的移动游戏背景”以及 **子任务 2** “构建一只可以上下移动的鸟”。  
>   
> 结果发现子代理1实际上误解了你的子任务，开始构建一个看起来像《超级马里奥兄弟》的背景。子代理2为你构建了一只鸟，但它看起来不像游戏资产，移动方式也与《飞扬的小鸟》中的鸟完全不同。现在，最终代理面临着将这两个错误传达的内容组合起来的棘手任务。

这可能看起来有些刻意，但大多数现实世界的任务都有许多细微差别层面，所有这些层面都有可能被误传达。你可能会认为一个简单的解决方案是将原始任务作为上下文也复制给子智能体。这样，它们就不会误解自己的子任务。但请记住，在实际的生产系统中，对话很可能是多轮的，智能体可能必须进行一些工具调用才能决定如何分解任务，而且任何数量的细节都可能对任务的解释产生影响。

> *原则1*  
> 共享上下文，并共享完整的智能体记录，而不仅仅是单个消息

让我们再次审视我们的智能体，这次要确保每个智能体都拥有前一个智能体的上下文信息。

[![](https://cdn.sanity.io/images/2mc9cv2v/production/e3bdf57c10a9b6c4531b93a10fb79a712464c712-1408x1232.png)](https://cdn.sanity.io/images/2mc9cv2v/production/e3bdf57c10a9b6c4531b93a10fb79a712464c712-1408x1232.png)

不幸的是，我们还未完全脱离困境。当你给你的智能体同样的《Flappy Bird》克隆任务时，这次，你可能最终得到一只鸟和背景，它们有着完全不同的视觉风格。子智能体 1 和子智能体 2 无法看到对方在做什么，所以它们的工作最终相互不一致。

子代理1采取的行动和子代理2采取的行动是基于事先未规定的相互冲突的假设。

> *原则2*  
> 行动蕴含着隐性决策，而相互冲突的决策会带来不良后果

我认为原则1和原则2非常关键，而且几乎不值得违反，以至于默认情况下你应该排除任何不遵守它们的智能体架构。你可能认为这具有局限性，但实际上仍有广泛的不同架构空间供你为智能体进行探索。

遵循这些原则的最简单方法是只使用一个单线程线性智能体：

[![](https://cdn.sanity.io/images/2mc9cv2v/production/06f64ae3557594588f702b2608d43564edc98c3d-1404x1230.png)](https://cdn.sanity.io/images/2mc9cv2v/production/06f64ae3557594588f702b2608d43564edc98c3d-1404x1230.png)

在这里，上下文是连续的。然而，对于有如此多子部分的非常大的任务，你可能会遇到问题，即上下文窗口开始溢出。

[![](https://cdn.sanity.io/images/2mc9cv2v/production/4a36b048810fb2cba4ee4055ed2d3c80f188befc-1394x1218.png)](https://cdn.sanity.io/images/2mc9cv2v/production/4a36b048810fb2cba4ee4055ed2d3c80f188befc-1394x1218.png)

说实话，简单的架构能让你走得很远，但对于那些有真正长期任务且愿意付出努力的人来说，你可以做得更好。有几种方法可以解决这个问题，但今天我只介绍一种：

[![](https://cdn.sanity.io/images/2mc9cv2v/production/836a7407ddf3dfacc0715c0502b4f3ffc7388829-1406x1230.png)](https://cdn.sanity.io/images/2mc9cv2v/production/836a7407ddf3dfacc0715c0502b4f3ffc7388829-1406x1230.png)

在这个世界里，我们引入了一种新的 LLM 模型，其主要目的是将一系列动作和对话历史压缩成关键细节、事件和决策。这很难做到恰到好处。这需要投入精力去弄清楚最终哪些是关键信息，并创建一个擅长此任务的系统。根据不同领域，你甚至可以考虑对一个较小的模型进行微调（实际上这是我们在 Cognition 所做的事情）。

你获得的好处是一个在处理更长上下文时有效的智能体。不过最终你仍然会遇到限制。对于热心的读者，我鼓励你思考更好的方法来管理任意长的上下文。这最终会变成一个相当深的无底洞！

## 应用这些原则

如果你是一个智能体构建者，要确保你的智能体的每一个行动都基于系统其他部分所做的所有相关决策的上下文信息。理想情况下，每一个行动都能了解到其他所有情况。不幸的是，由于上下文窗口有限和实际的权衡，这并不总是可行的，而且你可能需要根据你所追求的可靠性水平来决定愿意承担何种程度的复杂性。

当你思考如何构建你的智能体以避免冲突的决策时，这里有一些现实世界的例子供你思考：  
  
*克劳德代码子代理  
*截至 2025 年 6 月，Claude Code 是一个会生成子任务的智能体示例。然而，它从不与子任务智能体并行工作，并且子任务智能体通常只负责回答问题，不编写任何代码。为什么呢？子任务智能体缺乏主智能体的上下文信息，而要做回答明确问题之外的任何事情都需要这些上下文信息。而且如果它们要运行多个并行的子智能体，可能会给出相互冲突的回答，从而导致我们在早期智能体示例中看到的可靠性问题。在这种情况下使用子智能体的好处是，所有子智能体的调查工作不需要保留在主智能体的历史记录中，从而在上下文耗尽之前允许有更长的痕迹。Claude Code 的设计者采取了一种有意为之的简单方法。  
  
*编辑 应用 模型*  
在 2024 年，许多模型在编辑代码方面表现很差。编码代理、集成开发环境（IDE）、应用程序构建器等（包括德文）的常见做法是使用“编辑应用模型”。关键思想是，给定你想要的更改的 markdown 解释，让一个小模型重写你的整个文件，实际上比让一个大模型输出格式正确的差异更可靠。因此，构建者让大模型输出代码编辑的 markdown 解释，然后将这些 markdown 解释输入到小模型中以实际重写文件。然而，这些系统仍然存在很多问题。例如，小模型常常会误解大模型的指令，由于指令中最细微的歧义而进行错误的编辑。如今，编辑决策和应用更多地由单个模型一次性完成。

**多智能体**

如果我们真的想从系统中实现并行处理，你可能会想到让决策者们相互“交流”并解决问题。

这就是我们人类在意见不合时（在理想情况下）会做的事情。如果工程师 A 的代码与工程师 B 的代码引发了合并冲突，正确的做法是把分歧谈清楚并达成共识。然而，如今的智能体还不太能够像单个智能体那样可靠地进行这种长上下文的主动对话。人类在相互交流最重要的知识方面效率很高，但这种效率需要相当的智能。

自 ChatGPT 推出后不久，人们就一直在探索多个智能体相互协作以实现目标的想法\[[3](https://arxiv.org/abs/2304.03442)\]\[[4](https://github.com/FoundationAgents/MetaGPT)\]。虽然我对智能体之间协作的长期可能性持乐观态度，但很明显，在 2025 年，运行多个协作的智能体只会导致脆弱的系统。决策最终过于分散，并且智能体之间无法充分共享上下文。目前，我没看到有人专门致力于解决这个棘手的跨智能体上下文传递问题。我个人认为，随着我们让单线程智能体在与人类沟通方面变得更出色，这个问题将迎刃而解。当这一天到来时，它将释放出更高的并行性和效率。

**迈向更通用的理论**

这些关于上下文工程的观察仅仅是我们有朝一日可能会视为构建智能体标准原则的开端。还有许多挑战和技术在此未作讨论。在认知公司，构建智能体是我们所思考的一个关键前沿领域。我们围绕这些我们不断重新学习的原则来构建内部工具和框架，以此来强化这些理念。但我们的理论可能并不完美，而且我们预计随着该领域的发展情况会有所变化，所以也需要一定的灵活性和谦逊态度。