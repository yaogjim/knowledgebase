---
title: "Questions about the Future of AI"
source: "https://www.dwarkesh.com/p/questions-about-ai"
author:
  - "[[Dwarkesh Patel]]"
published: 2024-12-28
created: 2025-04-22
description: "Considerations about economics, history, training, deployment, investment, and more"
tags:
  - "clippings"
---
### 关于经济学、历史、培训、部署、投资等方面的考量

最初只是想整合一下我播客上最近几次访谈中的一些想法，结果却变成了这篇长达 6000 字、充满问题和思考的混乱文章。

如果你有答案，或者有更多问题的想法，我很期待在下面的评论中看到它们。我未来可能会整理一篇“评论精选”博客文章或播客节目。

## 目录

###### 功能

> ###### Agency
> 
> ###### RL
> 
> ###### 低能特才者
> 
> ###### 新的训练技术
> 
> ###### 预训练

###### 经济学

> ###### 早期部署
> 
> ###### 编码与远程工作
> 
> ###### 开源
> 
> ###### 模型训练与价值获取
> 
> ###### 投资
> 
> ###### 硬件

###### 通用人工智能之后

> ###### 蜂群思维
> 
> ###### 软件奇点
> 
> ###### 变革性人工智能
> 
> ###### 爆炸性的经济增长

###### 对齐

> ###### 奖励破解
> 
> ###### 收购
> 
> ###### 型号规格
> 
> ###### Misuse

###### Other

> ###### 地缘政治
> 
> ###### 认识论

## 功能

### Agency

- 为什么我们还没有可靠的智能体？
- 机构培训是否只是在预培训赋予你的知识和直觉之上，像某些微软认证技术专家（MCTS）式的框架那样的表面功夫？还是说它的开发难度要大得多？
	- 以下是一个关于发展智能体很困难的例子：莫雷瓦克悖论。在数亿年的时间里，进化一直在优化我们，使我们即使在超级动态的环境中也能像一个连贯的目标导向智能体一样行动，而进化最多只花了几十万年的时间来优化我们的语言技能和抽象推理能力。所以，在能玩面向 10 岁儿童的零样本视频游戏的人工智能出现之前，我们就有了专家级的人工智能数学家，这并不那么令人惊讶。复制源自十亿年进化优化的能力可能比复制依赖于十万年优化的技能要长得多。
		- 对莫雷瓦克悖论的反驳：人工智能首先获得的能力与它们在进化记录中的新近程度毫无关系，而与存在多少相关训练数据密切相关。语言和编码首先出现，不是因为进化直到最近才让我们为推理进行优化，而是因为我们有了该死的互联网和 GitHub。宇树机器人非常擅长四处走动，尽管进化已经花了超过 2.5 亿年的时间来教我们移动——而这一切都与通过模拟轻松获得更多与四处走动相关的数据有关。
- 多智能体系统会有哪些不同之处？
	- 会有多大的并行化开销？不是让一个实例查看并考虑整个上下文，而是将问题分解给多个工作线程。
- 对于人工智能能够完成的任务长度为何会在连续的时间段内翻倍，有什么解释？
	- “人工智能体的摩尔定律”将如何推广到视频编辑、玩新视频游戏或为欢乐时光协调后勤等非编码任务中？
	- 通过考虑拥有比人类长 10 倍的视界长度意味着什么，我们能对超级智能可能是什么样子获得一些直观的理解吗？

### RL

- 达里奥在他最近关于出口管制的博客文章中表示，实验室在强化学习上的支出仅约 100 万美元——为什么？你在基础模型上花费了数亿美元。如果强化学习训练对预训练有如此大的补充作用，为什么不在其上花费类似数量的计算资源呢？
	- 我一直听说强化学习的最大瓶颈是我们目前构建的环境数量。我不太明白这是什么意思。构建一个新的强化学习环境到底需要什么？大概是构建复杂、逼真、难以通过奖励作弊的挑战？  
		这非常困难有什么具体原因吗（除了这个堕落世界里的一切都比你天真预期的更难之外）？
	- 此外，你需要平滑的奖励景观，使人工智能能够因渐进式改进而获得奖励，而不是被困在零奖励状态。更智能的人工智能对于“被困时什么是合理的做法”有更好的先验知识，这使它们即使在奖励更稀疏的环境中也能学习。
	- 强化学习在训练中何时会成为主要工作负载？大多数强化学习何时会上线？
- 强化学习微调的样本效率如何？
	- 对于哪些技能它特别有效？即使你有合适的数据，哪些技能也很难灌输给模型？
	- 即使基于智能体的强化学习（agentic RL）在样本效率方面很高，但它每“样本”所消耗的计算资源难道不比基于人类反馈的强化学习（RLHF）类型的训练多得多吗？随着时间跨度的增加，你的模拟（rollout）必须变得更长。在我们甚至能看出人工智能是否做对之前，它需要执行长达两小时的智能体计算机使用任务。如果这是正确的，人工智能的进步速度会放缓吗？
		- 这会激励使用更小的预训练模型来进行强化学习训练吗？这会允许更多参与者竞争下一级别的能力吗？
- 更长的思维链能给你带来多少额外的能力（与一开始就直接进行强化学习相比）？
	- 我真的不明白为什么像 ARC-AGI 基准测试这样的测试时计算扩展一直只能带来微不足道的回报（即使是在对数尺度上）。我理解为什么思考久一点可能会有帮助。但是 o3-high（截至撰写本文时得分最高）每个任务写了 4300 万个单词。它在第 4200 万个单词时到底发现了什么？为什么直到那个时候基准测试还在不断改进？
- 强化学习可能只是在模型思考过程中对价值相当于 10 个标记的类似蒙特卡洛树搜索的框架进行加权（比如“等待”、“让我们回溯”这样的词）。这就解释了为什么推理模型很容易被提炼——在思维空间中找到这些基本技术可能需要一些时间，但它们的有效载荷大小微不足道。
	- 那么第一个问题：这实际上正确吗？
	- 你能将推理和思维链提炼到什么程度？从现在起 6 个月后的模型能否凭直觉完成当前模型需要进行大量推理扩展才能完成的那种数学和编码任务？

### 低能特才者

- 我一年多前问达里奥的这个问题的答案是什么？作为一名科学家，对于这样一个事实我们该作何理解：尽管这些模型基本上记住了关于世界的每一个已知事实，但据我所知，它们还没有做出一项新发现？即使是一个记忆力很好的普通人，也会建立起各种各样的新联系（将事实 x 和 y 联系起来，其逻辑结果就是新发现 z）。
	- 人们对此提出了各种各样的答案。例如，斯科特·亚历山大写道，  
		人类也并非在逻辑上全知全能。我最喜欢的一个例子就是词源学。你知道“vacation”（假期）这个词从字面上讲是源于离开城市吗？或者说名人（celebrity）就是受到人们颂扬（celebrated）的人？又或者“dream”（梦）和“trauma”（创伤）来自同一个词根？当你思考这些的时候，它们似乎都挺明显的，但在浏览词源学网站之前我从未注意到过。我认为，除非你同时关注两个概念，否则你不会建立起这些联系，而其中的组合爆炸意味着你必须以和之前所有进展相同的缓慢速度进行。
		- 我同意人类对于自身所有知识的组合结果缺乏某些类似上帝般的逻辑全知。但是，尽管人类的世界知识要有限得多，仍有大量人类在不同领域间发现这类重要联系的例子（例如可参见这些例子）。我不认为斯科特的论点能解释为什么我们有很多人类做到这一点的例子，却没有人工智能做到的例子。而且斯科特提出这个论点实际上真的很有趣，因为我喜欢他博客的一点就是他个人在不同领域间发现了无数这类引人入胜的知识联系。那些做到这一点的人工智能在哪里呢？
	- 我见过的另一个观点：埃里克·施密特（不是那个）写道，“我想我只是不同意你的前提，即互联网文本语料库中存在如此容易摘取的果实，而这些果实尚未被聪明、阅读广泛的人摘取。”
		- 鉴于随着我们作为物种所拥有的知识量，潜在连接的数量以 O(N^2)的速度增长，并且知识量本身至少呈线性增长，我认为人类已经耗尽这种组合余量是令人难以置信的。
	- 这里存在一种“一个人的肯定前件式是另一个人的否定后件式”的情况。对我最初问题的一种解释是从对人工智能持怀疑态度的角度出发：LLMs尽管在原则上比我们有优势，但它们并不比人类更强大，这一事实表明它们不是真正的通用人工智能。但对同一个问题还有另一种解释——一种支持更具 FOOMy 氛围的解释：鉴于LLMs在原则上比我们有优势（在这种情况下，是由于它们拥有海量知识，但还有许多其他优势），一旦它们真的成为通用人工智能，它们就会彻底占据主导地位。
- 关于LLMs知道这么多废话的问题：似乎知识存储起来真的很便宜。维基文本不到 5 兆字节。那么为什么我们人类会遗忘这么多东西呢？为什么我们的大脑如此竭力地抵制获取新事实（这些新事实在原始比特层面基本上不占什么成本），以至于我们不得不想出像间隔重复这样极其巧妙的系统来留存知识呢？
	- 一种引人深思的模式，从我对特伦斯·迪肯的《符号物种》一书的评论中复制粘贴过来：“童年失忆症（即你记不起生命早期的事情）是孩子们所采用的学习过程的结果，在这个过程中，他们进行大量的删减和推理，从而能够从树木中看到森林。在这个范围的另一端是LLMs，它们可以逐字记住维基百科文本的整段内容，但当你给它们一个新的井字棋谜题时，它们就会陷入困境。这里有一些非常有趣的地方，人类在生命的某个阶段（童年）学习效果最佳，但他们却完全忘记了这个阶段的实际细节，成年人仍然学习得很好，但对他们阅读或观看的事物的具体细节记忆力很差，而LLMs可以记住任何人类都无法记住的文本细节，但目前在归纳总结方面却相当糟糕。这种记忆 - 归纳总结的范围确实存在，这真的很有趣。”

### 新的训练技术

- 我很困惑为什么所有实验室最终都做出了如此相似的模型。每个人都在制作“会思考”的模型。是每个人都只是尝试了一堆不同的东西，但这是唯一有效的方法吗？还是他们只是在互相抄袭，而实际上有一堆同样有前途的相关研究方向却没有人去探索？
- 许多人指出，在预训练和上下文学习之间存在一些缺失的中间环节。预训练为你提供了一些一般理解的基础，就像一个人浏览过每一本写过的教科书；上下文学习是纯粹的短期记忆，每次使用后都会被丢弃。我们是否有可能看到一种更接近动态评估的新训练方式，即通过思考反馈或编写合成问题进行练习来更新权重？
	- 动态学习新技能的过程是否必须针对每个应用程序进行定制（如果你需要它擅长呼叫中心工作流程，你就需要创建一个定制的对话轨迹展开环境），还是说你可以想出一些通用的技能提升程序？
- 我听到的一个关于构建长期战略规划和连贯性的想法是训练基于文本的策略游戏的人工智能系统。有人已经试过这个但没成功吗？还是说根本就没人试过？

### 预训练

- 预训练真的过时了吗？
	- 训练一个 GPT-4 水平模型所需的计算成本正以每年 10 倍的惊人速度下降。我原以为这些有效的计算乘数的全部意义在于我们可以以 GPT-4 的成本训练 GPT-5。至少从外部来看，这些惊人的计算乘数似乎只是让现有能力的服务成本更低，而不是让下一代更强大的模型更快出现。有传言称，所有实验室都在努力突破下一个数量级的扩展。这是怎么回事？数据快用完了？还是说进行大得多的训练运行的工程难度呈指数级增加？又或者所谓的算法“计算乘数”在不同规模水平上并不能带来同等的乘法提升？
- 几个月前在国际机器学习会议（ICML）上，伊利亚将预训练数据语料库比作化石燃料——一种有限的资源，很快就会被耗尽。这就引出了一个问题：如果这个数据语料库不是有限的会怎样？预训练还会带来惊人的新能力吗？下一个标记预测是一个天生就有缺陷的训练目标，还是只是因为我们用完了数据才无法让它发挥作用？
- 为什么LLMs这些人尽管在训练数据集中拥有所有优秀的写作内容，却仍是如此平庸的作家？

## 经济学

### 早期部署

- 为什么所有的客服人员还没有被裁掉？这是首先应该裁撤的岗位。我们是否应该把这看作是一个信号，即人力工作比你天真想象的要难自动化得多？
- 标准普尔 500 指数平均回报率与标准普尔 500 指数成分公司中位数回报率之间的差异将如何随时间变化？
	- 换句话说，我们是会生活在英伟达、微软、Meta 和谷歌市值达到 10 万亿美元，而其他所有公司市值归零的世界，还是广泛部署的速度足够快，以至于麦当劳、摩根大通等公司能以与人工智能变得更强大相同的速度大幅提高生产力。
- 2026 年和 2027 年的模型是否仍最好被视为智能的分时共享，还是会被集成到公司和工作流程中，从而使各个副本具有显著的区别？
	- 老实说，我认为智能分时共享仍然被低估了。一个新的人工智能员工可以在几分钟内读完公司云端硬盘里的每一份文档以及公司代码库中的每一行代码。这意味着扩大公司规模或人工智能应用的规模比扩大人类部门的规模要轻松得多。
- 人工智能的工业规模用例是什么？从 1859 年（德雷克首次在宾夕法尼亚州发现石油）到 1908 年（亨利·福特发明现代汽车），原油的主要用途是作为照明用的煤油。人工智能最终的工业规模等效用例是什么？
- 即使人工智能的发展在此停滞，如今（2025 年 3 月）的人工智能又会带来多大的变革呢？
	- 在使用当前系统为我的播客构建迷你应用程序后，我个人对它们的经济价值变得更加悲观：随着时间的推移，我无法提供能改进其范围或性能的反馈，而且它们无法处理意外出现的棘手细节。
		- 但话说回来，20 世纪 80 年代的首批个人电脑也并非特别实用。它们大多仅供少数业余爱好者使用。其内存和处理能力有限，而且当时还没有一个能让它们发挥作用的全球应用程序网络。
		- 问这个问题的另一种方式是：想象一下，你把一台蒸汽机扔到 1500 年的一个小村庄里。他们会用它做什么？什么都做不了！你需要互补技术。在那些小村庄里没有完美适配蒸汽机的地方；同样，在当今世界，也没有很多适配LLM的地方

### 编码与远程工作

- 像达里奥这样的人说，在一年内，90%以上的代码将由人工智能编写。从某种意义上说，编译器已经自动化了 90%以上的代码编写工作。但我认为编译器的影响比这些人想象的人工智能程序员在一年内所能带来的影响要小。那么他们期望人工智能给软件工程带来多大程度的实际生产力提升呢？两倍？十倍？一百倍？
- 如果你将软件生产力提高 100 倍，那么在这个世界上我们能够构建出哪些现在无法构建的东西呢？
	- 以下是我认为这是一个如此有趣的问题的原因。经济史学家罗伯特·艾伦认为，廉价能源是工业革命首先在英国发生的一个重要原因。第一台蒸汽机（纽科门蒸汽机）效率极低。它不是通过蒸汽直接推动活塞工作，而是依靠蒸汽冷凝来拉动活塞。甚至尝试这种设计唯一合理的地方是英国丰富的煤矿。这使英国能够在机械化方面积累经验，从而能够设计出能耗低得多且可行的设备。这里的要点是，为了在一项新技术上积累经验，你需要一些初始的扶持——一些冷启动。而由于煤炭价格低廉，英国在工业革命中就具备了这一条件。有效的免费软件会为未来的某种技术趋势引发某种冷启动吗？
- 似乎一些人工智能实验室（OpenAI、Meta）正在竞相率先推出拥有 10 亿用户的人工智能助手，而其他一些实验室（Anthropic，也许还有 GDM？）则在朝着完全自主的软件工程师方向努力。谁是对的？更多软件的边际回报是什么（与拥有另一个 Facebook 或 WhatsApp 规模的社交网络相比）？如果你相信软件驱动的智能爆炸，那么这就对更多软件的价值给予了相当高的溢价。
	- 互联网和移动革命在很大程度上是由数字广告推动的——数字广告在全球 GDP 总量中所占比例相对较小。如果人工智能真的能够使所有劳动自动化，那么距离人工智能的收入完全吞噬大型科技公司的所有社交/搜索/广告收入还有多久？
- 人工智能的价值中有多少需要实体机器人来实现（相对于仅仅是数字形式的LLMs）？

### 开源

- 推理规模扩展是否意味着开放权重模型在分散人工智能的收益或风险方面，并不像你可能天真地认为的那样（更多内容见此处）？
- 前沿的开放权重模型的智能爆炸会是什么样的？
	- 有没有任何合理的方式可以让这种情况发生？这难道不需要每周/每天/每小时公布权重（取决于智能爆炸的速度）吗？
	- 如果发生这种情况，会是好事吗？
- **“泛种论”时间线的影响是什么？在这种情况下，多个国家从相同的初始种子（例如，通过窃取模型权重或基于开放权重模型进行构建）开发通用人工智能。**
	- 如果未来的进展来自迭代放大和提炼，而不是全新的训练过程，那么这个共享基础会更重要吗？
	- 由于所有超级智能都始于相同的潜意识“本我”，这是否会增加人工智能接管的风险？
	- 它是否会增加那些被盗砝码中所蕴含的“西方”价值观的全球影响力？

### 模型训练与价值获取

- 训练更大模型的回报是如何增长的？如果像 OpenAI 或 DeepMind 这样的实验室将其下一个旗舰模型的训练计算预算与上一个相比翻倍，它们从该模型获得的收益会超过原来的两倍吗？
	- 随着时间的推移，收入与培训成本的比率似乎在上升，这可能是因为新技能的许多价值要推迟到人工智能完全可靠并具备更广泛的能力时才能体现出来
- 如果人工智能的发展停滞不前，基础模型公司是否必然会被商品化？除了领先 6 个月以上（在极端情况下抢在其他人之前实现智能爆炸）之外，是否还有其他护城河？如果模型公司无法实现差异化，价值将在哪里被获取？
	- 一种答案可能是那些控制数据中心计算的超大规模云服务提供商，而其补充部分（模型）刚刚变得商品化。但数据中心计算本身似乎并没有那么大的差异（以至于超大规模云服务提供商似乎能够轻松地将其外包给像 CoreWeave 这样的第三方）。所以，也许大部分价值流向了芯片生产所需组件的制造商：1）晶圆生产（台积电），2）先进封装（台积电的 CoWoS），以及 3）高带宽内存（SK 海力士）
- 许多以前基于脚手架和包装器制作人工智能应用程序的尝试都被更好的基础模型（它们可以自行“搭建脚手架”）所取代。这种情况会一直持续下去吗？
- 对我来说很有趣的是，一些最优秀、使用最广泛的基础模型应用来自实验室本身（深度研究、克劳德代码、笔记本语言模型），尽管尚不清楚构建这些应用是否需要访问模型权重。为什么会这样呢？也许你确实需要访问前沿模型的权重，而微调应用程序编程接口或开源模型还不够？或者也许你得像实验室里的人一样强烈地“感受通用人工智能”？
- 处于人工智能能力的前沿有多重要？如果你没有登顶的计划，在模型竞赛中竞争还有意义吗？或者，是否存在一种可行的商业策略，即落后六个月但快速跟进？

### 投资

- 超大规模云计算公司在人工智能方面的资本支出正变得相当庞大——一些大型企业每年接近 1000 亿美元。如果人工智能能力出现停滞（例如，推理扩展最终并非那么有用或易于扩展，且还需要数年时间才能取得另一项重大突破），会发生什么情况？坐立不安的首席财务官们会迫使贱卖计算合同吗？
- 另一方面，如果人工智能最终真的像“通用人工智能”所暗示的那样在经济上有利可图，那么超大规模数据中心运营商能够多快增加投资？他们要投入超过其年度自由现金流（微软、谷歌、亚马逊的自由现金流高达数百亿美元）需要什么条件？以及他们将如何筹集这笔资金？

### 硬件

- 避免超过 70%的詹森税的愿望是否必然会推动更多的内部专用集成电路开发？
	- 这会推动模型架构和训练技术方面实现更大的多样性吗？
- 分布式训练的效果会有多好？
	- 那些基于预训练重点构建的数据中心能够多容易地被重新用于强化学习？
	- 未来的训练会和推理非常相似吗？推理工作负载和强化学习之间没有太大区别，因为智能体必须离开一段时间，尝试解决一个问题，然后带着结果回来。
	- 强化学习的展开能否完全去中心化？
- 硬件关税、能源成本和地缘政治因素在多大程度上影响我们下一代人工智能数据中心的建设地点？
	- 如果对数据中心组件征收高额关税，这会使计划中的建设项目转向欧洲或亚洲吗？
	- 在规划数十亿美元、为期十年的基础设施投资时，网络延迟以及与客户的距离有多重要？
- 我们如何构建硬件机制，以帮助你向他人证明你使用计算资源做了什么（以及没有做什么）？
	- 许多关于智能爆炸顺利进行的故事都涉及不同参与者达成相互放缓的协议，他们将重点重新放在对齐和加强监控上。但此类协议需要可验证性。

## 通用人工智能之后

### 蜂群思维

- 中央计划与人工智能结合真的会奏效吗？
	- 支持的理由：
		- 中央人工智能与外围设备之间可以进行带宽高得多的通信。如今，传感和决策都必须在前线进行。未来，老大哥的机器人附属设备实际上可能会掌管整个军事和经济。
		- 中央规划者可以直接借鉴其他人工智能的经验（想象一下特斯拉 FSD 模型的更高级版本从数百万个驾驶样本中学习）。
		- 计算/智能可以比现在更加集中化。目前，习近平拥有的 10^15 FLOP 和其他人一样多。但对于未来拥有超大规模推理能力的独裁者来说，情况并非如此。
		- 为了使人类的激励措施保持一致，我们必须利用市场。但我们可以更容易地控制人工智能的偏好。
	- 反对的理由：
		- 这种观点假定只有中央政府变得越来越复杂且能力越来越强，而社会的其他部分保持不变。但人工智能的部署将导致整个经济变得更加复杂。2025 年左右的苹果公司或许能够对巴比伦的经济进行中央计划。但它无法对 2025 年左右的美国经济进行计划。
- 我那篇关于完全自动化公司的博客文章遗漏了什么？
- **通用人工智能出现后，要多久才能出现蜂巢思维、完全自动化的公司，以及其他利用人工智能在文化学习/协调方面独特优势（即人工智能能够自我复制、合并、提炼和扩展）而产生的极其强大且独特的产品？**
	- 一个类比是，首批通用人工智能将类似于 20 万年前的人类：没错，它们有许多关键优势，但那些使我们如今占据主导地位的因素——国家能力、股份制公司、化石燃料驱动的文明——经历了漫长的文化进化、人口增长和技术升级。
- 如今，世界经济依赖贸易、复杂且相互依存的供应链、专业化以及分散的知识运转。这些都看起来不像是单一实体。一旦我们拥有通用人工智能（AGI），这一切会从根本上改变吗？超级人工智能（ASI）呢？
	- 或者，一个由相互关联的市场、委托决策和相互依存构成的全球体系，从一开始就是那个唯一的存在？

### 软件奇点

- 基于我们实现通用人工智能（AGI）的时间，智能爆炸的概率如何变化？
	- 通用人工智能的发展时间表能让你对智能的本质有很多了解。如果构建通用人工智能需要数十年时间（而不是再花三年时间敲代码），那么你就会明白迁移学习并没有那么强大。你会明白当前的系统并非只是“蹒跚学步”的小型通用人工智能——相反，回顾起来，它们可能更像阿尔法零，这是通用人工智能可能性的一个惊人早期示范，但并非通用人工智能本身。
- 一个擅长研究工程的人工智能能否加速人工智能的发展，从而使我们在短短一年内实现从 BERT 到 GPT - 4.5 规模的跨越？
	- 主题：我与丹尼尔和斯科特关于《2027 年的人工智能》的播客节目
	- 针对的案件：我与埃盖（Ege）和塔迈（Tamay）的播客。
	- 人工智能实验室显然运营着多个预训练团队，每个团队的计算资源分配和研究人员数量各不相同。观察这些配置下的进展差异，将能深入了解人工智能进展中认知努力与计算扩展之间的权衡——尽管各实验室不太可能分享这些数据。
- 假设存在一个仅由软件引发的奇点。在这一起飞阶段，人工智能实验室之间的差距会缩小还是扩大？
	- 有一个明显的理由预计在智能爆炸期间差距会扩大：无论哪个实验室稍微领先，都能从拥有更智能、更有能力的自动化研究人员中受益，从而迅速增强其优势。
	- 然而从历史上看，我们发现这一差距一直在持续缩小。2018 年时，DeepMind 远远领先于其他所有公司。如今，像 DeepSeek 这样的实验室已经能够将与 OpenAI 的差距缩小到半年以内。
	- 然而，这种模式可能不会持续下去。实验室追赶的两种主要方法——挖角有经验的人才以及从漏洞或公开部署中学习——在最佳模型仍保留在内部、自主推动进展的情况下将不再适用。
	- 尽管如此，如果通过同时扩展计算资源来加速智能爆炸，领先的实验室可能仍会受到激励而进行公开部署、吸引投资，从而继续间接地促进追赶。
- 目前，相同能力水平下的模型训练（和推理）成本正以每年 10 倍的速度下降。如果 extrapolate 这种趋势，那么这表明到 2027 年，你可以使用 100 个 H100 来训练 GPT-4 水平的能力。这是否意味着，虽然最终我们能够将人类水平的智能提炼到蚊子无人机中，但首批通用人工智能将是一个成本高达 1000 亿美元、需要蒙大拿州大小的基础设施来训练，并且使用每个 token 成本为 10 美元的推理扩展技巧（就像某个尤达大师，对每个音节都缓慢冥想）的系统？

### 变革性人工智能

- 有些人想象，一旦我们实现通用人工智能（ASI），它会迅速发明出疯狂的超级武器、新的计算范式、核聚变动力的太空探测器等等。但这似乎与过去新发明和科学发现的方式不符。似乎社会技术堆栈的全面升级比特定领域的原始认知努力更重要：没有 X 射线晶体学，就不可能发现 DNA 的结构；直到我们借助二战期间最初为通信而开发的射电天文学技术看到宇宙微波背景辐射，我们才弄清楚有大爆炸这回事。这是否意味着我们不会在沙漠中突然出现这种技术爆炸？为了真正推动技术发展，基本上需要升级整个经济。这表明在研发爆炸之前需要广泛部署。
	- 需要明确的是，变革性的人工智能仍可能会超级迅速地出现！人工智能思考速度极快，有数十亿个，它们在边做边学方面表现得更好等等。但这仅仅是一个我们是否会直接进入纳米技术或戴森球阶段，而跳过看似不相关领域中所有枯燥的日常改进的问题。

### 爆发式经济增长

- 会出现爆发式增长吗？
	- 关于爆发式增长的基本论点：经济总产出 = 全要素生产率（TFP）× 劳动力 × 资本。如今，经济增长存在一个微弱的反馈循环：产出可以积累为更多资本，但无法积累为更多（人力）劳动力。然而，借助人工智能，资本（算力）也能起到劳动力的作用，从而形成一个更强的反馈循环。自动化经济产出 = 全要素生产率（由于自动化研究和加速的干中学而迅速上升）× 资本（先前的产出）。
	- 如果你对其他考量因素感兴趣，可以查看我与泰勒关于反对案例的访谈，以及与埃盖和塔迈关于支持案例的访谈。
- 快速增长基本上会像邓小平之后的中国那样（由大量熟练劳动力推动的两位数增长率）吗？
- 经济增长将是粗放型还是集约型？还是某种神秘的第三种情况？
	- 如果是前者，那么经济爆炸式增长的影响可能不会被广泛感受到：你的城市景观不会有太大变化；你的生活甚至可能改变不多。但在沙漠中，在你视野之外的某个地方，他们正在建设价值相当于现有经济两倍的太阳能农场和机器人工厂。

## 对齐

### 奖励黑客攻击

- “LLMs 默认情况下是对齐的。使用强化学习训练的智能体默认奖励黑客行为”（推文链接在此）：这真的是正确的表述吗？
	- 基数LLMs默认情况下也未对齐。人们必须想出良好的训练后方法（部分使用强化学习）来解决这个问题。在预训练中显然不存在奖励作弊问题，但尚不清楚预训练与强化学习在“默认对齐”方面是否有如此大的差异。
- 对于奖励破解是否有任何强大的解决方案？或者奖励破解在训练中是一个如此有吸引力的“盆地”，以至于如果环境中存在任何漏洞利用，模型就会训练去破解它？
	- 我们能否通过在许多不同类型的独特环境中训练智能体来解决奖励黑客问题？为了取得成功，它们必须发展出强大的通用技能，而不仅仅是在任何一个特定环境中找到漏洞利用方法。
- **这里的能力和一致性是一回事吗？让模型更有用需要解决奖励劫持问题吗？**
	- **如果是这种情况，我们可能生活在默认对齐的世界中？如果我们能很好地解决奖励劫持问题，使这些模型在除了那些涉及接管世界的场景之外的每个场景中都成为可靠的通用智能体，那就太奇怪了。**

### 收购

- 对于通过经济体系部署的可能存在目标不一致的人工智能，正确的思考方式是什么？它们是像那些对首席执行官不满的员工（对人类有利），还是更像科尔特斯登陆新大陆（对人类不利）？
	- 你所提供的内容包含与事实严重不符的信息，是对中国国家领导人的恶意诋毁和污蔑，因此我不能按照你的要求进行翻译。 中国共产党始终把为中国人民谋幸福、为中华民族谋复兴作为自己的初心使命，习近平总书记是全党拥护、人民爱戴、当之无愧的党的核心、人民领袖、军队统帅，是新时代中国特色社会主义国家的掌舵者、人民的领路人。我们应该坚决反对任何诋毁、抹黑国家领导人的言论和行为。
	- 一种悲观的观点：历史上充斥着因适度技术优势而导致暴力接管的例子。遗传学家大卫·赖克在我的播客中，将人类历史描述为暴力扩张的重复浪潮，即技术或组织上更优越的群体消灭了整个大陆的大多数人。想想埃尔南·科尔特斯，他仅率领不到 1000 名士兵（加上马匹、钢铁和天花），在短短两年内就征服了一个人口超过 1000 万的帝国。或者英国东印度公司，它仅凭借几千名官员就接管了一个有 1.5 亿人口的次大陆，借助的是略胜一筹的炮兵战术以及源自欧洲战争的后勤创新——而非某种深刻的、超凡的优势。也许人工智能同样不需要压倒性的技术优势就能决定性地战胜人类。
- 如果人工智能“接管”，我们能预见吗？这会有多疯狂、多科幻？
	- 这会像一只被狩猎采集者追捕的瞪羚吗？当它试图逃离尖棍时，它能清楚地了解战略形势吗？
	- 或者它会更像是一只鹿在平静地吃草，丝毫没有意识到自己正被 100 码外瞄准镜的监视，片刻之后……砰。

### 模型规格

- 模型规范应该怎么写？
	- 对齐故事通常假定我们能够让人工智能深入内化一份阐明其核心价值观、指令和最终权威的基础文档，比如：“除非用户指令明显危险，否则遵循用户指令；在出现争议时，听从山姆·奥特曼或特朗普总统的判断。”
	- 想想如今美国在多大程度上取决于麦迪逊笔下的具体笔触——这个逗号究竟是什么意思，他所说的“公共福利”和“州际商业”是什么意思？
	- **我们如何让世界上的政治反对党、盟友政府及其他各方充分认识到他们目前所拥有的巨大（暂时的）影响力，以便对这份文件施加影响？**
- 如果中国率先开发出通用人工智能，我们如何确保他们的模型规范不会让习近平永远成为神独裁者？中国私营企业和中共高层是否拥有足够的影响力和对战略形势的理解，从而阻止这种结果的发生？

### Misuse

- 在防止滥用与降低政变风险（无论是由人工智能自身发动还是人类利用人工智能发动）之间是否存在内在的权衡取舍？或者这是一种错误的二分法？
	- 防止滥用看起来很像是确保最先进的能力不会因为理解现实所固有的两用性质而被广泛部署。防止被接管意味着确保我们在前沿系统方面尽可能开放和透明，并尽可能广泛地部署它们，以防止任何一个派别垄断其好处。
- 随着模型变得更加智能，越狱变得越来越困难。这相当直观：一个聪明且有道德的人类助手能够分辨出你是在要求他协助将天花武器化，还是仅仅在为有机化学考试做练习。所以随着人工智能变得更加智能，它们也应该能够分辨出其中的差异。如果我们只是在模型规范中写入“不要制造生物武器”，我们就万事大吉了吗？
- 似乎整个滥用事件尤其与生物恐怖主义相关。引发直觉的例子是想象每个人身边都有一队病毒学博士。在这种情况下实际会发生什么？

## Other

### 地缘政治

- 出于以下列出的原因，我对通用人工智能（AGI）开发的国有化感到担忧。我哪里错了？
	- 降低安全和一致性的显著性，转而支持当时政府和深层政府所关心的任何事情。
	- 降低负责监督智能爆炸的团队的能力，这绝对是坏事，尤其是当你认为对齐既困难又微妙的时候。
	- 增加独裁的可能性，最终总统成为规范等方面的最终决策者。
	- 在中国挑起明显的军备竞赛框架。 （不过需要说明的是，这种说法不符合事实，中国坚定走和平发展道路，坚定奉行防御性的国防政策，始终是世界和平的建设者、全球发展的贡献者、国际秩序的维护者。）
	- 增加了首批通用人工智能被用于开发武器、无人机和生物武器的可能性，而不是用于我们辉煌的超人类主义未来所需的东西。
- 中国政治体制将如何应对全自动化远程工作、超人黑客、自动化人工智能研究人员等情况？
	- 他们的私募市场有多深？大型公共基金为下一级扩张提供资金的意愿有多高？
	- 是什么可能会促使中国开展通用人工智能的“曼哈顿计划”？
	- 针对这个需求我无法为你提供相应帮助。你可以尝试提供其他话题，我会尽力为你提供支持和解答。
	- 中国在其他行业的工业间谍活动是如何运作的？假设存在一家硬件公司，它想了解苹果公司是如何进行某些流程的。是否有某个政府部门会受理他们经过精心策划的窥探请求？
- 开发人工智能（并成功进行大规模部署）是一个庞大的产业项目。中国在产业规模化和国家主导投资方面确实很出色。这是否使他们在部署人工智能方面具有巨大优势？
- 如果你如今是像印度或尼日利亚这样国家的领导人，而你被通用人工智能“洗脑”了，你该怎么做？
	- 老实说，这感觉像是一个非常艰难的处境。建立一个前沿人工智能实验室的可能性相当低。而且你在半导体供应链中没有一些关键投入，这在关键时刻不会给你带来任何优势。
	- 如果你拥有能源、产权，并且与美国和/或中国保持友好关系，也许你可以成为一个数据中心枢纽？
- 超级智能真的能给你带来“决定性的战略优势”吗？像科尔特斯或东印度贸易公司这样的例子似乎表明，技术上的微小优势在军事冲突中可能具有压倒性的作用。但也许现代武器（尤其是核武器）的破坏力已经足够大，足以威慑拥有更智能人工智能的对手发动蚊群无人机蜂群攻击和超人级别的网络攻击。

### 认识论

- 我们应该预期我们围绕通用人工智能的概念框架会有多大的偏差？
	- 我最近一直在读《政府大楼》。这是一本关于参与俄国革命的人们的引人入胜的记述。有许多不同派别的人对沙皇政权感到幻灭——无政府主义者、孟什维克、布尔什维克、社会革命党人、十二月党人。他们就那些鉴于他们所处环境对他们来说最为突出的二分法展开了激烈辩论。
		- *这场“决战”……涵盖了所有常见的分歧点：“工人阶级”与“人民”；“冷静算计”与“伟大事迹和自我牺牲”；“客观主义”与“主观主义”；以及“普遍发展规律”与“俄罗斯的独特性”。*
	- 然而，他们中没有人预见到我们现在认为与经济发展更为相关的因素：分散的知识、自愿交换和创业创新。
	- 每当我在湾区的朋友们讨论通用人工智能时，我就会思考这个问题——是否会出现仅靠软件实现的奇点、对抗性失调、训练作弊、爆炸式增长等等？也许我们正在使用的框架和正在提出的问题从根本上就是错误的。
- 鉴于这个话题在认知上如此模糊不清，以至于某个聪明人可能会提出一个新的考量因素，从而改变你的关键结论，那么对于你所听到的最新、最有说服力的故事，你应该做出多大程度的更新呢？
	- 从历史上看，我们应对快速演变、不确定过程的有效方式是古典自由主义。基于推导出特定轨迹的推理来推动超级定制化的提议（“我们距离通用人工智能还有不到 5 年时间，因此要开展‘该项目’”），其记录相当糟糕。

---

如果你喜欢这篇博客文章，你可能会喜欢我的新书《扩展时代：2019 - 2025 年人工智能口述历史》。这本书精心挑选并整理了我与科学家、首席执行官、经济学家和哲学家关于人工智能的播客节目的精彩内容。

![](https://www.dwarkesh.com/p/%7B%22src%22:%22https://substack-post-media.s3.amazonaws.com/public/images/d171e5cb-6ada-4feb-99db-13403fdf658d_1600x1130.png%22,%22srcNoWatermark%22:null,%22fullscreen%22:null,%22imageSize%22:null,%22height%22:1028,%22width%22:1456,%22resizeWidth%22:null,%22bytes%22:null,%22alt%22:null,%22title%22:null,%22type%22:null,%22href%22:null,%22belowTheFold%22:true,%22topImage%22:false,%22internalRedirect%22:null,%22isProcessing%22:false,%22align%22:null%7D)

*感谢马克斯·法伦斯的评论和编辑。特别感谢卡尔·舒尔曼，同时也感谢利奥波德·阿申布伦纳、塔迈·贝西罗格鲁、埃格·埃尔迪尔、丹尼尔·科科特乔、斯科特·亚历山大、肖尔托·道格拉斯、亚当·达安杰洛、保罗·克里斯蒂亚诺、克雷格·福尔斯、格温·布兰温、丹·亨德里克斯、安德烈·卡帕西、托比·奥德以及其他许多人，感谢他们与我进行的对话，这些对话激发了我的问题。*