---
title: "Aman's AI Journal • Primers • Ilya Sutskever's Top 30"
source: "https://aman.ai/primers/ai/top-30-papers/"
author:
published: 2025-07-11
created: 2025-04-22
description: "Aman's AI Journal | Course notes and learning material for Artificial Intelligence and Deep Learning Stanford classes."
tags:
  - "clippings"
---
- - [复杂动力学第一定律](https://aman.ai/primers/ai/top-30-papers/#the-first-law-of-complexodynamics)
	- [循环神经网络的不合理有效性](https://aman.ai/primers/ai/top-30-papers/#the-unreasonable-effectiveness-of-recurrent-neural-networks)
	- [理解长短期记忆网络](https://aman.ai/primers/ai/top-30-papers/#understanding-lstm-networks)
	- [循环神经网络正则化](https://aman.ai/primers/ai/top-30-papers/#recurrent-neural-network-regularization)
	- [通过最小化权重的描述长度来简化神经网络](https://aman.ai/primers/ai/top-30-papers/#keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights)
	- [指针网络](https://aman.ai/primers/ai/top-30-papers/#pointer-networks)
	- [使用深度卷积神经网络进行 ImageNet 分类](https://aman.ai/primers/ai/top-30-papers/#imagenet-classification-with-deep-convolutional-neural-networks)
	- [顺序很重要：集合的序列到序列](https://aman.ai/primers/ai/top-30-papers/#order-matters-sequence-to-sequence-for-sets)
	- [GPipe：通过微批次流水线并行实现轻松扩展](https://aman.ai/primers/ai/top-30-papers/#gpipe-easy-scaling-with-micro-batch-pipeline-parallelism)
	- [用于图像识别的深度残差学习](https://aman.ai/primers/ai/top-30-papers/#deep-residual-learning-for-image-recognition)
	- [通过空洞卷积进行多尺度上下文聚合](https://aman.ai/primers/ai/top-30-papers/#multi-scale-context-aggregation-by-dilated-convolutions)
	- [用于量子化学的神经消息传递](https://aman.ai/primers/ai/top-30-papers/#neural-message-passing-for-quantum-chemistry)
	- [注意力是你所需要的一切](https://aman.ai/primers/ai/top-30-papers/#attention-is-all-you-need)
	- [通过联合学习对齐与翻译实现神经机器翻译](https://aman.ai/primers/ai/top-30-papers/#neural-machine-translation-by-jointly-learning-to-align-and-translate)
	- [深度残差网络中的恒等映射](https://aman.ai/primers/ai/top-30-papers/#identity-mappings-in-deep-residual-networks)
	- [用于关系推理的简单神经网络模块](https://aman.ai/primers/ai/top-30-papers/#a-simple-neural-network-module-for-relational-reasoning)
	- [变分有损自编码器](https://aman.ai/primers/ai/top-30-papers/#variational-lossy-autoencoder)
	- [关系循环神经网络](https://aman.ai/primers/ai/top-30-papers/#relational-recurrent-neural-networks)
	- [量化封闭系统中复杂性的兴衰：咖啡自动机](https://aman.ai/primers/ai/top-30-papers/#quantifying-the-rise-and-fall-of-complexity-in-closed-systems-the-coffee-automaton)
	- [神经图灵机](https://aman.ai/primers/ai/top-30-papers/#neural-turing-machines)
	- [深度语音 2：英语和普通话的端到端语音识别](https://aman.ai/primers/ai/top-30-papers/#deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin)
	- [神经语言模型的缩放定律](https://aman.ai/primers/ai/top-30-papers/#scaling-laws-for-neural-language-models)
	- [最小描述长度原则教程介绍](https://aman.ai/primers/ai/top-30-papers/#a-tutorial-introduction-to-the-minimum-description-length-principle)
	- [机器超级智能](https://aman.ai/primers/ai/top-30-papers/#machine-super-intelligence)
	- [柯尔莫哥洛夫复杂度与算法随机性](https://aman.ai/primers/ai/top-30-papers/#kolmogorov-complexity-and-algorithmic-randomness)
	- [斯坦福大学的 CS231n 视觉识别卷积神经网络](https://aman.ai/primers/ai/top-30-papers/#stanfords-cs231n-convolutional-neural-networks-for-visual-recognition)
- [Meta](https://aman.ai/primers/ai/top-30-papers/#meta)
	- [通过多标记预测实现更好、更快的大语言模型](https://aman.ai/primers/ai/top-30-papers/#better--faster-large-language-models-via-multi-token-prediction)
		- [关键要点：](https://aman.ai/primers/ai/top-30-papers/#key-takeaways)
	- [开放域问答的密集段落检索](https://aman.ai/primers/ai/top-30-papers/#dense-passage-retrieval-for-open-domain-question-answering)
		- [密集段落检索器（DPR）：](https://aman.ai/primers/ai/top-30-papers/#dense-passage-retriever-dpr)
	- [用于知识密集型自然语言处理任务的检索增强生成](https://aman.ai/primers/ai/top-30-papers/#retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks)
- [HuggingFace](https://aman.ai/primers/ai/top-30-papers/#huggingface)
	- [Zephyr：语言模型对齐的直接蒸馏](https://aman.ai/primers/ai/top-30-papers/#zephyr-direct-distillation-of-lm-alignment)
- [斯坦福](https://aman.ai/primers/ai/top-30-papers/#stanford)
	- [迷失在中间地带：语言模型如何使用长上下文](https://aman.ai/primers/ai/top-30-papers/#lost-in-the-middle-how-language-models-use-long-contexts)
- [Misc](https://aman.ai/primers/ai/top-30-papers/#misc)
	- [阿尔库纳：大语言模型遇见新知识](https://aman.ai/primers/ai/top-30-papers/#alcuna-large-language-models-meet-new-knowledge)
	- [使用大语言模型进行事实核查的风险与前景](https://aman.ai/primers/ai/top-30-papers/#the-perils--promises-of-fact-checking-with-large-language-models)
- 伊利亚·苏茨克维（Ilya Sutskever）与约翰·卡马克（John Carmack）分享了一份包含 30 篇论文的列表，并说道：“如果你真的学完所有这些，你就会了解当今 90%重要的东西”。下面我们将回顾这些论文/资源。

### 复杂动力学第一定律

- 作者：斯科特·阿伦森
- 文章《复杂动力学第一定律》讨论了肖恩·卡罗尔在基础问题研究所（FQXi）的“拨正时间”会议上提出的一个有趣问题，该会议汇聚了来自各个领域的专家来讨论时间的本质。卡罗尔的问题围绕着为什么物理系统的复杂性似乎会随着时间的推移而增加、达到最大值然后下降，这与持续增加的熵不同。
- 文章解释说，熵衡量系统的无序程度，并且单调增加。然而，复杂性的表现有所不同，在中间时刻达到峰值后下降。为了深入研究这一现象，作者引入了柯尔莫哥洛夫复杂性的概念。柯尔莫哥洛夫复杂性被定义为能够生成给定字符串的最短计算机程序的长度。一个相关的概念，精致度，将字符串的复杂性衡量为描述该字符串作为典型成员的一组的最短程序。
- 为了解决卡罗尔的问题，作者提出了“复杂熵”的概念，作为一种考虑计算资源限制的复杂性度量。复杂熵应反映最短有效程序中的位数，该程序从一个集合中输出一个样本，使得目标字符串相对于该集合显得随机。推测是，复杂熵在系统演化的开始和结束时会很小，但在中间阶段会很大，这反映了观察到的复杂性模式。
- 从理论上或实证上证明这一猜想都面临挑战，特别是由于计算复杂度的难度。一种建议的实用方法是使用 gzip 压缩文件的大小作为柯尔莫哥洛夫复杂度的近似值。作者提到了一个正在进行的研究项目，旨在使用这种方法对该猜想进行实证验证。
- 该文章还提出了复杂性或复杂熵随时间变化，在中间阶段达到峰值的观点。作者建议使用计算资源界限来定义这一衡量标准，并讨论了验证复杂性以这种方式表现这一猜想的理论和实证方法。这一探索为理解物理系统中复杂性的动态本质提供了有价值的见解。

### 循环神经网络的不合理有效性

- 作者：安德烈伊·卡帕西
- 安德烈·卡帕西所著的《循环神经网络的不合理有效性》一文深入探讨了循环神经网络（RNN）的惊人能力。卡帕西讲述了他首次使用 RNN 进行图像字幕训练的经历，即使在随机设置的情况下，RNN 也开始生成可信的图像描述。这一成功令人惊讶，因为许多人认为 RNN 很难训练，这表明了它们是多么简单而强大。
- 循环神经网络（RNN）很特别，因为它们可以处理向量序列，这使得它们非常适合那些涉及序列作为输入和输出的任务。与处理固定大小输入和输出的常规神经网络不同，循环神经网络可以处理任意长度的序列，这使得它们在许多领域都非常有用。卡帕西解释说，循环神经网络通过保持一个隐藏状态来工作，该隐藏状态存储来自先前输入的信息，从而使它们能够“记住”过去的数据。
- 卡帕西详细介绍了循环神经网络（RNN）的工作原理，包括一个简单的接口，在该接口中输入向量会影响输出向量，并考虑所有先前的输入。他展示了循环神经网络如何使用矩阵乘法和非线性函数来更新其隐藏状态。他还提到了长短期记忆（LSTM）网络，这是一种更高级的循环神经网络，解决了一些实际问题并得到广泛应用。
- 为了展示循环神经网络（RNN）的强大之处，卡帕西描述了字符级语言模型的训练过程。通过将大量文本输入到 RNN 中，它学会预测序列中的下一个字符，从而能够一次生成一个字符的文本。他给出了来自不同来源的 RNN 生成文本的示例，如保罗·格雷厄姆的文章、莎士比亚的作品、维基百科文章、用 LaTeX 编写的代数几何、Linux 源代码和婴儿名字。这些示例展示了 RNN 如何从原始文本中学习复杂的结构、语法和上下文。
- 卡帕西还谈到了训练过程，以及循环神经网络生成的文本是如何随着时间的推移而改进的，展示了模型是如何逐渐在理解语言方面变得更好的。他将循环神经网络的内部工作可视化，展示了不同的神经元如何对特定模式（如网址或 Markdown 语法）做出反应，这有助于解释模型是如何学习的。
- 最后，卡帕西鼓励读者使用他在 GitHub 上分享的代码来尝试循环神经网络（RNN），强调训练字符级语言模型的趣味性和教育意义。他简要提及了 RNN 研究的整体情况以及它们在自然语言处理、计算机视觉和机器学习等领域日益增长的重要性。文章以一个有趣的内容结尾，展示了由本文生成的一个 RNN 样本，证明了 RNN 是多么有效和通用。

### 理解长短期记忆网络

- 作者：克里斯托弗·奥拉
- 克里斯托弗·奥拉（Christopher Olah）所著的《理解长短期记忆网络》一文解释了长短期记忆（LSTM）网络的结构和功能。LSTM 网络是循环神经网络（RNN）的一种特殊类型，它解决了传统 RNN 在处理长期依赖关系方面的局限性。
- 奥拉首先强调了传统神经网络和循环神经网络在保持持久信息方面的局限性，而持久信息对于涉及序列和列表的任务（如语言建模、翻译和语音识别）至关重要。
- 循环神经网络（RNN）具有允许信息持续存在的循环，这使得它们适用于序列数据。然而，它们在处理长期依赖关系时存在困难，即在序列中需要很久之后才用到来自早期输入的相关信息。
- 本文介绍了旨在克服这一局限性的长短期记忆网络（LSTM）。LSTM 具有独特的架构，包括一个细胞状态和三个门控（输入门、遗忘门和输出门），这些门控调节信息的流动。这些门控使 LSTM 能够选择性地记住和忘记信息，从而使其在学习长期依赖关系方面很有效。
- 遗忘门决定从细胞状态中丢弃哪些信息，输入门确定要添加哪些新信息，而输出门控制传递到下一步的信息。
- 奥拉使用图表和符号解释了长短期记忆网络（LSTM）的逐步运行机制，从而更易于理解网络内部的复杂交互。他还讨论了长短期记忆网络的变体，如窥孔连接和门控循环单元（GRU），它们提供了处理长期依赖关系的不同方法。
- 本文最后强调了长短期记忆网络（LSTM）在各种应用中取得显著成果的重要性，并暗示了循环神经网络（RNN）研究的未来进展，如注意力机制和网格长短期记忆网络（Grid LSTM），这些进一步增强了神经网络的能力。

### 循环神经网络正则化

- 作者：沃伊切赫·扎伦巴、伊利亚·苏茨克维、奥里奥尔·温亚尔斯
- 论文《循环神经网络正则化》提出了一种将随机失活应用于长短期记忆（LSTM）网络以减轻过拟合的新方法。由于循环连接中的噪声放大，传统的随机失活技术对循环神经网络（RNN）无效，这阻碍了学习。作者提出了一种专门的随机失活应用，该应用仅针对 LSTM 中的非循环连接，在减少过拟合的同时保留网络在长序列上保留信息的能力。
- 该研究表明，在包括语言建模、语音识别、机器翻译和图像字幕生成在内的各种任务中，性能有显著提升。在语言建模方面，与未正则化的模型相比，正则化的长短期记忆网络（LSTMs）在宾州树库数据集上实现了更好的词级困惑度。中等规模和大规模的正则化 LSTMs 的困惑度大幅降低，突出了所提出方法的有效性。
- 对于语音识别，作者在谷歌内部的冰岛语语音数据集上测试了他们的方法，结果表明随机失活提高了帧准确率，这是一个与词错误率（WER）相关的关键指标。正则化长短期记忆网络（Regularized LSTM）实现了更好的泛化能力，这表明所提出的正则化技术在改进声学建模方面具有潜力。
- 在机器翻译中，该方法在 WMT’14 英语到法语数据集上进行了评估。正则化的长短期记忆网络（LSTM）优于未正则化的模型，展现出更高的 BLEU 分数，BLEU 分数用于衡量翻译质量。尽管正则化的 LSTM 没有超过基于短语的 LIUM 统计机器翻译（SMT）系统，但结果证实随机失活（dropout）提高了翻译性能。
- 图像字幕生成任务涉及在一个将图像向量转换为字幕的 LSTM 模型上测试随机失活变体。作者使用 MSCOCO 数据集进行此评估。结果表明，随机失活有助于提高字幕质量，正则化模型的表现与模型集成相当。
- 总体而言，该论文证实，将随机失活正确应用于长短期记忆网络（LSTM）可有效减少过拟合，并在各种应用中提高性能。作者认为，这种方法可以扩展到其他循环神经网络（RNN）架构，有可能拓宽神经网络中改进正则化的范围。

### 通过最小化权重的描述长度来简化神经网络

- 作者：杰弗里·E·辛顿和德鲁·范·坎普
- 辛顿（Hinton）和范坎普（van Camp）所著的论文《通过最小化权重的描述长度使神经网络保持简单》介绍了一种通过惩罚权重中的信息内容来对神经网络进行正则化的方法。其关键思想是向权重添加高斯噪声，并在训练过程中调整噪声水平，以平衡网络误差与权重复杂度之间的权衡。
- 最小描述长度（MDL）原则是这种方法的基础，这表明最佳模型会将描述模型及其所产生错误的总成本降至最低。对于神经网络而言，这意味着要最小化编码权重以及预测输出与实际输出之间差异所需的比特数。
- 通过对权重应用高斯噪声，作者有效地控制了权重值的精度。这种方法有助于减少过拟合，特别是在训练数据有限的情况下。调整噪声水平以优化网络性能，同时保持权重尽可能简单。
- 该方法涉及计算期望均方误差和权重中的信息内容的导数。只要输出单元是线性的，这些导数就能高效计算，无需借助耗时的蒙特卡罗模拟。
- 作者引入了“噪声权重”的概念，其中添加高斯噪声可实现权重的更紧凑编码。这种噪声权重方法利用最小描述长度（MDL）原理来更有效地传达权重，平衡权重精度与网络误差之间的权衡。
- 该研究探讨了这项技术在不同任务中的应用，包括语言建模、语音识别和图像字幕生成。结果表明，所提出的正则化方法通过减少过拟合显著提高了泛化能力。
- 此外，本文讨论了使用高斯混合自适应方法对权重进行编码的好处。这种混合模型在训练过程中适应权重的分布，进一步增强了网络从有限数据中进行泛化的能力。
- 在具有稀缺训练数据的高维任务上进行的初步实验表明，新方法能够有效地拟合复杂的非线性模型。结果表明，这种方法比传统的权重衰减方法略胜一筹，为神经网络正则化提供了新的视角。
- 作者们在结论中承认，虽然新方法显示出了前景，但还需要更多的实验工作来确定它与其他用于处理训练数据有限的非线性任务的统计技术相比的竞争力。他们还强调了进一步改进以提高其性能的潜力。

### 指针网络

- 作者：奥里奥尔·温亚尔斯、梅雷·福尔图纳托、纳夫迪普·贾特利
- 论文《指针网络》介绍了一种新颖的神经架构，旨在学习输出序列的条件概率，该输出序列的元素是与输入序列中的位置相对应的离散标记。这种模型称为指针网络（Ptr-Nets），解决了现有序列到序列模型和神经图灵机的局限性，这些模型在处理可变大小的输出字典时存在困难。Ptr-Nets 利用神经注意力机制选择输入序列的元素作为输出，使其在诸如对可变大小序列进行排序和各种组合优化任务等问题上特别有效。
- **主要贡献：**
	- 提出了 Ptr-Net 架构，以使用 softmax 概率分布作为指针来处理可变长度字典。该方法简单有效，并且使模型能够推广到不同的输入和输出长度。
	- 指针网络被应用于三个具有挑战性的几何问题：计算平面凸包、德劳内三角剖分以及平面旅行商问题（TSP）。这些模型学会了仅从训练示例中生成近似解，相较于具有输入注意力的序列到序列模型有显著改进。
	- 学习到的模型能够在超出其训练时的最大长度上进行泛化，这表明指针网络（Ptr-Nets）在处理可变长度的输入和输出序列时具有鲁棒性和通用性。
- **Models:**
	- 序列到序列模型：此基线模型使用编码器-解码器循环神经网络框架将输入序列映射到输出序列，但它需要固定的输出字典大小。它使用长短期记忆（LSTM）网络来估计条件概率，但在输出大小取决于输入长度的任务中表现不佳。
	- 基于内容的输入注意力机制：这种方法是对普通序列到序列模型的一种改进，它引入了一种注意力机制，使解码器能够专注于输入序列的不同部分。然而，它仍然假设输出字典大小是固定的。
	- 指针网络（Ptr-Net）：Ptr-Net 修改了注意力机制，使其起到指针的作用，从输入序列中选择元素作为输出。这使得 Ptr-Net 能够处理可变大小的输出字典，并有效地解决组合优化问题。
- **实证结果：**
	- 凸包：在凸包问题上，指针网络（Ptr-Nets）的表现显著优于长短期记忆网络（LSTM）以及带注意力机制的 LSTM 模型。指针网络实现了高精度和近 100%的面积覆盖率，证明了其在处理这一组合任务方面的有效性。
	- 德劳内三角剖分：指针网络实现了高三角形覆盖率和准确性，显示出它们在解决德劳内三角剖分问题方面的能力。尽管对于较大的输入规模准确性会下降，但该模型仍然具有竞争力。
	- 旅行商问题（TSP）：指针网络在平面对称旅行商问题上进行了测试，证明了其学习具有竞争力的解决方案的能力。该模型在小规模旅行商问题实例上表现良好，并能推广到更大的实例，不过性能会有一定程度的下降。
- **结论：**
	- Ptr-Net 架构成功解决了可变长度输出字典的挑战，在固定输入大小问题上优于传统的序列到序列模型。通过使用注意力机制来解决组合优化问题，Ptr-Net 为神经网络开辟了新的可能性，使其能够在没有人为约束的情况下处理更广泛的问题类别。未来的工作将探索 Ptr-Net 在其他组合问题（如排序）中的应用，旨在进一步证明其通用性和有效性。

### 使用深度卷积神经网络进行 ImageNet 分类

- 作者：亚历克斯·克里泽夫斯基、伊利亚·苏茨克维、杰弗里·E·辛顿
- 论文《使用深度卷积神经网络进行 ImageNet 分类》详细介绍了一个大型深度卷积神经网络（CNN）的开发和训练，该网络旨在对 ImageNet 数据集中的图像进行分类。该网络在分类准确率上取得了显著提高，超过了之前在 2010 年和 2012 年 ImageNet 大规模视觉识别挑战赛（ILSVRC）数据集上的最先进结果。
- **主要贡献：**
	- 卷积神经网络（CNN）架构由五个卷积层和三个全连接层组成，最后是一个具有 1000 个输出类别的 softmax 输出层。这种设计利用了图像数据的分层特性，卷积层用于捕捉局部特征，全连接层则将这些特征整合起来进行最终分类。
	- 为了加速训练，该网络使用修正线性单元（ReLU）而非传统的双曲正切或 Sigmoid 神经元。ReLU 有助于降低梯度消失问题的可能性，并在训练期间实现更快的收敛。
	- 该网络使用模型并行方法在两个 GPU 上进行训练，其中网络的不同层分布在这些 GPU 之间。这种设置允许处理无法装入单个 GPU 内存的大型模型。
	- 局部响应归一化（LRN）通过对同一层内神经元的活动进行归一化来提高泛化能力，模拟在真实神经元中观察到的一种侧向抑制形式。
	- 重叠池化用于对特征图的空间维度进行下采样。与传统的非重叠池化不同，重叠池化有助于保留更多信息并减少过拟合。
	- 为了对抗过拟合，作者使用了数据增强技术，包括图像平移、水平翻转以及对 RGB 值进行主成分分析（PCA）抖动。这些技术增加了训练数据集的有效规模并提高了泛化能力。
	- 随机失活（Dropout）应用于全连接层，在训练期间随机将一部分神经元设置为零。这种正则化技术可防止神经元的复杂共适应，并增强学习到的特征的鲁棒性。
- **实证结果：**
	- 在 ILSVRC - 2010 数据集上，卷积神经网络（CNN）的 top - 1 错误率为 37.5%，top - 5 错误率为 17.0%，显著优于之前的方法。
	- 在 ILSVRC - 2012 数据集上，该网络的 top - 5 错误率为 18.2%。当与多个模型的预测结果相结合时，这个错误率进一步降低到了 15.3%，大幅超越了排名第二的结果，其 top - 5 错误率为 26.2%。
	- 对学习到的特征进行定性分析表明，该网络在早期层捕获了各种类型的频率和方向选择性内核，在更深层捕获了更抽象的特征。
- **结论：**
	- 该论文表明，大型深度卷积神经网络（CNNs）可以通过纯监督学习在具有挑战性的图像分类任务上取得最先进的成果。网络的深度和复杂性对其性能至关重要，当移除任何一个卷积层时准确率下降就证明了这一点。
	- 网络的成功为通过利用更大的数据集和更强大的计算资源在计算机视觉领域取得进一步进展开辟了可能性。此后，这项工作中开发的方法和技术已成为深度学习和计算机视觉领域的基础。

### 顺序很重要：集合的序列到序列

- 作者：奥里奥尔·温亚尔斯、萨米·本吉奥、曼朱纳思·库德卢尔
- 论文《顺序很重要：集合的序列到序列》探讨了序列到序列（seq2seq）模型中输入和输出顺序的重要性，特别是对于输入或输出为集合而非自然有序序列的任务。作者提出了使 seq2seq 模型适应处理集合的方法，并展示了顺序对各种任务性能的影响。
- **主要贡献：**
	- 作者强调了传统序列到序列模型在处理元素顺序无关紧要的集合时的局限性。他们表明，输入和输出数据的呈现顺序会显著影响这些模型的学习和性能。
	- 他们引入了 seq2seq 框架的一个扩展，以便以一种有原则的方式处理输入集。这涉及使用注意力机制来处理无序集，使模型对输入顺序保持不变。
	- 对于输出集，作者提出了一种损失函数，该函数在训练期间搜索可能的顺序以找到最优排列，从而提高模型的泛化能力和准确执行能力。
- **实验与结果：**
	- 语言建模：作者对输入句子的不同排序进行了实验，并表明在源句子中颠倒单词顺序可以提高机器翻译任务的性能。他们还发现，对于句法分析任务，遍历顺序（深度优先与广度优先）的选择会显著影响模型的准确性。
	- 组合问题：本文展示了排序在组合问题中的重要性，例如对数字进行排序和计算凸包。例如，按角度对输入点进行排序可简化凸包计算，从而实现更快的训练和更高的准确率。
	- 图形模型：作者使用星型图形模型创建了人工数据集，并表明当先呈现头部变量时，学习联合概率分布会更容易。该实验突出了为随机变量之间的复杂依赖关系建模时选择最优顺序的重要性。
- **模型架构：**
	- 读取、处理、写入模型：所提出的模型由三个组件组成：一个嵌入每个输入元素的读取模块、一个使用注意力机制对嵌入进行计算的处理模块，以及一个使用指针网络生成输出序列的写入模块。这种架构确保了排列不变性，并有效地处理输入集。
	- 注意力机制：作者利用注意力机制来整合来自可变长度输入结构的信息，保持对于处理集合至关重要的顺序不变性属性。
	- 寻找最优排序：为应对确定最佳输出顺序这一挑战，作者提出了一种在训练期间探索不同排序的算法。通过从可能顺序的概率分布中进行采样，模型可以识别并强化最适合该任务的顺序。
- **结论：**
	- 该论文得出结论，在处理集合时，顺序对序列到序列模型的性能有显著影响。所提出的处理输入和输出集合的方法提高了模型的泛化能力和准确性。作者通过各种实验证明了他们方法的有效性，包括排序、语言建模、句法分析和图形模型估计。这项工作为将序列到序列模型扩展到更广泛的涉及无序集合的任务开辟了新的可能性。

### GPipe：通过微批次流水线并行实现轻松扩展

- 作者：黄燕萍、程友龙、安库尔·巴普纳、奥尔汉·菲拉特、陈美霞、陈德浩、李孝钟、吴继权、乐国威、吴永辉、陈志峰
- 论文《GPipe：通过微批次流水线并行实现轻松扩展》介绍了 GPipe，这是一个可扩展的模型并行库，旨在通过在多个加速器之间划分模型来实现大型神经网络的高效训练。GPipe 通过使用一种新颖的批次拆分流水线算法克服了内存限制并实现了几乎线性的加速。
- **主要贡献：**
	- GPipe 架构：GPipe 库将神经网络划分为更小的层子序列或“单元”，这些子序列分布在多个加速器上。这种设置允许训练超出单个加速器内存容量的模型。
	- 批处理拆分流水线并行：GPipe 将每个训练数据的小批次划分为更小的微批次。然后，这些微批次以流水线方式在不同的加速器上进行处理，确保高硬件利用率并最小化空闲时间。
	- 同步梯度下降：该库使用同步小批量梯度下降，即在应用梯度更新模型参数之前，先在所有微批次上累积梯度。这种方法确保了无论分区数量多少，梯度更新都是一致的。
- **实验与结果：**
	- 图像分类：使用 GPipe 在 ImageNet-2012 数据集上训练了一个拥有 5.57 亿参数的变形虫网络（AmoebaNet）模型。该模型的 top-1 准确率达到了 84.4%，证明了 GPipe 在扩展大型卷积网络方面的有效性。
	- 多语言神经机器翻译：GPipe 支持在跨越 100 多种语言的语料库上训练单个 60 亿参数、128 层的 Transformer 模型。该模型优于单独训练的双语模型，突出了 GPipe 处理多样化和大规模自然语言处理任务的能力。
- **性能优化：**
	- 重新计算激活值：为了降低激活值的内存需求，GPipe 支持重新计算激活值，即在正向传播过程中，仅存储分区边界处的输出激活值。所需的激活值在反向传播过程中重新计算，从而降低峰值内存使用量。
	- 负载均衡：分区算法旨在通过最小化所有单元估计成本的方差来平衡跨加速器的计算负载。这种优化可确保高效的流水线执行。
- **设计特点与权衡：**
	- 灵活性：GPipe 支持任何可以表示为层序列的神经网络，为各种架构和任务提供了通用的解决方案。
	- 效率：通过最小化通信开销并利用批处理拆分流水线并行性，即使在设备间通信带宽有限的环境中，GPipe 也能实现与加速器数量近乎线性的扩展。
	- 训练稳定性：使用同步梯度更新可确保在不同分区配置下进行稳定且一致的训练，从而使 GPipe 适用于大规模模型训练。
- **结论：**
	- GPipe 库提供了一种高效且灵活的方法，用于扩展深度神经网络，使其超越单加速器内存限制。其批处理拆分流水线算法能够显著提高训练吞吐量和模型容量。GPipe 的设计原则确保它可应用于广泛的机器学习任务，从图像分类到多语言机器翻译，并取得了强大的实证结果。该库处理大型模型并实现近线性加速的能力使其成为推进深度学习研究和应用的宝贵工具。

### 用于图像识别的深度残差学习

- 作者：何恺明、张翔宇、任少卿、孙剑
- 所属机构：微软研究院
- 这篇具有开创性的论文介绍了深度残差网络（ResNets）的概念，它显著简化了比以前使用的网络深得多的网络的训练。通过使用允许各层拟合残差映射而不是直接尝试拟合期望的底层映射的残差块，ResNets 促进了训练过程，并通过增加深度提高了准确性。

该论文的关键创新点和研究结果包括：

1. 残差学习框架：ResNet 中的层参照层输入学习残差函数，这简化了学习过程，因为网络学习修改恒等映射，而不必估计完整输出。
2. 优化的简易性：残差块使更深的网络更容易优化，因为它们通过使用执行恒等映射的捷径连接来缓解梯度消失问题。
3. 深度网络的卓越性能：大量实验表明，具有更深架构的残差网络（ResNets）在诸如 ImageNet 和 CIFAR-10 等主要数据集上的表现优于传统网络。例如，与 VGG 网络相比，深度高达 152 层的残差网络表现出更好的性能和更低的复杂度。
4. 广泛适用性：本文还强调了残差网络（ResNets）在图像分类之外的各种任务中的有效性，例如目标检测和定位，这是通过诸如瓶颈设计等改进来实现的，这些改进提高了计算效率。
- 这些贡献对深度学习领域产生了深远影响，影响了学术界和工业界随后的广泛研究和应用。

### 通过空洞卷积进行多尺度上下文聚合

- 作者：费舍尔·于、弗拉德连·科尔图恩
- 所属机构：普林斯顿大学、英特尔实验室
- 论文《通过空洞卷积进行多尺度上下文聚合》提出了一种利用空洞卷积改进语义分割的新方法。该方法使卷积神经网络能够系统地聚合多尺度上下文信息而不损失分辨率。
- 主要贡献：
	1. 扩张卷积
	- 介绍了空洞卷积的概念，空洞卷积能够在不降低分辨率或覆盖范围的情况下实现感受野的指数级扩展。
	- 扩张卷积，也称为空洞卷积，对于密集预测任务至关重要，因为它们在保持空间分辨率的同时支持多尺度上下文的聚合。
1. 多尺度上下文聚合：
	- 提出了一种新的卷积网络模块，该模块聚合多尺度上下文信息，提升了语义分割等密集预测架构的性能。
	- 该网络使用具有不同扩张因子的卷积层长方体，无需池化层或下采样层，从而在整个网络中保持高分辨率。
2. 简化网络设计：
	- 通过去除对性能没有贡献的不必要组件和层，简化适用于密集预测的现有图像分类网络。
	- 具体而言，移除 VGG-16 网络中的最后两个池化层和步幅层，并在后续层中使用空洞卷积以保持高分辨率输出。
3. 对照实验：
	- 在 Pascal VOC 2012 数据集上进行实验，以评估所提出的上下文模块的性能。
	- 证明了上下文模块在集成到现有的语义分割架构中时，无论有无像条件随机场（CRF）和 CRF 循环神经网络（CRF-RNN）这样的结构化预测方法，都能可靠地提高准确率。
4. 性能提升：
	- 上下文模块提高了语义分割模型的准确性，在 Pascal VOC 2012 测试集上优于先前的最先进模型。
	- 仅简化的前端模块就比先前的模型实现了更高的准确率，这表明去除残留组件的有效性。
- 实验：
	- 数据集：使用经额外注释扩充的 Pascal VOC 2012 数据集进行训练。
	- 训练过程：采用具有特定学习率和动量的随机梯度下降（SGD），并在验证集和测试集上评估性能。
	- 评估：针对诸如 FCN-8s 和 DeepLab 等模型对上下文模块和简化前端进行了测试，结果显示平均交并比（IoU）分数有显著提高。
- 结论：
	- 该论文表明，空洞卷积对于密集预测任务非常有效，能够在不损失分辨率的情况下整合多尺度上下文信息。
	- 所提出的上下文模块和简化的前端模块在语义分割方面带来了显著的性能提升。
	- 这种方法表明，要转向用于密集预测的专用架构，而不再采用图像分类网络的改编版本。

### 用于量子化学的神经消息传递

- 作者：贾斯汀·吉尔默、塞缪尔·S·舍恩霍尔茨、帕特里克·F·莱利、奥里奥尔·维尼亚尔斯、乔治·E·达尔
- 论文《用于量子化学的神经消息传递》介绍了消息传递神经网络（MPNNs），这是一种用于分子图监督学习的框架，对分子对称性具有不变性。目标是预测分子的量子力学性质，这在药物发现和材料科学等领域至关重要。
- 简介：
	- 本文强调了对机器学习模型的需求，这些模型能够直接从分子结构预测其性质，而无需依赖手工制作的特征。以前的方法严重依赖特征工程，这限制了通用性和性能。
	- 消息传递神经网络（MPNN）统一了几种现有的对图结构数据进行操作的神经网络模型，并允许直接从原始分子图中学习分子属性。
- 方法：
	- 消息传递阶段：在这个阶段，节点（原子）通过消息函数与其邻居交换信息。每个节点根据从其邻居接收到的消息及其当前状态更新其状态。
		- 形式上，对于具有节点特征 $xv$ 和边特征 $evw$ 的图 $G$ ，消息 $mvt+1$ 和节点更新 $hvt+1$ 由以下公式给出： $mvt+1=∑w∈N(v)Mt(hvt,hwt,evw)$ $hvt+1=Ut(hvt,mvt+1)$
		- 消息函数 $Mt$ 和更新函数 $Ut$ 在训练期间学习得到。
	- 读出阶段：在消息传递阶段之后，读出函数 $R$ 聚合节点状态以产生最终输出。读出函数必须对节点的排列不变，以确保模型对图同构的不变性。
- 主要贡献：
	- 最新成果：作者证明，消息传递神经网络（MPNNs）在 QM9 数据集上取得了最新的性能表现，该数据集是预测小分子有机量子力学性质的基准。消息传递神经网络能够高精度地预测诸如原子化能、基本振动频率和电子性质等性质。
	- 化学精度：这些模型在 QM9 数据集中 13 种属性中的 11 种上达到了化学精度（在化学可接受的误差范围内）。
	- 可扩展性：本文还探讨了将消息传递神经网络（MPNN）扩展到更大图的方法，使其在不牺牲性能的情况下提高计算效率。这包括使用“虚拟图元素”以及诸如“塔”结构之类的修改。
- 结果：
	- 作者提供了大量实证结果，表明消息传递神经网络（MPNNs）优于依赖特征工程的传统方法。他们证明了消息传递神经网络可以直接从数据中学习复杂的分子相互作用。
	- 他们比较了 MPNN 的不同变体，并表明使用边网络消息函数和集合到集合读出函数的模型表现特别出色。
- 结论：
	- 该研究将消息传递神经网络确立为分子性质预测的强大工具，突出了其通过从原始分子图进行端到端学习来取代特征工程的潜力。
	- 未来建议开展的工作包括提高对更大分子图的泛化能力，并进一步优化消息传递神经网络（MPNNs）的计算效率。

### 注意力是你所需要的一切

- 作者：阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马尔、雅各布·乌兹科雷特、利昂·琼斯、艾丹·N·戈麦斯、卢卡斯·凯泽、伊利亚·波洛苏欣
- 所属机构：谷歌大脑、谷歌研究院、多伦多大学
- 论文《Attention Is All You Need》介绍了 Transformer，这是一种全新的神经网络架构，它完全依赖自注意力机制，完全摒弃了循环和卷积。这种模型架构显著提高了计算效率和并行化能力，在诸如机器翻译等各种序列转换任务中取得了领先的性能。
- 主要贡献：
	1. Transformer 架构：
		- Transformer 使用了一种仅基于注意力机制的新颖架构，使模型能够在不使用序列对齐的循环神经网络（RNN）或卷积的情况下，捕捉输入和输出之间的全局依赖关系。
		- 该架构由编码器-解码器结构组成，其中编码器和解码器均由多个相同的层组成，每个层都由多头自注意力机制和逐位置全连接前馈网络组成。
	2. 自注意力机制
		- 缩放点积注意力机制：这是自注意力机制的核心组件，其中查询与所有键的点积被计算、缩放，并通过 softmax 函数以获得值上的权重。
		- 多头注意力机制：通过并行执行多个注意力操作，每个操作都有不同的学习线性投影，使模型能够同时关注不同位置的不同表示子空间中的信息。
	3. 位置编码：
		- 由于 Transformer 模型不使用循环来处理序列顺序，因此将位置编码添加到输入嵌入中，以注入有关序列中每个标记位置的信息。作者对这些编码使用不同频率的正弦和余弦函数。
	4. 训练效率与性能：
		- 与基于循环神经网络（RNN）的模型相比，Transformer 模型在机器翻译任务上表现更优，同时具有更高的并行性，训练所需时间也显著减少。
		- 对于 WMT 2014 英德翻译任务，Transformer 的 BLEU 得分为 28.4，比之前的最先进模型高出超过 2 个 BLEU 分数。同样，在 WMT 2014 英法翻译任务中，它在少得多的训练时间下达到了 41.8 的 BLEU 分数。
	5. 推广到其他任务：
		- Transformer 模型在机器翻译之外的其他任务上也具有良好的泛化能力。该论文展示了其在英语成分句法分析中的有效性，通过较少的特定任务调整就取得了有竞争力的结果。
	6. 相较于先前型号的优势：
		- 与循环神经网络（RNN）和卷积模型不同，Transformer 将长距离依赖之间的路径长度减少到固定数量的操作，RNN 和卷积模型的操作数量会随着序列长度线性或对数增长。
		- 路径长度的这种缩短提高了模型学习远距离位置之间依赖关系的能力，从而在序列转导任务中带来更好的性能。
- 实验结果：
	- 机器翻译：Transformer 在英德和英法翻译任务的 BLEU 分数上都设定了新的基准，展示了其卓越的翻译质量和训练效率。
	- 模型变体：本文探讨了对 Transformer 架构的各种修改，包括注意力头的数量和注意力键/值维度的大小，展示了模型的鲁棒性和灵活性。
	- 英文选区分析：该模型在宾州树库数据集上取得了较高的 F1 分数，表明其能够推广到不同的自然语言处理任务。
- 结论：
	- Transformer 代表了序列 transduction 模型的重大进步，为传统基于 RNN 和卷积的架构提供了一种高效且有效的替代方案。
	- 它对自注意力机制的依赖不仅提高了性能，还允许更大程度的并行化，使其适用于自然语言处理及其他领域的广泛应用。

### 通过联合学习对齐与翻译实现神经机器翻译

- 作者：德米特里·巴丹诺夫、赵京勋、约书亚·本吉奥
- 摘要：神经机器翻译（NMT）是一种新兴的方法，它构建单个神经网络以最大化翻译性能。与传统方法不同，NMT 使用编码器-解码器架构来翻译句子。本文介绍了一种方法，该方法允许模型在翻译过程中搜索源句子的相关部分，从而提高性能。
- 关键概念：
	- 编码器-解码器模型：神经机器翻译的基本架构，其中编码器将源句子转换为固定长度的向量，解码器生成翻译结果。
	- 固定长度向量瓶颈：传统编码器-解码器模型的一个显著限制是固定长度向量，这会阻碍性能，尤其是对于长句子。
	- 注意力机制：此模型引入了一种注意力机制，使解码器能够动态地关注源句子的相关部分。通过解决固定长度向量瓶颈问题，这提高了翻译质量。
- 提议的模型：
	- 双向循环神经网络编码器：将输入句子编码为一系列向量而非单个向量，从而捕捉更多上下文信息。
	- 基于注意力的解码器：为每个目标词计算这些向量的加权和，使模型能够针对每个目标词关注源句子的不同部分。
- 性能：
	- 所提出的模型优于传统的循环神经网络编码器-解码器模型，尤其是在处理较长句子时。
	- 在英法翻译任务上取得了与最先进的基于短语的系统相当的结果。
	- 定性分析表明，该模型生成的对齐方式在语言上是合理的。
- 实验：
	- 这些模型在 WMT ’14 英-法翻译任务上进行了测试。
	- 所提出的模型在 BLEU 分数方面比基本的编码器-解码器模型有显著改进。
- 结论：
	- 注意力机制显著增强了神经机器翻译（NMT）模型处理长句子和复杂语言结构的能力。
	- 未来的工作应解决处理未知或罕见词汇的问题，以进一步提高翻译性能。

### 深度残差网络中的恒等映射

- 作者：何恺明、张翔宇、任少卿、孙剑
- 所属机构：微软研究院
- 论文《深度残差网络中的恒等映射》探讨了恒等映射在深度残差网络（ResNets）架构中的作用，深度残差网络在计算机视觉任务中被广泛使用。作者分析了残差网络中前向和反向信号的传播，并提出了改进训练和泛化的修改方法。
- 主要贡献：
	1. 身份映射分析：
		- 作者们关注恒等映射在残差网络（ResNets）中的重要性，恒等映射允许前向和反向信号直接从一个残差块传播到任何其他块。
		- 他们证明，当使用恒等映射作为跳跃连接和加法后激活函数时，训练过程会变得更容易，并且网络的泛化能力会得到提高。
	2. 提议的残差单元：
		- 提出了一种新的残差单元设计，将恒等映射同时用作跳跃连接和加法后激活。
		- 这种设计确保信号能够在模块之间直接传播，简化了训练过程并提高了网络的泛化能力。
	3. 实证验证：
		- 作者进行了一系列消融实验，以支持身份映射的重要性。
		- 结果表明，他们提出的修改在诸如 CIFAR-10、CIFAR-100 和 ImageNet 等基准数据集上导致了更低的训练误差和更高的测试准确率。
	4. 深度残差网络：
		- 他们训练了极深的网络，包括在 CIFAR-10 和 CIFAR-100 上的 1001 层残差网络（ResNet），以及在 ImageNet 上的 200 层残差网络（ResNet）。
		- 这些深度网络实现了最先进的性能，证明了所提出修改的有效性。
- 实验结果：
	- CIFAR-10 和 CIFAR-100：
		- 一个 1001 层的残差网络（ResNet）在 CIFAR-10 数据集上的错误率达到了 4.62%，并且在 CIFAR-100 数据集上也展现出了卓越的性能。
		- 与原始的 ResNet 设计相比，所提出的恒等映射改进了训练收敛性和泛化能力。
	- **ImageNet**:
		- 在 ImageNet 上训练的 200 层残差网络（ResNet）比原始的 152 层 ResNet 具有更高的准确率，这表明了所提出的恒等映射方法的可扩展性。
- 结论：
	- 该研究表明，恒等映射在深度残差网络的效率中起着至关重要的作用。
	- 通过在跳跃连接和加法后激活中引入恒等映射，所提出的设计简化了训练并增强了泛化能力。
	- 研究结果表明，在现代深度学习架构中进一步挖掘网络深度具有巨大潜力。

### 用于关系推理的简单神经网络模块

- 作者：亚当·桑托罗、大卫·拉波索、大卫·G·T·巴雷特、马泰乌什·马林诺夫斯基、拉兹万·帕斯库努、彼得·巴塔利亚、蒂莫西·利利克拉普
- 所属机构：DeepMind，英国伦敦
- 论文《用于关系推理的简单神经网络模块》介绍了关系网络（RNs）的概念，它是一种神经网络模块，用于解决需要关系推理的任务。该论文展示了关系网络在多个领域的有效性，包括视觉问答、基于文本的问答以及对动态物理系统的推理。
- 主要贡献：
	1. 关系网络（RNs）介绍：
		- 循环神经网络旨在显式计算对象对之间的关系，使其适用于涉及关系推理的任务。
		- RN 是一个即插即用模块，可以添加到现有的神经网络架构中，增强其对关系进行推理的能力。
	2. 应用于视觉问答（CLEVR）：
		- 作者们在 CLEVR 数据集上测试了 RN，该数据集需要对视觉场景进行复杂的关系推理。
		- 基于 RN 增强的模型取得了当前最优的性能，在 CLEVR 基准测试中超越了人类的准确率。
	3. Sort-of-CLEVR 数据集：
		- 本文介绍了 Sort-of-CLEVR 数据集，该数据集旨在明确区分关系型问题和非关系型问题。
		- 在类似 CLEVR 数据集上的实验表明，关系网络（RNs）在关系问题上显著优于标准神经网络架构，凸显了显式关系推理的重要性。
	4. 基于文本的问答（bAbI）：
		- 循环神经网络（RNN）也被应用于 bAbI 任务集，这些任务涉及各种类型的推理，如演绎和归纳。
		- 基于循环神经网络增强的模型成功解决了 20 个 bAbI 任务中的 18 个，证明了其在基于文本的关系推理中的通用性和有效性。
	5. 动态物理系统：
		- 本文探讨了使用循环神经网络（RNs）对动态物理系统进行推理，例如推断移动物体之间的连接并计算连接系统的数量。
		- 注册护士在这些任务中取得了很高的准确率，展示了他们在物理模拟中处理复杂关系推理的能力。
- 模型详情：
- 架构：
	- 注册护士对对象集进行操作，其中每个对象由一个特征向量表示。
	- RN 使用函数 $gθ$ 计算成对关系，并使用函数 $fϕ$ 聚合这些关系，使网络能够推断和推理对象之间的关系。
	- 训练：
		- 这些模型使用标准优化技术（如 Adam 优化器）进行训练，并在各种基准测试中进行评估以验证其性能。
- 结果：
- **CLEVR**:
	- 基于关系网络增强的模型在 CLEVR 数据集上达到了 95.5%的准确率，显著优于之前缺乏显式关系推理组件的模型。
	- 有点像 CLEVR：
		- 在 Sort-of-CLEVR 数据集上，基于关系网络增强的模型在关系型和非关系型问题上的准确率均超过了 94%，而标准模型在关系型问题上表现不佳。
	- **bAbI**:
		- RN 模型在 20 项任务中通过了 18 项，证明了其处理 bAbI 任务所需的不同类型推理的能力。
	- 动态物理系统：
		- 注册护士准确地推断出连接并计算了连接的系统，显示了它们在推理物理相互作用方面的有效性。
- 结论：
	- 关系网络的引入为增强具有关系推理能力的神经网络提供了一个强大的工具。
	- 循环神经网络是通用的，可应用于广泛的任务，包括视觉和基于文本的问答以及对物理系统的推理。
	- 跨不同领域的关系网络（RNs）的成功凸显了它们作为需要关系推理的任务的通用解决方案的潜力。

### 变分有损自编码器

- 作者：陈曦、迪德里克·P·金马、蒂姆·萨利曼斯、段岩、普拉富拉·达里瓦尔、约翰·舒尔曼、伊利亚·苏茨克维、彼得·阿比尔
- 发表于：2017 年国际学习表征会议
- 机构：加州大学伯克利分校、OpenAI
- | 本文介绍了一种通过将变分自编码器（VAE）与神经自回归模型（例如，RNN、MADE、PixelRNN/CNN）相结合来学习全局表示的方法。这种模型，即变分有损自编码器（VLAE），可以控制学习到的全局潜在代码以丢弃二维图像中的纹理等不相关信息，从而以有损方式“自动编码”数据。使用自回归模型作为先验分布 $p(z)$ 和解码分布$$ p(x | z)$$ 提升了生成式建模性能，在多个数据集上取得了领先成果。 |
	| --- | --- |
- **关键概念：**
	1. 表示学习：旨在揭示观测数据的某些方面，使其适用于分类等下游任务。变分自编码器（VLAE）专注于捕捉全局结构并丢弃细节纹理。
	2. 变分自编码器（VAE）：VAE 通常将概率生成模型与推理模型相结合，以优化数据对数似然的下限。
	3. 自回归模型：这些模型，如循环神经网络（RNN）、掩码自编码器密度估计（MADE）和像素卷积神经网络（PixelCNN），处理序列中的数据依赖性，从而实现强大的密度估计。
- **技术亮点：**
	1. **变分自编码器（VAE）与自回归模型的结合：**
		- 当使用像循环神经网络（RNN）这样强大的解码器时，传统变分自编码器（VAE）可能无法有效地使用潜在代码。
		- 作者建议在解码器中使用局部感受野，以确保潜在代码捕获全局结构。
	2. **比特回传编码与信息偏好：**
		- 比特回退编码是变分推断的一种信息论观点。
		- 该模型通过减去通过近似后验分布传输的额外信息，使期望代码长度最小化。
	3. **通过显式信息放置实现的有损编码：**
		- 通过将解码器设计为仅对局部依赖关系进行建模，变分自编码器（VLAE）迫使潜在代码捕获全局信息。
		- 这会导致有损压缩，即在丢弃局部细节的同时保留基本的全局结构。
	4. **基于自回归流的学习先验：**
		- 先验分布 $p(z;θ)$ 由自回归模型进行参数化，提高了回位编码（Bits-Back Coding）的效率。
		- 自回归流（AF）将一个简单的噪声源转换为一个复杂的潜在代码，增强了模型的表达能力。
- **实验与结果：**
	1. **数据集：**
		- 该模型在二值图像数据集（MNIST、OMNIGLOT、加州理工学院 101 轮廓数据集）和 CIFAR10 上进行评估。
	2. **性能：**
		- MNIST：变分潜自编码器（VLAE）取得了新的最优结果，性能超过了像像素循环神经网络（PixelRNN）和可逆自编码器变分自编码器（IAF VAE）这样的模型。
		- OMNIGLOT 和加州理工学院 101 数据集：与之前的模型相比，对数似然有显著提高。
		- CIFAR10：VLAE 展现出具有竞争力的性能，在变分潜变量模型中取得了最先进的成果。
	3. **可视化：**
		- 作者提供了来自 VLAE 的原始图像和解压缩图像的可视化，表明该模型在生成合理的局部细节时捕捉到了全局结构。
- **结论：**
- 变分有损自编码器（VLAE）有效地结合了变分自编码器（VAE）和自回归模型的优势，实现了可控的表示学习并改进了密度估计。该模型的设计确保了潜在代码能够捕获重要的全局信息，使其适用于各种生成任务。未来的工作包括将 VLAE 扩展到其他数据类型，如音频和视频，并设计特定任务的表示以增强半监督学习。

### 关系循环神经网络

- 作者：亚当·桑托罗、瑞安·福克纳、大卫·拉波索、杰克·雷、迈克·赫扎诺夫斯基、西奥菲 ane·韦伯、达恩·维斯特拉、奥里奥尔·维尼亚尔斯、拉兹万·帕斯库努、蒂莫西·利利克拉普
- 机构：深度思维公司、伦敦大学学院
- 摘要：论文《关系循环神经网络》研究了标准的基于记忆的神经网络架构（如长短期记忆网络）在处理需要复杂关系推理的任务时的局限性。作者引入了一种新的记忆模块——关系记忆核心（RMC），它采用多头点积注意力机制，使记忆能够相互作用。在需要跨序列信息进行关系推理的任务上，包括强化学习、程序评估和语言建模，RMC 表现出了更好的性能。
- **要点：**
	- 标准架构中的关系推理缺陷：像长短期记忆网络（LSTM）这样的标准内存架构在处理涉及理解实体之间复杂关系推理的任务时常常遇到困难。
	- 关系记忆核心（RMC）介绍：RMC 采用多头点积注意力机制，允许记忆之间进行交互，从而提高模型执行关系推理的能力。
	- **应用与结果：**
		- 关系推理的玩具任务：开发了一个玩具任务来对序列信息的关系推理进行压力测试，展示了 RMC 相对于标准架构的卓越性能。
		- 强化学习：在小吃豆人任务中，RMC 的表现显著优于 LSTM，尤其是在进行全观察训练时，性能几乎提高了一倍。
		- 语言建模：RMC 在各种语言建模任务中获得了更低的困惑度分数，证明了数据效率的提高以及对高频词的更好建模。
	- **模型设计与功能：**
		- 内存交互：RMC 允许使用多头点积注意力在内存插槽之间进行交互，这随着时间的推移提高了模型进行关系推理的能力。
		- 任务性能：RMC 在诸如部分可观测强化学习、程序评估和语言建模等任务中表现优于标准架构。
- 结论：RMC 的引入表明，对记忆交互进行显式建模可以提高神经网络在需要跨序列信息进行复杂关系推理的任务上的性能。该研究强调了在循环神经网络中实现记忆向量之间的交互以提高关系推理能力的重要性。

### 量化封闭系统中复杂性的兴衰：咖啡自动机

- 作者：斯科特·阿伦森、肖恩·M·卡罗尔、劳伦·乌埃莱特
- 本文探讨了封闭系统中复杂性的行为，并将其与单调增加的熵进行比较。作者使用二维元胞自动机，模拟“咖啡”和“奶油”的混合，来建模和测量被称为“表观复杂性”的复杂性，其定义为粗粒度状态的柯尔莫哥洛夫复杂性。
- 引言：本文开篇对比了熵与复杂性。随着时间推移，熵会增加，而复杂性似乎会上升、达到最大值，然后下降。作者旨在使用一个简单的自动机模型对这种模式进行量化。
- 背景：讨论了熵和复杂性的几个概念：
	- 熵：玻尔兹曼熵、吉布斯熵、香农熵和柯尔莫哥洛夫复杂度。
	- 复杂性：引入了不同的复杂性度量，包括表观复杂性、精致度、逻辑深度和光锥复杂性。
- 表观复杂性：定义为状态的去噪或平滑版本的柯尔莫哥洛夫复杂性。此度量旨在捕捉系统中“有趣的”非随机信息。
- 复杂性：一种基于柯尔莫哥洛夫复杂性的度量，旨在捕捉系统中非随机信息的数量。它涉及找到一个集合 S，使得字符串 x 是 S 的一个通用元素。
- 逻辑深度：由贝内特提出，它衡量最短程序输出一个字符串所需的时间，体现了产生一种状态的“计算工作量”。
- 光锥复杂度：由沙利齐等人提出，它衡量时空历史中某一点的过去和未来光锥之间的互信息，反映预测信息内容。
- 咖啡自动机模型：
	- 交互模型：粒子相互作用，如果相邻且不同则交换位置。
	- 非相互作用模型：粒子在随机游走中独立移动。
- 实验与结果：
	- 自动装置开始时咖啡和奶油是分开的，随着时间推移会混合在一起。
	- 粗粒化：对局部区域的状态进行平均，以生成粗粒化版本。
	- 测量：使用细粒度和粗粒度状态的文件压缩（例如 gzip）来估计复杂度和熵。
	- 结果表明复杂性先增加、达到峰值然后下降，而熵则稳步增加。
- 调整后的粗粒化：
	- 为了减少阈值处理产生的伪影，引入了一种调整方法，增强了复杂度测量的鲁棒性。
- 结论与进一步工作：
	- 粗粒度方法有效地反映了人类对复杂性的直觉。
	- 未来的工作可以探索其他指标，如光锥复杂度，并改进复杂度度量的理论基础。

### 神经图灵机

- 作者：亚历克斯·格雷夫斯、格雷格·韦恩、伊沃·达尼埃尔卡
- 总结：
	- 简介：
		- 本文介绍了神经图灵机（NTMs），这是一种将神经网络与外部存储资源相结合的新型架构。这种设置的灵感来自图灵机的结构，但它是端到端可微的，这使得它能够使用梯度下降进行训练。
	- 基础研究：
		- 心理学与神经科学：讨论工作记忆作为一个涉及信息短期存储和操作的系统，通常与前额叶皮层和基底神经节相关联。
		- 认知科学与语言学：重点介绍认知科学的发展历程以及围绕联结主义理论、变量绑定和递归处理的争论，这些对于人类认知和语言处理至关重要。
		- 循环神经网络：描述了循环神经网络（RNN）和长短期记忆（LSTM）网络，强调了它们处理序列的能力以及图灵完备性，这使得它们在有足够资源的情况下能够模拟任何算法。
	- 神经图灵机
		- 非传统机器（NTMs）将神经网络控制器与记忆矩阵相结合。这种记忆可以通过可微操作进行读取和写入，使得整个系统能够通过梯度下降进行训练。
		- 读写：神经图灵机通过对存储位置使用加权机制来执行读写操作，这允许进行细粒度控制和可靠的数据存储。
		- 寻址机制：神经 Turing 机（NTMs）采用基于内容和基于位置的寻址方式来高效管理内存操作。基于内容的寻址侧重于存储值的相似性，而基于位置的寻址则便于迭代和随机访问。
		- 控制器网络：该架构可以使用循环（LSTM）神经网络或前馈神经网络作为控制器，每种选择都有不同的优势。
	- 实验：
		- 本文展示了在各种任务上的实验，如复制、重复复制、联想记忆、动态 N 元语法和优先级排序。与标准长短期记忆网络（LSTM）相比，神经图灵机（NTM）表现出了卓越的性能和泛化能力。
		- 复制任务：与长短期记忆网络（LSTM）相比，神经图灵机（NTM）学会了更有效地存储和回忆序列，对更长序列表现出更好的泛化能力。
		- 重复复制任务：神经图灵机（NTMs）凭借其记忆和寻址机制，在按指定次数重复序列方面表现出色。
		- 关联记忆：神经图灵机利用其管理复杂数据结构的能力，在基于关联查询召回项目方面表现出色。
		- 动态 N 元语法：神经图灵机能够快速适应不断变化的预测分布，性能优于长短期记忆网络。
		- 优先级排序：非确定性图灵机能够根据优先级对数据进行排序，展示了它们的算法学习能力。
	- 结论：
		- 非传统机器学习方法代表了朝着更通用、更强大的神经网络架构迈出的重要一步。它们学习和归纳简单算法的能力为机器学习和人工智能应用开辟了新的可能性。
- 本文介绍了神经图灵机架构，重点阐述了其基础、结构以及在各种算法任务中的性能，展示了通过集成外部记忆和寻址机制来革新神经网络能力的潜力。

### 深度语音 2：英语和普通话的端到端语音识别

- 作者：百度研究院 - 硅谷人工智能实验室
- 摘要：本文介绍了深度语音 2（Deep Speech 2），这是一种用于语音识别的端到端深度学习模型，可处理英语和汉语普通话。该方法用神经网络取代了传统的自动语音识别（ASR）管道，增强了对嘈杂环境、口音和不同语言的鲁棒性。利用高性能计算技术，该模型实现了显著的加速，从而能够进行快速实验和模型改进。该系统在多个基准测试中展现出与人工转录员相当的性能，并且可以在低延迟的在线环境中高效部署。
- 简介：传统的自动语音识别（ASR）系统依赖多个手工设计的组件，这使得它们复杂且难以适应新语言或新环境。深度语音 2 通过使用深度学习来端到端地训练单个模型，从而简化了这一过程。该系统在英语和普通话方面都能实现高精度，并且由于高效的高性能计算技术，可以快速迭代。
- 模型架构：该模型架构包括多个层，例如用于特征提取的卷积层和用于时间建模的循环层。与先前模型相比的关键改进包括使用批量归一化以实现更快的收敛，以及使用 SortaGrad 以在可变长度序列上进行高效训练。该系统还探索了不同的循环单元类型，如门控循环单元（GRU），并采用步幅和行卷积以获得更好的性能和可部署性。
- 训练数据：训练利用了大量数据集，其中包含 11940 小时的英语语音和 9400 小时的普通话语音。数据增强技术，如添加噪声，可提高对不同环境的鲁棒性。训练过程涉及使用分布在多个 GPU 上的大型小批次数据，并采用同步随机梯度下降（SGD）以保持可重复性。
- **结果：**
	- 英文：在诸如 WSJ 和 LibriSpeech 等多个朗读语音基准测试中，深度语音 2（Deep Speech 2）的表现优于人工转录员。在处理带口音和有噪音的语音方面，它也有显著改进，不过在非常嘈杂的环境中，其表现仍落后于人类。
	- 普通话：该系统在短语音查询话语方面与人工转录员取得了具有竞争力的结果。诸如更深的网络和批量归一化等架构改进显著提高了性能。
- 部署：该系统专为在生产环境中进行高效部署而设计，采用诸如批量调度等技术，以确保在处理多个用户流时具有低延迟。这使其适用于实时应用程序。
- 结论：深度语音 2 代表了端到端语音识别的重大进步，在不同语言和条件下都展现出了高精度。它利用大型数据集和高性能计算技术的能力使得强大的自动语音识别系统能够快速开发和部署。
- 本总结涵盖了《深度语音 2》论文的主要发现和贡献，重点介绍了其端到端深度学习方法、架构创新以及在英语和普通话语音识别方面的显著性能提升。

### 神经语言模型的缩放定律

- 作者：贾里德·卡普兰、山姆·麦坎德利斯、汤姆·亨尼汉、汤姆·B·布朗、本杰明·切斯、雷翁·蔡尔德、斯科特·格雷、亚历克·拉德福德、杰弗里·吴、达里奥·阿莫代伊
- 机构：OpenAI、约翰霍普金斯大学
- 论文《神经语言模型的缩放定律》探讨了经验缩放定律，这些定律描述了语言模型性能与模型大小、数据集大小以及用于训练的计算资源等因素之间的关系。该研究发现，在几个数量级上，性能根据幂律可预测地缩放。主要发现包括：
1. 幂律关系：语言模型的性能会随着模型大小（参数数量）、数据集大小（词元数量）和计算量（浮点运算次数）的增加而可预测地提高。这些改进遵循简单的幂律关系。
2. 模型大小与数据效率：更大的模型在样本效率上显著更高，这意味着与较小的模型相比，它们在达到相同性能水平时所需的数据点更少。
3. 最优计算分配：对于固定的计算预算，在相对少量的数据上训练非常大的模型，并在完全收敛之前停止训练是最有效的。
4. 最小化架构影响：性能在很大程度上取决于规模（大小、数据、计算量），而在较弱程度上取决于特定的架构超参数，如网络宽度或深度。
- 关键方程
- 作为参数函数的模型性能：- $L(N)=(NcN)αN$
	- 其中 $L$ 为损失， $N$ 为非嵌入参数的数量， $Nc$ 为常数， $αN$ 为缩放指数。
- 数据集大小关系：- $L(D)=(DcD)αD$
	- 其中 $D$ 是以词元为单位的数据集大小， $Dc$ 是一个常数， $αD$ 是缩放指数。
- 计算效率：- $L(Cmin)=(Cmin,cCmin)αmin,C$
	- 其中 $Cmin$ 是所需的最小计算量， $Cmin,c$ 是一个常数， $αmin,C$ 是缩放指数。
- 样本效率：使用相同数量的数据训练的更大模型由于其利用数据能力的提升而实现更好的性能。
- 训练动态：训练曲线遵循可预测的幂律，允许早期外推以预测模型的最终性能。
- 泛化：在不同数据集上的性能与训练数据集上的性能一致提高，这表明更好的分布内性能转化为更好的分布外性能。
- 模型大小与数据集大小：随着模型大小的增加，数据集大小应按次线性缩放以避免过拟合，这意味着适度增加数据对于大得多的模型就足够了。
- 计算高效训练：通过使用与模型大小相比相对较小的数据集，对非常大的模型进行较少步骤的训练来实现最佳性能。
- 这些发现为理解和预测大规模神经语言模型的性能提供了一个框架，指导未来在优化模型训练和部署方面的研究和实际应用。

### 最小描述长度原则教程介绍

- 作者：彼得·格伦瓦尔德
- 本文对里斯南的最小描述长度（MDL）原则进行了广泛的介绍和技术阐述。本教程的结构旨在对 MDL 进行概念性和技术上精确的探索，首先在概念层面上使这些思想易于理解，然后深入探讨数学细节。
- 关键技术细节：
	1. MDL 与数据压缩：MDL 原理作为一种统计建模和推理方法被引入，它从数据压缩的角度看待学习和模型选择。它概括了这样一种观点，即数据集的最佳模型是最有效地压缩数据的模型，在模型复杂性和拟合优度之间取得平衡。
	2. 柯尔莫哥洛夫复杂度与最小描述长度：本教程将柯尔莫哥洛夫复杂度作为最小描述长度的理论基础进行讨论，将其描述为在某种固定通用语言中对字符串的最短可能描述的长度。
	3. 实用最小描述长度（MDL）：这涉及对理想 MDL 进行近似，以使其适用于现实世界的场景，在这些场景中，精确计算柯尔莫哥洛夫复杂度是不可行的。实际实现通常使用近似柯尔莫哥洛夫复杂度的统计模型和编码方案。
	4. 精炼与粗略的最小描述长度（MDL）：阐述了粗略 MDL 与精炼 MDL 之间的区别。粗略 MDL 在不考虑精确拟合的情况下近似模型成本，而精炼 MDL 通过同时考虑模型成本和将模型拟合到数据的成本来提供更精确的模型。
	5. 用于模型选择的最小描述长度（MDL）：MDL 在模型选择中的效用尤为突出，它通过评估哪个模型能对数据进行最佳压缩，从而作为在竞争模型之间进行选择的标准。
	6. 统计与信息论基础：本教程主要通过克拉夫特不等式和信息不等式，介绍与最小描述长度（MDL）相关的信息论基本概念，如熵、互信息以及概率与码长之间的关系。
	7. 应用与扩展：本文档讨论了最小描述长度（MDL）在编码、机器学习和统计推断等领域的各种应用，展示了 MDL 如何成为一种统一的方法，用于理解和应用这些领域的概念。
- 本文档作为对最小描述长度（MDL）的全面介绍，提供了对该原理的理论和实践方面的重要见解。它强调了 MDL 在选择模型时的重要性，这些模型不仅要善于拟合数据，还要以简洁的方式提供有意义的见解。

### 机器超级智能

- 谢恩·莱格的博士论文《机器超级智能》对超级智能机器发展背后的挑战和理论基础进行了广泛分析。论文中的关键技术讨论包括：
1. 智能度量框架：莱格引入了一种正式的机器智能度量方法，该方法涵盖了理论和实践两个方面。此度量旨在评估系统在不同环境中实现各种目标的能力，这对于超级智能的概念至关重要。
2. 超级智能之路：本论文探讨了可能通向超级智能的各种途径，包括通过生物手段增强人类智能、机器学习算法、脑机接口以及自我改进的人工智能系统。莱格评估了每条途径的可行性及其对开发超级智能系统的潜在影响。
3. 对智能的算法洞察：详细讨论了算法在模拟或复制类人智能方面的作用。这包括对现有机器学习技术及其局限性的分析，以及它们如何可能发展以处理与更高智能相关的更复杂、抽象的任务。
4. 机器学习的理论模型：莱格深入探讨了可能支撑超级智能人工智能的理论模型，讨论了诸如机器学习的贝叶斯框架、强化学习在决策过程中的作用，以及可能使人工智能达到或超越人类智能水平的递归自我改进算法的潜力。
5. 安全与控制：论文的很大一部分致力于探讨通用人工智能的影响，尤其是控制和安全问题。莱格讨论了确保超级智能系统在人类预期范围内运行的策略，这对于防止不良或灾难性情况至关重要。
- 莱格博士论文的这些组成部分为理解和推动超级智能人工智能系统的发展提供了深厚的理论基础，同时也解决了此类发展中的控制和安全关键问题。

### 柯尔莫哥洛夫复杂度与算法随机性

- A. 申恩、V. A. 乌斯宾斯基和 N. 韦列夏金所著的《柯尔莫哥洛夫复杂性与算法随机性》一书全面概述了柯尔莫哥洛夫复杂性和算法随机性的基本概念。以下是本书中讨论的详细技术见解和框架：
- 定义与意义：柯尔莫哥洛夫复杂度被定义为能够生成给定字符串然后停机的最短二进制程序（在图灵机代码的意义上）。该复杂度衡量字符串中包含的信息量，本质上是对其随机性进行量化。
- 不可预测性与随机序列：算法随机性加深了我们对序列随机性构成要素的理解。这对于密码学和计算理论等领域至关重要，在这些领域中，随机性确保了安全性和效率。
- 理论基础
	- 形式主义与证明：作者深入探讨形式定义，提供严谨的证明以支持算法信息论的理论基础。
	- 不可压缩性方法：这本书的很大一部分致力于解释不可压缩性方法，该方法使用柯尔莫哥洛夫复杂性来证明解决计算问题所需资源的下限。
- 实际应用
	- 数据压缩：柯尔莫哥洛夫复杂性原理直接适用于数据压缩，其目标是以尽可能短的形式对数据进行编码。
	- 心理模型：本书探讨了如何使用算法信息论对人类对随机性和复杂性的认知进行建模。
- 高级主题
	- 互信息：在柯尔莫哥洛夫复杂性的背景下对互信息进行详细讨论，探讨信息如何在字符串的不同部分之间或不同字符串之间共享或传递。
	- 条件复杂度：条件复杂度的概念，即给定另一个字符串时一个字符串的复杂度，得到了全面的解释，这有助于理解数据中的依赖关系。
- 数学严谨性
	- 深度数学分析：这本书包含丰富的数学讨论，能让人深入理解相关概念。它涵盖了复杂的证明和理论探索，这对于计算机科学和数学的高级研究至关重要。
- 未来方向：结论部分讨论了当前理论的局限性以及进一步研究的潜在领域。作者推测了算法信息论在新兴技术和科学中的未来应用。
- 这本书对于对构成信息论、计算机科学及相关学科基础的深层数学结构感兴趣的研究人员、学者和学生来说是一份宝贵的资源。它不仅对柯尔莫哥洛夫复杂性和算法随机性进行了严谨的介绍，还探讨了它们在实践和理论领域的影响。

### 斯坦福大学的 CS231n 视觉识别卷积神经网络

- 目的：本课程向学生介绍卷积神经网络（ConvNets）的基本概念及其在图像识别和处理任务中的应用。卷积神经网络是一类神经网络，已在图像识别和分类等领域证明非常有效。
- 架构优势：卷积神经网络（ConvNets）本质上利用了输入数据的二维结构，这使得它们特别适合图像处理。与常规的全连接神经网络不同，卷积神经网络保留了像素之间的空间层次结构，以管理处理大图像时涉及的计算复杂性。
- 卷积神经网络的核心组件
	- 层：卷积神经网络中使用的主要层包括卷积层、池化层和全连接层（密集层）。
		- 卷积层：对输入应用卷积操作，将结果传递到下一层。该层的参数由一组可学习的滤波器组成，这些滤波器在空间上较小，但贯穿输入体的整个深度。
		- 池化（下采样或降采样）层：通常用于减小输入数据体的空间维度（宽度和高度），以便为下一个卷积层做准备。它有助于减少网络中的参数数量和计算量。
		- 全连接层：全连接层中的神经元与前一层的所有激活值都有完全连接。该层通常计算类别得分，得到大小为\[1x1xN\]的张量，其中 N 是类别数。
- 训练卷积神经网络
	- 损失函数：训练涉及定义一个损失函数（如交叉熵损失），该函数衡量网络预测与实际标签相比的好坏程度。
	- 反向传播：使用微积分的链式法则迭代计算网络中每个权重的梯度，通过随机梯度下降等技术最小化损失函数来有效训练模型。
- 实际挑战
	- 过拟合：训练卷积神经网络时的一个主要挑战，尤其是当参数数量与训练样本数量相比很大时。诸如随机失活、数据增强和 L2 正则化等技术被用于缓解这个问题。
	- 超参数调优：包括选择学习率、学习率衰减、正则化常数等。
- 高级主题
	- 批归一化：一种提高人工神经网络训练速度和稳定性的技术。它对每个小批量的输入进行归一化，使平均输出接近 0，输出标准差接近 1。
	- 迁移学习与微调：一种技术，即针对特定任务开发的网络被重新用作第二个任务模型的起点。在对没有大量标记训练样本的数据集进行建模时特别有效。

## Meta

### 通过多标记预测实现更好、更快的大语言模型

- 作者：法比安·格洛克勒、巴德尔·尤比·伊德里斯、巴普蒂斯特·罗齐埃、大卫·洛佩斯-帕斯和加布里埃尔·辛纳埃夫
- 大型语言模型（LLMs）的最新进展主要围绕下一个标记预测方法展开。然而，在题为《通过多标记预测实现更好更快的大型语言模型》的论文中引入的一种新方法表明，正在朝着同时预测多个标记的方向发生重大转变。这种方法不仅提高了LLMs的效率和速度，而且在各种任务中，尤其是在编码基准测试中，模型性能也有了显著提升。
- 多令牌预测架构重新定义了LLMs处理和生成文本的方式，它允许模型一次预测多个未来令牌。与传统架构按顺序预测下一个单个令牌不同，这种方法利用多个并行工作的独立输出头，显著加快了训练和推理过程。
- 多令牌预测架构的核心是共享主干，这是一个处理输入数据的通用特征提取器。这个主干负责生成输入的丰富、情境化表示，然后将其输入到多个输出头中。每个头的任务是根据共享表示预测不同的未来令牌，确保所有预测的令牌在上下文上连贯且相关。
- 多令牌预测架构的引入具有几个深远的影响。首先，它提高了样本效率，这意味着模型需要更少的数据迭代就能实现高性能。其次，它显著加快了推理过程，因为可以并行生成多个令牌，减少了生成输出所需的时间。随着模型规模的增加，这种架构还显示出很强的可扩展性，使其对于传统上面临速度和效率瓶颈的更大模型特别有效。
- Empirical results from the study highlight the effectiveness of the multi-token prediction model. On coding benchmarks like HumanEval and MBPP, models equipped with this new architecture outperform traditional next-token prediction models by a considerable margin. For instance, models trained with multi-token prediction solve up to 17% more problems on MBPP and demonstrate similar improvements on HumanEval.
- Moreover, these models are up to three times faster at inference compared to their traditional counterparts. This speed increase is crucial for real-time applications and services that rely on quick responses from LLMs. The architecture’s benefits are also more pronounced as the model size increases, which confirms its suitability for large-scale implementations where efficiency and speed are critical.
- Thus, the multi-token prediction architecture presents a viable and promising alternative to the conventional methodologies used in training large language models, pushing the boundaries of what is possible in natural language processing and machine learning.

#### Key Takeaways:

- 🔹 The model consists of a shared trunk and several independent output heads. It processes incoming data to generate a contextualized representation, which is then utilized simultaneously by all output heads for predicting multiple future tokens.
- 🔹 Departing from traditional single-token prediction, this model enables simultaneous prediction of multiple tokens, significantly accelerating both training and inference processes.
- 🔹 The shared trunk, built on transformer technology, extracts a latent representation from the input data. This unified representation is shared across all output heads, ensuring consistent and coherent predictions.
- 🔹 Each output head functions independently to predict a distinct future token. This design reduces the sequential dependencies typical in conventional language models, enhancing the model’s efficiency.
- 🔹 The model’s ability to make multiple predictions concurrently not only speeds up learning but also improves sample efficiency. This results in quicker model convergence and less data required for effective training.
- 🔹 At the inference stage, the model can leverage all output heads simultaneously, leading to swift generation of text sequences. This is particularly advantageous for real-time application scenarios.

### Dense Passage Retrieval for Open-Domain Question Answering

- Authors: Vladimir Karpukhin, Barlas Oguz,Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih
- In open-domain question answering (system’s capability to answer questions on any topic rather than being restricted on a specific domain), it’s vital to efficiently identify the right passages from vast information sources (retrieval). Traditional methods, like TF-IDF and BM25, utilize sparse vector models to pick these passages. However, Karpukhin and colleagues in their 2020 EMNLP paper demonstrate a novel approach: using dense vector representations. They employ a dual-encoder framework to generate embeddings from a select set of questions and passages.
- Their objective is metric learning: crafting a vector space where relevant question-passage pairs are closer together than unrelated ones. They optimize this by focusing on the likelihood of selecting the correct (positive) passage amidst a sea of irrelevant (negative) ones.
- Collecting negative examples for training from such a vast pool is challenging. Their solution? Utilizing random passages, ones that match the most question tokens without the actual answer (via BM25), and relevant passages paired with other questions. The most effective model they produced uses these “gold” passages from the same training batch as negative instances, combined with one BM25 negative passage.
- Results were promising. When tested on diverse open-domain QA datasets, their model greatly outperformed the established Lucene-BM25 system, enhancing top-20 passage retrieval accuracy by 9%-19%. This led to their model setting new performance benchmarks in open-domain QA.

#### Dense Passage Retriever (DPR):

1. **Purpose**: The goal of the DPR is to improve the retrieval component in open-domain QA. This involves efficiently retrieving relevant text passages from a vast collection when given a question.
2. **Key Task**: Given a large number $M$ of text passages, the DPR aims to index all of these passages in a low-dimensional continuous space, making it efficient to retrieve the top $k$ most relevant passages for a given input question. $M$ can be very large, like 21 million passages, but $k$ (the number of passages we want to retrieve for a given question) is relatively small, often between 20 and 100.
3. **DPR’s Mechanism**:
	- **Dense Encoder for Passages $EP(\cdot)$**: It converts any text passage to a $d$ -dimensional real-valued vector. This encoder processes and indexes all $M$ passages for retrieval.
	- **Encoder for Questions $EQ(\cdot)$**: At runtime, when a question is posed, this encoder turns the question into a $d$ -dimensional vector.
	- **Similarity Measurement**: The similarity between a question and a passage is calculated using the dot product of their respective vectors: $sim(q, p) = EQ(q) \cdot EP(p)$ .
4. **Passage Size and Boundaries**: The passage’s size and the decision of where a passage begins and ends affect the retriever and reader. Fixed-length passages have been found to be more effective in retrieval and QA accuracy.
5. **Encoders Implementation**: The encoders for both questions and passages are based on BERT networks, a popular deep learning model for NLP. They use the representation at the \[CLS\] token as the output, meaning the output vector has 768 dimensions.
6. **Inference**: During the process of answering a question, the system uses the passage encoder to process all passages and then indexes them using FAISS, an efficient library for similarity search. For any given question, its embedding is computed, and the top $k$ passages with the closest embeddings are retrieved.
7. **Training**:
	- The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.
	- The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.
	- For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.
8. **In-batch Negatives**: A training optimization method is discussed where they use relevant passages from the same batch of questions as negatives, which makes computation more efficient. This technique leverages the similarities between passages in the same batch to boost the number of training examples, effectively reusing computation.

### Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

- The paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.
- Addressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.
- The RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.
- The retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.
- RAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.
- In open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn’t in any retrieved document.
- RAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.
- On the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.
- This study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.

## HuggingFace

### Zephyr: Direct Distillation of LM Alignment

- Authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf
- The paper introduces a technique termed “distilled direct preference optimization” (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:
	1. Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.
	2. AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.
	3. Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.
- They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.
- Results:
	- Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.
	- It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.
	- Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.
- The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.
- The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.
- Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.
- The image below [(source)](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) gives a graphical sense of Zephyr’s performance on tasks as compared with our LLMs.

![](https://aman.ai/primers/ai/top-30-papers/assets/paper/1.png)

## Stanford

### Lost in the Middle: How Language Models Use Long Contexts

- This paper by Liu et al. from Stanford University, University of California Berkeley, and Samaya AI, focuses on analyzing language models’ performance in tasks that require identifying relevant information in long input contexts. The research particularly highlights issues in multi-document question answering and key-value retrieval tasks, revealing a significant degradation in performance when relevant information is situated in the middle of lengthy contexts.
- The study involved an experimental setup for multi-document question answering. Models were tasked with identifying relevant information from a set of documents to answer questions. The researchers manipulated both the length of the input context and the position of the relevant information to observe changes in task performance.
- Several state-of-the-art open and closed language models were evaluated. Among the open models were MPT-30B-Instruct, capable of handling up to 8192 tokens, and LongChat-13B (16K), which extends the context window to 16384 tokens. Closed models included GPT-3.5-Turbo and its variant with an expanded context length of 16K tokens, as well as Claude-1.3 and Claude-1.3 (100K).
- The results revealed a distinct U-shaped performance curve across these models. They performed best when relevant information appeared at the beginning or end of the input context. However, the performance significantly declined when accessing information in the middle of long contexts, challenging the efficacy of extended-context models in utilizing their input effectively.
- A synthetic key-value retrieval task was also used to assess models’ ability to retrieve exact matches from an input context. The task’s simplicity varied across models, with some achieving near-perfect performance, while others struggled with larger contexts.
- The study also explored the impact of model architecture on context usage, comparing decoder-only and encoder-decoder models. Encoder-decoder models like Flan-T5-XXL and Flan-UL2 exhibited more stable performance across various contexts. However, they also began to show performance degradation with sequences longer than their training-time context windows.
- The impact of query-aware contextualization was examined. While this dramatically improved performance in the key-value retrieval task, it had only a minimal effect on the multi-document question answering task.
- Instruction fine-tuning’s effect was analyzed by comparing models like MPT-30B and MPT-30B-Instruct, both fine-tuned for instructions. Both models showed similar U-shaped performance curves, indicating that instruction fine-tuning alone is not responsible for these trends.
- In a case study on open-domain question answering, the research found that model performance does not always improve with an increase in the amount of context provided. The study observed that performance saturates before retriever recall, suggesting that providing too much context may not be beneficial and could potentially reduce accuracy.

## Misc

- The paper by Gao, Ma, Lin, and Callan from Carnegie Mellon University and University of Waterloo introduces Hypothetical Document Embeddings (HyDE), a novel approach for fully zero-shot dense retrieval in the absence of relevance labels. HyDE utilizes instruction-following language models (like InstructGPT) to generate a hypothetical document capturing relevance patterns, although these documents may contain inaccuracies or fictional details.
- Dense retrieval has been effective across various tasks and languages but creating an effective fully zero-shot dense retrieval system without relevance labels remains challenging. Traditional methods like negative mining, distillation, and task-specific pre-training have been proposed to enhance supervised dense retrieval models, yet zero-shot dense retrieval still presents difficulties.
- HyDE’s methodology involves two main steps: generating a hypothetical document that answers the query, and then encoding this document into an embedding vector using an unsupervised contrastively learned encoder like Contriever. This process pivots away from traditional dense retrieval’s reliance on relevance judgments, instead utilizing a language model’s ability to generate relevant content.
- Experiments conducted with HyDE used InstructGPT and Contriever models, along with datasets such as TREC DL19, DL20 (based on MS-MARCO), and a collection from the BEIR dataset for web search, question answering, fact verification, and non-English retrieval tasks. The results showed that HyDE outperforms the state-of-the-art unsupervised dense retriever Contriever and is comparable to fine-tuned retrievers across these tasks and languages.
- The paper concludes by reflecting on HyDE’s novel approach to relevance modeling, which shifts from traditional numerical relevance scores to leveraging natural language generation models. This paradigm suggests a future where the need for relevance labels might be eliminated, and relevance modeling and instruction understanding can be delegated to more powerful and flexible language models. HyDE is practical in the initial stages of a search system’s life, providing performance comparable to fine-tuned models without reliance on relevance labels.

### ALCUNA: Large Language Models Meet New Knowledge

- Authors: Xunjian Yin, Baizhou Huang, and Xiaojun Wan
- The paper proposes a new method called KnowGen to generate artificial entities with new knowledge by making changes to the attributes and relationships of existing entities. This simulates the natural process of new knowledge emerging in the real world.
- KnowGen is applied to structured biological taxonomic data from the EOL database to create artificial organisms. This results in a benchmark dataset called ALCUNA for evaluating large language models (LLMs) on their ability to handle new knowledge.
- ALCUNA contains questions testing the model’s knowledge understanding, differentiation, and association abilities when faced with new entities.
- Several popular LLMs like ChatGPT, Alpaca, Vicuna, and ChatGLM are evaluated on ALCUNA in zero-shot and few-shot settings. The results show these models still struggle with reasoning between new and existing knowledge.
- Analysis reveals factors impacting model performance on new knowledge like entity similarity, contextual knowledge, and input representation format.
- The paper argues benchmarks with truly new knowledge like ALCUNA are important to drive progress in LLMs’ ability to understand and reason with new information, as opposed to existing knowledge already seen during training.
- The artificial nature of the knowledge in ALCUNA makes it reusable as a standard benchmark to assess different models on new knowledge without having to collect new data repeatedly.
- This paper proposes a novel method to automatically generate new structured knowledge for evaluating LLMs’ capabilities in more realistic and challenging settings involving unfamiliar information. The ALCUNA benchmark constructed using this approach provides insights into current model limitations and opportunities for improvement.

### The Perils & Promises of Fact-checking with Large Language Models

- Authors: Dorian Quelle & Alexandre Bovet
- The paper evaluates using large language models (LLMs) like GPT-3.5 and GPT-4 for automated fact-checking of claims. This is important as LLMs are being used more in high stakes domains like research and journalism.
- They test the models on two datasets: PolitFact (US political claims) and a multilingual dataset from Data Commons. The models are evaluated with and without providing contextual information from web searches.
- Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.
- Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.
- Key Results:
	- GPT-4 outperformed GPT-3.5 overall.
	- Providing context significantly improved accuracy, highlighting the importance of evidence gathering.
	- Models struggled with ambiguous “half-true” type verdicts.
	- Performance varied across languages - non-English claims saw a boost when translated to English first.
	- No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.
- Limitations:
	- Biased evaluation due to use of GPT-4 as a scorer.
	- Did not explore model scaling or curating better training data.
	- Safety/ethics of potential misinformation not addressed.
- Implications:
	- LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.
	- Critical examination of LLM reasoning is important before deployment.
	- Understanding model limitations and language-specific differences is key.
	- Continued learning after initial training needs more investigation.
- The paper provides a comprehensive evaluation of GPT-3.5 and GPT-4 on fact-checking, using novel context retrieval and multilingual data. Key findings highlight the models’ strengths as well as areas needing improvement before responsible LLM-assisted fact-checking.