---
title: "使用 GRPO 对 LLMs 进行强化微调！"
source: "https://x.com/AndrewYNg/status/1925213790892929149"
author:
  - "[[@AndrewYNg]]"
created: 2025-05-23
description:
tags:
  - "@AndrewYNg #LLM #强化学习 #微调 #推理 #GRPO"
---
新课程： 使用 GRPO 对 LLMs 进行强化微调 ！  
  
在本短期课程中学习如何使用强化学习来提升你的 LLM 性能，该课程由与

[@Predibase](https://x.com/Predibase)

合作构建，并由

[@TravisAddair](https://x.com/TravisAddair)

授课，他是该公司的联合创始人兼首席技术官，以及

[@grg\_arnav](https://x.com/grg_arnav)

，他是该公司的高级工程师兼机器学习负责人。 推理模型一直是语言模型中最重要的发展之一。强化微调（RFT）使用奖励来鼓励语言模型找到解决多步推理任务的方法，例如解决数学问题和调试代码，而无需像传统监督微调那样预先存在训练示例。 分组相对策略优化（GRPO）是一种正在迅速被采用的强化微调算法。由 DeepSeek 团队开发并用于训练 R1 推理模型，GRPO 使用你可以用 Python 编写的奖励函数来为模型响应分配奖励。它对具有可验证结果的任务很有帮助，即使训练示例少于 100 个也能很好地工作。它还可以显著提高较小的语言模型（LLMs）的推理能力，使应用程序更快且更具成本效益。 在本课程中，你将深入技术层面，学习使用 GRPO 进行基于回合的强化学习（RFT）。你将学习构建奖励函数，以便在 GRPO 训练过程中使用，从而引导语言模型（LLM）在多步推理任务上实现更好的性能。 详细来说，你将： - 了解何时强化微调比监督微调更合适，特别是对于涉及多步推理或有限标记数据的任务。 - 了解广义基于偏好的离线策略优化（GRPO）如何使用可编程奖励函数，作为其他强化学习算法（如基于人类反馈的强化学习（RLHF）和近端策略优化（DPO））所需的人类反馈的更具可扩展性的替代方案。 - 将 Wordle 游戏构建为一个强化微调问题，看看大语言模型（LLM）如何随着时间的推移学习制定计划、分析反馈并改进其策略。 - 设计为强化微调过程提供动力的奖励函数。 - 学习评估更为主观的任务的技巧，例如使用 LLM 作为评判工具来评估文本摘要的质量。 - 了解奖励机制被破解的原因，以及如何通过添加惩罚函数来避免这种情况，以抑制不良行为。 - 了解 GRPO 算法中损失计算的四个关键组成部分：令牌概率分布比率、优势、裁剪和 KL 散度。 - 使用 Predibase 的托管训练服务启动强化微调作业。 在本课程结束时，你将能够使用强化学习来构建和微调 LLMs，从而在不依赖大量标记数据集或主观人工反馈的情况下提高推理能力。