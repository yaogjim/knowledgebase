## 构建主动型RAG系统 - YouTube视频内容

【开场介绍】  
大家好,我很高兴看到你们都已经构建了RAG解决方案。不过我今天要告诉你们的是,RAG可能并不是终点,主动型RAG才是。我叫Erica Cardinas,抱歉忘了自我介绍。我是we8(一个开源向量数据库)的技术合作伙伴经理。在我的工作中,我与合作伙伴和集成协作者一起构建端到端解决方案,并在we recipes(我们的版本的食谱)和其他媒介中分享它们。

【演讲大纲】  
以下是我演讲的大纲:我将首先从比较普通RAG和主动型RAG开始,从基本的检索-增强-生成管道开始,然后讲解如何为语言模型添加层使其具有主动性,当然还会介绍主动型RAG及其好处。然后我会深入agent生态系统,讲解如何使用大型语言模型加函数调用来构建这些系统,以及agent框架(比如Luke刚展示的llama index),再加上可观察性。最后我会以生成式反馈循环作为结尾,讲解我们在we8是如何构建agent的。

【普通RAG vs 主动型RAG】  
让我们先从普通RAG与主动型RAG的比较开始。这里我们有典型的普通RAG或简单RAG工作流:我们有存储文档块的向量索引,它存储在向量数据库中。当用户有查询时,它会进入嵌入模型,然后我们进行语义搜索 - 比较用户查询嵌入和向量索引中的嵌入。然后我们进行增强,取相关上下文和提示一起发送给语言模型。语言模型有了相关上下文和提示后,就会向用户输出响应。

【Agent的组成部分】  
要让语言模型具有主动性,我们需要添加更多组件:首先是agent(即语言模型),然后我们还需要添加记忆功能 - 包括短期记忆(即上下文窗口,不同模型大小不同,比如Gemini如果不在白名单上是200万)和长期记忆(即与用户应用交互的对话历史)。此外还有规划功能,现在语言模型可以思考如何最好地回答用户查询,可以进行某种反思,可以使用思维链或react来实现。当然还有工具 - 可以将向量数据库集合作为工具,也可以有计算器,可以访问网络,甚至可以访问Gmail账户或Slack。

【React框架介绍】  
我想重点介绍如何使用react构建主动型RAG。react是一个迭代框架,可以进行决策也可以基于用户查询采取行动。大语言模型会思考和推理用户问题,它可以路由到特定工具,可以将复杂查询分解成子查询,就像是在进行思考过程。然后它会执行action并观察当前状态,然后会不断迭代循环,直到获得足够信息来回答用户问题,或者如果已经有足够信息就停止。

【普通RAG的局限性示例】  
我想分享一个普通RAG失效的例子。假设我的查询是"总结我最后一次与Connor的对话"。如果将其输入基本RAG管道,我的查询会发送到向量搜索引擎,它可能会检索到我与Connor的对话,但它不知道我要的是最新的对话,因为上下文中没有这个信息。它知道我在和Connor聊天,但没有显示是上次对话的日期信息。

【主动型RAG的优势】  
这就是主动型RAG发挥作用的地方:它可以直接访问我的Slack消息(因为我将其定义为工具),也可以访问我的数据库集合。如果Connor和我在讨论we的混合搜索和多租户功能,而我的数据库集合中有来自weate博客关于这两个功能的内容,它就会从两个来源检索信息,并将其合并成一个连贯的答案,给我上次与Connor对话的总结。

【主动型RAG的具体优势】  
主动型RAG的优势包括:

- 能够从提示中格式化搜索查询
- 理解用户查询意图
- 可以并行调用工具(如Slack API和向量数据库集合)
- 可以导航数据库
- 可以进行迭代搜索

【局限性讨论】  
当然任何好东西都有其不好的一面。不过我列出的这些限制是在看到主动型优化的新特性和研究之前写的。显然延迟是个问题 - 当人们听到主动型RAG时会想"天哪这会很慢,它要循环直到满足用户问题"。这确实是个顾虑,但现在有优化技术可以让这些系统运行得更快。当然还有推理成本 - 你确实需要为每次对语言模型的推理付费,但嘿,也许你想微调一个开源模型,这样就不用为推理调用付费了,因为你的模型是开源的。

【性能对比】  
为了准备这次演讲,我做了普通RAG和主动型RAG的对比。我们用45个来自we8博客的FAQ做了一个小基准测试。这里有三个查询示例,涉及bm25、LangChain以及比较向量库和向量数据库。重点是主动型RAG在34次比较中胜出,这是因为它知道如何理解用户问题的意图,可以导航我的数据库说"我需要去这个集合"或"我需要多种方式运行查询"或"格式化搜索查询以获得最佳响应"。

【多Agent系统探索】  
接下来我想讨论多Agent系统的探索,以及如何整合角色、路由,以及在Agent之间共享和隔离不同工具。在单个Agent系统中,查询会进入语言模型,语言模型会决定是需要访问集合A中的信息,还是需要去集合B,或者这是个数学问题需要用计算器,或者数据库信息不足需要去网上查。所有信息收集完后会发送给最后的语言模型,生成发送给用户的响应。

【多Agent系统详解】  
在多Agent部分,我们有查询和顶层Agent。我们可以建立一个多Agent系统,每个Agent都有专门的工具和角色。Agent A负责不同的数据库集合,Agent B负责从网络检索新信息,Agent C可以访问Slack和Gmail等其他工具。这种管道的优点是不仅工具可以并行调用,而且每个都有专门的重点。

【Agent生态系统】  
这就是主动型RAG与普通RAG的区别的背景,但现在我想讨论Agent生态系统,以及如何通过LLM加函数调用或使用Agent框架来实现这点,当然还要添加可观察性。

【函数调用示例】  
这里我们有Gemini,它显然支持函数调用。这是一个计算器工具的JSON示例。当用户发送查询时,Gemini会获得所有可用工具,它会说"是的这是个适合计算器的查询",将其发回你的应用,然后你的应用需要调用工具,然后你将工具输出发回Gemini形成最终输出。

【Agent框架介绍】  
关于Agent框架,显然Luke很好地解释了llama index的不同工具,但我们还有其他框架:

- ADSP:支持react
- Leta:用于更新Agent记忆的新框架
- LangChain:有其表达语言
- L Graph等其他工具
- ACI用于多Agent编排系统

【AI原生应用堆栈】  
在we,我们对AI原生应用有相同的堆栈思维:

- 向量数据库
- 数据平台
- 框架
- 模型提供者
- 计算基础设施
- 云超大规模提供商

【生成式反馈循环】  
这实际上是我们在we8如何构建Agent。API设计将允许创建新内容并存储回数据库。比如可以创建内容概览,获取we8数据库中的博客,然后总结或创建概览并存储在新的概览属性中。

【GFLs应用场景】  
GFLs的几个应用:

- 数据清理
- 分块(如Anthropic的语义分块)
- 生成合成数据用于数据标注或模型训练

【总结】  
我的演讲总结了以下几点:

- 比较了普通RAG和主动型RAG
- 介绍了Agent生态系统
- 介绍了如何今天就构建主动型RAG
- 介绍了生成式反馈循环

最后提供了一些资源:我和同事L写的博文、我在we播客上谈主动型RAG的内容,当然还有recipes - 在集成文件夹中有llm框架子文件夹,你可以查看大量不同的recipes。